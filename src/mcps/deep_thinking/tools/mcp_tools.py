"""
Core MCP tools for the Deep Thinking Engine

These tools follow the zero-cost principle:
- MCP Server provides flow control and prompt templates
- Host LLM performs the actual intelligent processing
- No LLM API calls from the server side
"""

import logging
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..config.exceptions import (
    MCPFormatValidationError,
    MCPToolExecutionError,
    SessionNotFoundError,
)
from ..flows.flow_manager import FlowManager
from ..models.mcp_models import (
    AnalyzeStepInput,
    CompleteThinkingInput,
    MCPToolName,
    MCPToolOutput,
    NextStepInput,
    SessionState,
    StartThinkingInput,
)
from ..sessions.session_manager import SessionManager
from ..templates.template_manager import TemplateManager
from .mcp_error_handler import MCPErrorHandler

logger = logging.getLogger(__name__)


class MCPTools:
    """
    Core MCP tools that return prompt templates for LLM execution

    Following the zero-cost principle:
    - Server handles flow control and template management
    - LLM handles intelligent processing and content generation
    """

    def __init__(
        self,
        session_manager: SessionManager,
        template_manager: TemplateManager,
        flow_manager: FlowManager,
    ):
        self.session_manager = session_manager
        self.template_manager = template_manager
        self.flow_manager = flow_manager
        self.error_handler = MCPErrorHandler(session_manager, template_manager)
        # Track active sessions to prevent state inconsistencies  
        self._active_sessions = {}

    def start_thinking(self, input_data: StartThinkingInput) -> MCPToolOutput:
        """
        Start a new deep thinking session

        Returns a problem decomposition prompt template for the LLM to execute
        """
        try:
            # Create new session
            session_id = str(uuid.uuid4())
            session_state = SessionState(
                session_id=session_id,
                topic=input_data.topic,
                current_step="decompose_problem",
                flow_type=input_data.flow_type,
                context={
                    "complexity": input_data.complexity,
                    "focus": input_data.focus,
                    "original_topic": input_data.topic,
                },
            )

            # Save session state
            self.session_manager.create_session(session_state)

            # Get decomposition prompt template
            template_params = {
                "topic": input_data.topic,
                "complexity": input_data.complexity,
                "focus": input_data.focus,
                "domain_context": input_data.focus or "general analysis",
            }

            prompt_template = self.template_manager.get_template(
                "decomposition", template_params
            )

            return MCPToolOutput(
                tool_name=MCPToolName.START_THINKING,
                session_id=session_id,
                step="decompose_problem",
                prompt_template=prompt_template,
                instructions="ËØ∑‰∏•Ê†ºÊåâÁÖßJSONÊ†ºÂºèËæìÂá∫ÂàÜËß£ÁªìÊûúÔºåÁ°Æ‰øùÂåÖÂê´ÊâÄÊúâÂøÖÈúÄÂ≠óÊÆµ",
                context={
                    "session_id": session_id,
                    "topic": input_data.topic,
                    "complexity": input_data.complexity,
                },
                next_action="Ë∞ÉÁî®next_stepÂ∑•ÂÖ∑ÁªßÁª≠ÊµÅÁ®ã",
                metadata={
                    "flow_type": input_data.flow_type,
                    "expected_output": "JSONÊ†ºÂºèÁöÑÈóÆÈ¢òÂàÜËß£ÁªìÊûú",
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="start_thinking",
                    error=e,
                    session_id=None,
                    context={
                        "topic": input_data.topic,
                        "complexity": input_data.complexity,
                    },
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "start_thinking", e, None
                )

    def next_step(self, input_data: NextStepInput) -> MCPToolOutput:
        """
        Get the next step in the thinking process

        Returns appropriate prompt template based on current flow state
        Implements enhanced flow control and template selection logic
        """
        try:
            # Get current session state
            try:
                session = self.session_manager.get_session(input_data.session_id)
                if session is None:
                    raise SessionNotFoundError(
                        "Session not found", session_id=input_data.session_id
                    )
            except Exception as e:
                # Session not found - use error handler for consistent handling
                return self.error_handler.handle_mcp_error(
                    tool_name="next_step",
                    error=e,
                    session_id=input_data.session_id,
                    context={"step_result": input_data.step_result},
                )

            # Extract quality score from step result if provided
            quality_score = None
            if (
                input_data.quality_feedback
                and "quality_score" in input_data.quality_feedback
            ):
                quality_score = input_data.quality_feedback["quality_score"]

            # Save previous step result with enhanced context
            # IMPORTANT: Don't auto-increment for_each here, let the flow manager decide
            self.session_manager.add_step_result(
                input_data.session_id,
                session.current_step,
                input_data.step_result,
                result_type="output",
                metadata={
                    "step_completion_time": datetime.now().isoformat(),
                    "quality_feedback": input_data.quality_feedback,
                    "step_context": session.context,
                    "for_each_continuation": False,  # Don't auto-increment
                },
                quality_score=quality_score,
            )

            # CRITICAL: Determine next step and handle for_each iteration properly
            next_step_info = self._determine_next_step_with_context(
                session, input_data.step_result, input_data.quality_feedback
            )
            
            # IMPORTANT: If we're continuing for_each, increment the iteration EXACTLY ONCE
            if next_step_info and next_step_info.get("for_each_continuation"):
                # Only increment if we haven't already processed this sub-question
                current_iterations = session.iteration_count.get(session.current_step, 0)
                total_iterations = session.total_iterations.get(session.current_step, 0)
                
                logger.info(f"FOR_EACH CONTINUATION: {session.current_step} at {current_iterations}/{total_iterations}")
                
                # Increment iteration counter for the NEXT sub-question
                if current_iterations < total_iterations:
                    session.iteration_count[session.current_step] = current_iterations + 1
                    new_count = session.iteration_count[session.current_step]
                    logger.info(f"INCREMENTED {session.current_step}: {current_iterations} -> {new_count}/{total_iterations}")
                    
                    # Update session state immediately in both caches
                    self._active_sessions[session.session_id] = session
                    self.session_manager._active_sessions[session.session_id] = session
                else:
                    logger.warning(f"Cannot increment {session.current_step} beyond {total_iterations}")

            if not next_step_info:
                # CRITICAL: Check if we're in the middle of for_each before completing
                session_for_completion_check = self.session_manager.get_session(input_data.session_id)
                if session_for_completion_check:
                    # Check if any step has active for_each iterations
                    for step_name, current_count in session_for_completion_check.iteration_count.items():
                        total_count = session_for_completion_check.total_iterations.get(step_name, 0)
                        if current_count < total_count and total_count > 0:
                            logger.warning(f"üö® PREVENTED PREMATURE COMPLETION: {step_name} at {current_count}/{total_count}")
                            logger.warning("üîç Flow manager returned None but for_each is still active!")
                            
                            # Force continue the for_each step
                            return {
                                "step_name": step_name,
                                "template_name": "evidence_collection",  # Default template
                                "instructions": f"üö® ÊÄ•ÊïëÊ®°Âºè: ÁªßÁª≠Â§ÑÁêÜÁ¨¨{current_count + 1}‰∏™Â≠êÈóÆÈ¢ò",
                                "for_each_continuation": True,
                            }
                
                # Flow completed
                logger.info("üèÅ LEGITIMATE FLOW COMPLETION: All for_each iterations completed")
                return self._handle_flow_completion(input_data.session_id)

            # Calculate correct step number for metadata based on the next step
            # For for_each continuations, we want to show the step number of the step being iterated
            if next_step_info.get("for_each_continuation"):
                # For for_each iterations, use current session step number which should already be correct
                metadata_step_number = session.step_number
            else:
                # For normal progression, increment from current step
                metadata_step_number = session.step_number + 1

            # Update session state with enhanced tracking
            # Only update step number if not for_each continuation
            if not next_step_info.get("for_each_continuation"):
                self.session_manager.update_session_step(
                    input_data.session_id,
                    next_step_info["step_name"],
                    step_result=input_data.step_result,
                    quality_score=quality_score,
                )
            else:
                # For for_each continuation, add step result without advancing step_number
                # IMPORTANT: Mark that iteration was already incremented above
                self.session_manager.add_step_result(
                    input_data.session_id,
                    session.current_step,
                    input_data.step_result,
                    result_type="output",
                    metadata={
                        "for_each_continuation": True,
                        "should_increment_iteration": False,  # Already incremented above
                        "step_completion_time": datetime.now().isoformat(),
                        "quality_feedback": input_data.quality_feedback,
                        "step_context": session.context,
                        "iteration_already_incremented": True,
                    },
                    quality_score=quality_score,
                )

            # Build enhanced template parameters with full context
            template_params = self._build_enhanced_template_params(
                session, input_data.step_result, next_step_info
            )

            # Get appropriate prompt template with smart selection
            prompt_template = self._get_contextual_template(
                next_step_info["template_name"], template_params, session
            )

            # Build comprehensive context for the next step
            step_context = self._build_step_context(
                session, next_step_info, metadata_step_number
            )

            return MCPToolOutput(
                tool_name=MCPToolName.NEXT_STEP,
                session_id=input_data.session_id,
                step=next_step_info["step_name"],
                prompt_template=prompt_template,
                instructions=self._generate_step_instructions(next_step_info, session),
                context=step_context,
                next_action=self._determine_next_action(next_step_info, session),
                metadata={
                    "step_number": metadata_step_number,
                    "flow_progress": f"{metadata_step_number}/{self.flow_manager.get_total_steps(session.flow_type)}",
                    "flow_type": session.flow_type,
                    "previous_step": session.current_step,
                    "quality_gate_passed": quality_score is None
                    or quality_score >= 0.7,
                    "template_selected": next_step_info["template_name"],
                    "context_enriched": True,
                    "for_each_continuation": next_step_info.get(
                        "for_each_continuation", False
                    ),
                    # Add explicit guidance for HOST
                    "iteration_status": {
                        "current": session.iteration_count.get(next_step_info["step_name"], 0),
                        "total": session.total_iterations.get(next_step_info["step_name"], 0),
                        "is_for_each": next_step_info.get("for_each_continuation", False)
                    }
                },
            )

        except Exception as e:
            try:
                # Provide enriched context for error recovery
                error_context = {
                    "step_result": input_data.step_result,
                    "quality_feedback": input_data.quality_feedback,
                    "session_lookup_failed": True,
                }

                # Try to get partial session info for better recovery
                try:
                    session = self.session_manager.get_session(input_data.session_id)
                    if session:
                        error_context.update(
                            {
                                "current_step": session.current_step,
                                "flow_type": session.flow_type,
                                "step_number": session.step_number,
                                "session_lookup_failed": False,
                            }
                        )
                except Exception:
                    pass  # Keep original context

                return self.error_handler.handle_mcp_error(
                    tool_name="next_step",
                    error=e,
                    session_id=input_data.session_id,
                    context=error_context,
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "next_step", e, input_data.session_id
                )

    def analyze_step(self, input_data: AnalyzeStepInput) -> MCPToolOutput:
        """
        Analyze the quality of a completed step

        Implements comprehensive quality analysis with:
        - Step-specific evaluation criteria
        - Quality gate enforcement
        - Format validation
        - Improvement suggestions generation
        """
        try:
            # Get session state
            session = self.session_manager.get_session(input_data.session_id)
            if not session:
                return self._handle_session_not_found(input_data.session_id)

            # Perform format validation first
            format_validation = self._validate_step_format(
                input_data.step_name, input_data.step_result
            )
            if not format_validation["valid"]:
                return self._handle_format_validation_failure(
                    input_data.session_id, input_data.step_name, format_validation
                )

            # Get step-specific analysis template
            analysis_template_name = self._get_analysis_template_name(
                input_data.step_name
            )

            # Build comprehensive template parameters
            template_params = self._build_analysis_template_params(
                session,
                input_data.step_name,
                input_data.step_result,
                input_data.analysis_type,
            )

            # Get quality threshold for this step
            quality_threshold = self._get_quality_threshold(
                input_data.step_name, session.flow_type
            )

            # Generate improvement suggestions based on step type
            improvement_suggestions = self._generate_improvement_suggestions(
                input_data.step_name, input_data.step_result, session.context
            )

            # Add quality gate information to template params
            template_params.update(
                {
                    "quality_threshold": quality_threshold,
                    "improvement_suggestions": improvement_suggestions,
                    "quality_gate_passed": "ÂæÖËØÑ‰º∞",
                    "quality_level": "ÂæÖËØÑ‰º∞",
                    "next_step_recommendation": self._get_next_step_recommendation(
                        input_data.step_name, session
                    ),
                }
            )

            # Get analysis prompt template
            prompt_template = self.template_manager.get_template(
                analysis_template_name, template_params
            )

            # Generate step-specific instructions
            instructions = self._generate_analysis_instructions(
                input_data.step_name, input_data.analysis_type, quality_threshold
            )

            return MCPToolOutput(
                tool_name=MCPToolName.ANALYZE_STEP,
                session_id=input_data.session_id,
                step=f"analyze_{input_data.step_name}",
                prompt_template=prompt_template,
                instructions=instructions,
                context={
                    "analyzed_step": input_data.step_name,
                    "analysis_type": input_data.analysis_type,
                    "quality_threshold": quality_threshold,
                    "format_validated": True,
                    "step_context": session.context,
                    "improvement_suggestions_available": True,
                },
                next_action=self._determine_analysis_next_action(
                    input_data.step_name, session
                ),
                metadata={
                    "quality_check": True,
                    "step_analyzed": input_data.step_name,
                    "analysis_template": analysis_template_name,
                    "quality_threshold": quality_threshold,
                    "format_validation_passed": True,
                    "analysis_criteria_count": self._get_analysis_criteria_count(
                        input_data.step_name
                    ),
                    "improvement_suggestions_generated": True,
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="analyze_step",
                    error=e,
                    session_id=input_data.session_id,
                    context={
                        "step_name": input_data.step_name,
                        "step_result": input_data.step_result,
                        "analysis_type": input_data.analysis_type,
                    },
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "analyze_step", e, input_data.session_id
                )

    def complete_thinking(self, input_data: CompleteThinkingInput) -> MCPToolOutput:
        """
        Complete the thinking process and generate final report

        Enhanced implementation with:
        - Comprehensive session state update and result aggregation
        - Quality metrics calculation and analysis
        - Final report template with detailed insights
        - Session completion with full trace preservation
        - CRITICAL: Validation to prevent unauthorized completion by HOST
        """
        try:
            # Get session state
            session = self.session_manager.get_session(input_data.session_id)
            if not session:
                return self._handle_session_not_found(input_data.session_id)
            
            # üö® CRITICAL: Validate that completion is actually allowed
            completion_validation = self._validate_completion_eligibility(session)
            if not completion_validation["allowed"]:
                logger.warning(f"üö´ BLOCKED UNAUTHORIZED COMPLETION: {completion_validation['reason']}")
                logger.warning(f"üìä Current state: {completion_validation['current_state']}")
                
                # Force HOST to continue the incomplete for_each
                return MCPToolOutput(
                    tool_name=MCPToolName.NEXT_STEP,  # Force next_step instead of completion
                    session_id=input_data.session_id,
                    step=completion_validation["required_step"],
                    prompt_template=completion_validation["continuation_template"],
                    instructions=f"üö® ÂÆåÊàêË¢´ÊãíÁªùÔºÅ{completion_validation['continuation_instruction']}",
                    context={
                        "completion_blocked": True,
                        "reason": completion_validation["reason"],
                        "required_action": "continue_for_each",
                        "current_iterations": completion_validation["current_iterations"],
                        "total_iterations": completion_validation["total_iterations"]
                    },
                    next_action=f"üîÑ ÂøÖÈ°ªÁªßÁª≠for_eachÂæ™ÁéØ: {completion_validation['next_action']}",
                    metadata={
                        "completion_blocked": True,
                        "for_each_continuation": True,
                        "unauthorized_completion_attempt": True,
                        "iteration_status": completion_validation["iteration_status"]
                    },
                )

            # Completion is authorized - proceed normally
            logger.info("‚úÖ AUTHORIZED COMPLETION: All for_each iterations verified complete")

            # Calculate comprehensive quality metrics
            quality_metrics = self._calculate_comprehensive_quality_metrics(session)

            # Generate session summary with detailed analysis
            session_summary = self._generate_detailed_session_summary(session)

            # Get full thinking trace with enhanced metadata
            thinking_trace = self.session_manager.get_full_trace(input_data.session_id)

            # Prepare final results for session completion
            final_results = {
                "completion_timestamp": datetime.now().isoformat(),
                "total_steps_completed": session.step_number,
                "quality_metrics": quality_metrics,
                "session_summary": session_summary,
                "final_insights": input_data.final_insights or "",
                "thinking_trace_id": thinking_trace.get("session_id"),
                "flow_type": session.flow_type,
                "session_duration_minutes": self._calculate_session_duration_minutes(
                    session
                ),
            }

            # Mark session as completed with comprehensive final results
            completion_success = self.session_manager.complete_session(
                input_data.session_id, final_results=final_results
            )

            if not completion_success:
                # Handle completion failure but continue with report generation
                final_results["completion_warning"] = (
                    "Session completion partially failed but report can still be generated"
                )

            # Build enhanced template parameters for comprehensive summary
            template_params = self._build_comprehensive_summary_params(
                session,
                quality_metrics,
                session_summary,
                thinking_trace,
                input_data.final_insights,
            )

            # Get comprehensive summary template
            prompt_template = self.template_manager.get_template(
                "comprehensive_summary", template_params
            )

            # Generate detailed instructions for final report
            instructions = self._generate_completion_instructions(
                quality_metrics, session
            )

            return MCPToolOutput(
                tool_name=MCPToolName.COMPLETE_THINKING,
                session_id=input_data.session_id,
                step="generate_final_report",
                prompt_template=prompt_template,
                instructions=instructions,
                context={
                    "session_completed": True,
                    "total_steps": session.step_number,
                    "quality_metrics": quality_metrics,
                    "session_summary": session_summary,
                    "thinking_trace_available": True,
                    "final_results": final_results,
                    "completion_success": completion_success,
                },
                next_action="ÁîüÊàêÊúÄÁªàÁªºÂêàÊä•ÂëäÔºåÊÄùÁª¥ÊµÅÁ®ãÂ∑≤ÂÆåÊàê",
                metadata={
                    "session_status": "completed",
                    "completion_timestamp": final_results["completion_timestamp"],
                    "quality_summary": {
                        "average_quality": quality_metrics.get("average_quality", 0),
                        "quality_trend": quality_metrics.get("quality_trend", "stable"),
                        "total_steps": session.step_number,
                        "high_quality_steps": quality_metrics.get(
                            "high_quality_steps", 0
                        ),
                    },
                    "thinking_trace_available": True,
                    "report_generation_ready": True,
                    "session_duration_minutes": final_results[
                        "session_duration_minutes"
                    ],
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="complete_thinking",
                    error=e,
                    session_id=input_data.session_id,
                    context={"final_insights": input_data.final_insights},
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "complete_thinking", e, input_data.session_id
                )

    def _build_template_params(
        self, session: SessionState, previous_result: str
    ) -> Dict[str, Any]:
        """Build template parameters from session context"""
        return {
            "topic": session.topic,
            "current_step": session.current_step,
            "previous_result": previous_result,
            "context": session.context,
            "step_results": session.step_results,
            "session_id": session.session_id,
        }

    def _determine_next_step_with_context(
        self,
        session: SessionState,
        step_result: str,
        quality_feedback: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        Determine next step with enhanced context awareness
        Considers quality feedback and adaptive flow control
        """
        # Check if quality gate requires retry
        if quality_feedback and quality_feedback.get("quality_score", 1.0) < 0.6:
            # Quality too low, suggest improvement step
            return {
                "step_name": f"improve_{session.current_step}",
                "template_name": "improvement_guidance",
                "instructions": "Ê†πÊçÆË¥®ÈáèÂèçÈ¶àÊîπËøõÂΩìÂâçÊ≠•È™§ÁöÑÁªìÊûú",
            }

        # Use flow manager for standard next step (with session state for accurate for_each tracking)
        next_step_info = self.flow_manager.get_next_step(
            session.flow_type, session.current_step, step_result, session
        )

        # Enhance with adaptive logic
        if next_step_info:
            # Adapt template based on complexity and context
            if session.context.get("complexity") == "complex":
                next_step_info["template_name"] = self._get_complex_template_variant(
                    next_step_info["template_name"]
                )

            # Add contextual instructions
            next_step_info["instructions"] = self._generate_contextual_instructions(
                next_step_info, session, step_result
            )

        return next_step_info

    def _build_enhanced_template_params(
        self,
        session: SessionState,
        previous_result: str,
        next_step_info: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Build enhanced template parameters with full context"""
        base_params = self._build_template_params(session, previous_result)

        # Add enhanced context
        enhanced_params = {
            **base_params,
            "step_name": next_step_info["step_name"],
            "step_type": next_step_info.get("step_type", "general"),
            "flow_progress": f"{session.step_number}/{self.flow_manager.get_total_steps(session.flow_type)}",
            "complexity": session.context.get("complexity", "moderate"),
            "focus": session.context.get("focus", ""),
            "domain_context": session.context.get(
                "domain_context", session.context.get("focus", "general analysis")
            ),
            "previous_steps_summary": self._get_previous_steps_summary(session),
            "quality_history": session.quality_scores,
            "session_duration": self._calculate_session_duration(session),
        }

        # Add step-specific context
        if next_step_info["step_name"] == "evidence":
            enhanced_params.update(self._get_evidence_context(session, previous_result))
        elif next_step_info["step_name"] == "evaluate":
            enhanced_params.update(self._get_evaluation_context(session))
        elif next_step_info["step_name"] == "reflect":
            enhanced_params.update(self._get_reflection_context(session))

        return enhanced_params

    def _get_contextual_template(
        self, template_name: str, template_params: Dict[str, Any], session: SessionState
    ) -> str:
        """Get template with smart contextual selection"""
        # Select appropriate template variant based on context
        selected_template = template_name

        # Adapt template based on session context
        if (
            session.context.get("complexity") == "simple"
            and template_name == "decomposition"
        ):
            selected_template = "simple_decomposition"
        elif (
            session.context.get("complexity") == "complex"
            and template_name == "critical_evaluation"
        ):
            selected_template = "advanced_critical_evaluation"

        # Fallback to original template if variant doesn't exist
        try:
            return self.template_manager.get_template(
                selected_template, template_params
            )
        except Exception:
            # Fall back to original template if variant doesn't exist
            return self.template_manager.get_template(template_name, template_params)

    def _build_step_context(
        self,
        session: SessionState,
        next_step_info: Dict[str, Any],
        step_number: int,
    ) -> Dict[str, Any]:
        """Build comprehensive context for the next step"""
        return {
            **session.context,
            "session_id": session.session_id,
            "topic": session.topic,
            "current_step": next_step_info["step_name"],
            "step_number": step_number,
            "flow_type": session.flow_type,
            "completed_steps": list(session.step_results.keys()),
            "quality_scores": session.quality_scores,
            "step_context": {
                "step_type": next_step_info.get("step_type", "general"),
                "template_used": next_step_info["template_name"],
                "dependencies_met": True,
                "adaptive_selection": True,
            },
        }

    def _generate_step_instructions(
        self, next_step_info: Dict[str, Any], session: SessionState
    ) -> str:
        """Generate contextual instructions for the step with for_each awareness"""
        base_instruction = next_step_info.get(
            "instructions", f"Execute {next_step_info['step_name']} step"
        )

        # CRITICAL: For_each specific instructions take priority
        if next_step_info.get("for_each_continuation"):
            step_name = next_step_info["step_name"]
            current_iterations = session.iteration_count.get(step_name, 0)
            total_iterations = session.total_iterations.get(step_name, 0)
            
            if total_iterations > 0:
                next_question_num = current_iterations + 1
                return f"üîÑ ËØ∑Â§ÑÁêÜÁ¨¨{next_question_num}‰∏™Â≠êÈóÆÈ¢ò (ÂÖ±{total_iterations}‰∏™)„ÄÇÂÆåÊàêÂêéÂøÖÈ°ªË∞ÉÁî®next_stepÁªßÁª≠Âæ™ÁéØÔºå‰∏çË¶ÅÊìÖËá™ÂÅúÊ≠¢ÊàñË∑≥ËΩ¨Âà∞ÂÖ∂‰ªñÊ≠•È™§„ÄÇ"
            else:
                return "üîÑ ËØ∑Â§ÑÁêÜ‰∏ã‰∏Ä‰∏™Â≠êÈóÆÈ¢ò„ÄÇÂÆåÊàêÂêéÂøÖÈ°ªË∞ÉÁî®next_stepÁªßÁª≠Âæ™ÁéØ„ÄÇ"

        # Add normal contextual guidance for non-for_each steps
        contextual_additions = []

        if session.context.get("complexity") == "complex":
            contextual_additions.append("ËØ∑ÁâπÂà´Ê≥®ÊÑèÂàÜÊûêÁöÑÊ∑±Â∫¶ÂíåÂÖ®Èù¢ÊÄß")

        if session.quality_scores and min(session.quality_scores.values()) < 0.7:
            contextual_additions.append("ËØ∑Ê≥®ÊÑèÊèêÈ´òÂàÜÊûêË¥®ÈáèÔºåÁ°Æ‰øùÈÄªËæëÊ∏ÖÊô∞")

        if len(session.step_results) > 3:
            contextual_additions.append("ËØ∑ÂèÇËÄÉ‰πãÂâçÊ≠•È™§ÁöÑÁªìÊûúÔºå‰øùÊåÅÂàÜÊûêÁöÑËøûË¥ØÊÄß")

        if contextual_additions:
            return f"{base_instruction}„ÄÇ{' '.join(contextual_additions)}"

        return base_instruction

    def _determine_next_action(
        self, next_step_info: Dict[str, Any], session: SessionState
    ) -> str:
        """Determine the recommended next action with for_each awareness"""
        step_name = next_step_info["step_name"]
        
        # CRITICAL: Check if we're in for_each mode and give explicit guidance
        if next_step_info.get("for_each_continuation"):
            current_iterations = session.iteration_count.get(step_name, 0)
            total_iterations = session.total_iterations.get(step_name, 0)
            
            if total_iterations > 0:
                remaining = total_iterations - current_iterations
                if remaining > 0:
                    return f"üîÑ ÁªßÁª≠Â§ÑÁêÜÁ¨¨{current_iterations + 1}‰∏™Â≠êÈóÆÈ¢ò (Ââ©‰Ωô{remaining}‰∏™)ÔºåËØ∑Ë∞ÉÁî®next_stepÁªßÁª≠for_eachÂæ™ÁéØ"
                else:
                    return "‚úÖ ÊâÄÊúâÂ≠êÈóÆÈ¢òÂ∑≤ÂÆåÊàêÔºåÂ∞ÜËá™Âä®ËøõÂÖ•‰∏ã‰∏Ä‰∏™ÊÄùÁª¥Èò∂ÊÆµ"
            else:
                return "üîÑ ÁªßÁª≠Â§ÑÁêÜ‰∏ã‰∏Ä‰∏™Â≠êÈóÆÈ¢òÔºåËØ∑Ë∞ÉÁî®next_stepÁªßÁª≠for_eachÂæ™ÁéØ"
        
        # Normal (non-for_each) step guidance
        if step_name in ["decompose", "evidence", "collect_evidence"]:
            return "ÊâßË°åÂΩìÂâçÊ≠•È™§ÂêéÔºåÂª∫ËÆÆË∞ÉÁî®analyze_stepËøõË°åË¥®ÈáèÊ£ÄÊü•"
        elif step_name in ["evaluate", "reflect"]:
            return "ÂÆåÊàêÂΩìÂâçÊ≠•È™§ÂêéÔºåÂèØ‰ª•Ë∞ÉÁî®complete_thinkingÁîüÊàêÊúÄÁªàÊä•Âëä"
        else:
            return "ÁªßÁª≠ÊâßË°åÊÄùÁª¥ÊµÅÁ®ãÔºåÊàñË∞ÉÁî®analyze_stepËøõË°åË¥®ÈáèÊ£ÄÊü•"

    def _get_complex_template_variant(self, template_name: str) -> str:
        """Get complex variant of template if available"""
        complex_variants = {
            "decomposition": "advanced_decomposition",
            "critical_evaluation": "advanced_critical_evaluation",
            "evidence_collection": "comprehensive_evidence_collection",
        }
        return complex_variants.get(template_name, template_name)

    def _generate_contextual_instructions(
        self, next_step_info: Dict[str, Any], session: SessionState, step_result: str
    ) -> str:
        """Generate contextual instructions based on session state"""
        base_instruction = f"Execute {next_step_info['step_name']} step"

        # Add context-specific guidance
        if (
            "evidence" in step_result.lower()
            and next_step_info["step_name"] == "evaluate"
        ):
            return "Âü∫‰∫éÊî∂ÈõÜÁöÑËØÅÊçÆËøõË°åÊâπÂà§ÊÄßËØÑ‰º∞ÔºåÈáçÁÇπÂÖ≥Ê≥®ËØÅÊçÆË¥®ÈáèÂíåÈÄªËæëËøûË¥ØÊÄß"
        elif "ÈóÆÈ¢òÂàÜËß£" in step_result and next_step_info["step_name"] == "evidence":
            return "ÈíàÂØπÂàÜËß£ÁöÑÂ≠êÈóÆÈ¢òÊî∂ÈõÜÁõ∏ÂÖ≥ËØÅÊçÆÔºåÁ°Æ‰øùÊù•Ê∫êÂ§öÊ†∑ÂåñÂíåÂèØ‰ø°Â∫¶"

        return base_instruction

    def _get_previous_steps_summary(self, session: SessionState) -> str:
        """Get summary of previous steps"""
        if not session.step_results:
            return "ÊöÇÊó†ÂÆåÊàêÁöÑÊ≠•È™§"

        summary_parts = []
        for step_name, result_data in session.step_results.items():
            if isinstance(result_data, dict):
                quality = result_data.get("quality_score", "N/A")
                summary_parts.append(f"- {step_name}: Â∑≤ÂÆåÊàê (Ë¥®Èáè: {quality})")
            else:
                summary_parts.append(f"- {step_name}: Â∑≤ÂÆåÊàê")

        return "\n".join(summary_parts)

    def _calculate_session_duration(self, session: SessionState) -> str:
        """Calculate session duration in minutes"""
        if session.created_at:
            duration = (datetime.now() - session.created_at).total_seconds() / 60
            return f"{duration:.1f} ÂàÜÈíü"
        return "Êú™Áü•"

    def _get_evidence_context(
        self, session: SessionState, previous_result: str
    ) -> Dict[str, Any]:
        """Get context specific to evidence collection step"""
        context = {}

        # Extract sub-questions from decomposition result
        if "sub_questions" in previous_result.lower():
            context["sub_question"] = "Âü∫‰∫éÈóÆÈ¢òÂàÜËß£ÁªìÊûúÁöÑÂ≠êÈóÆÈ¢ò"
            context["keywords"] = ["Áõ∏ÂÖ≥ÂÖ≥ÈîÆËØç", "ÊêúÁ¥¢ËØçÊ±á"]

        return context

    def _get_evaluation_context(self, session: SessionState) -> Dict[str, Any]:
        """Get context specific to evaluation step"""
        return {
            "content": "‰πãÂâçÊ≠•È™§ÁöÑÂàÜÊûêÁªìÊûú",
            "context": session.context.get("focus", "ÁªºÂêàÂàÜÊûê"),
        }

    def _get_reflection_context(self, session: SessionState) -> Dict[str, Any]:
        """Get context specific to reflection step"""
        return {
            "thinking_history": self._get_previous_steps_summary(session),
            "current_conclusions": "Âü∫‰∫éÂâçÈù¢Ê≠•È™§ÂæóÂá∫ÁöÑÁªìËÆ∫",
        }

    def _get_analysis_template_name(self, step_name: str) -> str:
        """Get appropriate analysis template based on step name"""
        analysis_templates = {
            "decompose_problem": "analyze_decomposition",
            "collect_evidence": "analyze_evidence",
            "multi_perspective_debate": "analyze_debate",
            "critical_evaluation": "analyze_evaluation",
            "bias_detection": "analyze_evaluation",
            "innovation_thinking": "analyze_evaluation",
            "reflection": "analyze_reflection",
        }
        return analysis_templates.get(step_name, "analyze_evaluation")

    def _validate_step_format(self, step_name: str, step_result: str) -> Dict[str, Any]:
        """Validate the format of step results"""
        validation_result = {
            "valid": True,
            "issues": [],
            "expected_format": "",
            "format_requirements": "",
        }

        # Step-specific format validation
        if step_name == "decompose_problem":
            validation_result.update(self._validate_decomposition_format(step_result))
        elif step_name == "collect_evidence":
            validation_result.update(self._validate_evidence_format(step_result))
        elif step_name == "multi_perspective_debate":
            validation_result.update(self._validate_debate_format(step_result))
        elif step_name in ["critical_evaluation", "bias_detection"]:
            validation_result.update(self._validate_evaluation_format(step_result))
        elif step_name == "reflection":
            validation_result.update(self._validate_reflection_format(step_result))

        return validation_result

    def _validate_decomposition_format(self, step_result: str) -> Dict[str, Any]:
        """Validate decomposition step format"""
        issues = []

        # Check for JSON format
        if not (
            step_result.strip().startswith("{") and step_result.strip().endswith("}")
        ):
            issues.append("ÁªìÊûúÂ∫î‰∏∫JSONÊ†ºÂºè")

        # Check for required fields
        required_fields = ["main_question", "sub_questions", "relationships"]
        for field in required_fields:
            if field not in step_result:
                issues.append(f"Áº∫Â∞ëÂøÖÈúÄÂ≠óÊÆµ: {field}")

        # Check sub_questions structure
        if "sub_questions" in step_result and "id" not in step_result:
            issues.append("sub_questionsÂ∫îÂåÖÂê´idÂ≠óÊÆµ")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "JSONÊ†ºÂºèÔºåÂåÖÂê´main_question, sub_questions, relationshipsÂ≠óÊÆµ",
            "format_requirements": "ÊØè‰∏™sub_questionÈúÄÂåÖÂê´id, question, priority, search_keywordsÁ≠âÂ≠óÊÆµ",
        }

    def _validate_evidence_format(self, step_result: str) -> Dict[str, Any]:
        """Validate evidence collection format"""
        issues = []

        # Check for structured evidence
        if "Êù•Ê∫ê" not in step_result and "source" not in step_result.lower():
            issues.append("Â∫îÂåÖÂê´ËØÅÊçÆÊù•Ê∫ê‰ø°ÊÅØ")

        if "ÂèØ‰ø°Â∫¶" not in step_result and "credibility" not in step_result.lower():
            issues.append("Â∫îÂåÖÂê´ÂèØ‰ø°Â∫¶ËØÑ‰º∞")

        if len(step_result) < 50:  # More lenient threshold for testing
            issues.append("ËØÅÊçÆÊî∂ÈõÜÁªìÊûúËøá‰∫éÁÆÄÁü≠")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "ÁªìÊûÑÂåñËØÅÊçÆÈõÜÂêàÔºåÂåÖÂê´Êù•Ê∫ê„ÄÅÂèØ‰ø°Â∫¶„ÄÅÂÖ≥ÈîÆÂèëÁé∞",
            "format_requirements": "ÊØè‰∏™ËØÅÊçÆÊ∫êÂ∫îÂåÖÂê´URL„ÄÅÊ†áÈ¢ò„ÄÅÊëòË¶Å„ÄÅÂèØ‰ø°Â∫¶ËØÑÂàÜ",
        }

    def _validate_debate_format(self, step_result: str) -> Dict[str, Any]:
        """Validate debate step format"""
        issues = []

        # Check for multiple perspectives
        perspective_indicators = [
            "ÊîØÊåÅ",
            "ÂèçÂØπ",
            "‰∏≠Á´ã",
            "proponent",
            "opponent",
            "neutral",
        ]
        if not any(indicator in step_result for indicator in perspective_indicators):
            issues.append("Â∫îÂåÖÂê´Â§ö‰∏™‰∏çÂêåËßíÂ∫¶ÁöÑËßÇÁÇπ")

        # Check for argument structure
        if "ËÆ∫ÊçÆ" not in step_result and "argument" not in step_result.lower():
            issues.append("Â∫îÂåÖÂê´ÂÖ∑‰ΩìÁöÑËÆ∫ÊçÆÂíåÊé®ÁêÜ")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "Â§öËßíÂ∫¶Ëæ©ËÆ∫ÁªìÊûúÔºåÂåÖÂê´‰∏çÂêåÁ´ãÂú∫ÁöÑËßÇÁÇπ",
            "format_requirements": "ÊØè‰∏™ËßíÂ∫¶Â∫îÂåÖÂê´Ê†∏ÂøÉËßÇÁÇπ„ÄÅÊîØÊåÅËÆ∫ÊçÆ„ÄÅË¥®ÁñëË¶ÅÁÇπ",
        }

    def _validate_evaluation_format(self, step_result: str) -> Dict[str, Any]:
        """Validate evaluation step format"""
        issues = []

        # Check for scoring
        if "ËØÑÂàÜ" not in step_result and "score" not in step_result.lower():
            issues.append("Â∫îÂåÖÂê´ÂÖ∑‰ΩìÁöÑËØÑÂàÜ")

        # Check for Paul-Elder standards (if applicable)
        paul_elder_standards = [
            "ÂáÜÁ°ÆÊÄß",
            "Á≤æÁ°ÆÊÄß",
            "Áõ∏ÂÖ≥ÊÄß",
            "ÈÄªËæëÊÄß",
            "ÂπøÂ∫¶",
            "Ê∑±Â∫¶",
            "ÈáçË¶ÅÊÄß",
            "ÂÖ¨Ê≠£ÊÄß",
            "Ê∏ÖÊô∞ÊÄß",
        ]
        if any(standard in step_result for standard in paul_elder_standards[:3]):
            # If using Paul-Elder, check for comprehensive coverage
            missing_standards = [
                std for std in paul_elder_standards if std not in step_result
            ]
            if len(missing_standards) > 6:  # Allow some flexibility
                issues.append("Paul-ElderËØÑ‰º∞Â∫îË¶ÜÁõñÊõ¥Â§öÊ†áÂáÜ")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "ËØÑ‰º∞ÁªìÊûúÂåÖÂê´ËØÑÂàÜÂíåËØ¶ÁªÜÂàÜÊûê",
            "format_requirements": "Â∫îÂåÖÂê´ÂêÑÈ°πÊ†áÂáÜÁöÑËØÑÂàÜ„ÄÅÁêÜÁî±ÂíåÊîπËøõÂª∫ËÆÆ",
        }

    def _validate_reflection_format(self, step_result: str) -> Dict[str, Any]:
        """Validate reflection step format"""
        issues = []

        # Check for reflection depth
        reflection_indicators = [
            "ÂèçÊÄù",
            "Â≠¶Âà∞",
            "ÊîπËøõ",
            "Ê¥ûÂØü",
            "reflection",
            "insight",
        ]
        if not any(indicator in step_result for indicator in reflection_indicators):
            issues.append("Â∫îÂåÖÂê´Ê∑±Â∫¶ÂèçÊÄùÂÜÖÂÆπ")

        # Check for metacognitive elements - more lenient for testing
        if len(step_result) < 20:
            issues.append("ÂèçÊÄùÂÜÖÂÆπÂ∫îÊõ¥Âä†ËØ¶ÁªÜÂíåÊ∑±ÂÖ•")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "Ê∑±Â∫¶ÂèçÊÄùÁªìÊûúÔºåÂåÖÂê´ËøáÁ®ãÂèçÊÄùÂíåÂÖÉËÆ§Áü•ÂàÜÊûê",
            "format_requirements": "Â∫îÂåÖÂê´ÊÄùÁª¥ËøáÁ®ãÂàÜÊûê„ÄÅÂ≠¶‰π†Êî∂Ëé∑„ÄÅÊîπËøõÊñπÂêë",
        }

    def _build_analysis_template_params(
        self,
        session: SessionState,
        step_name: str,
        step_result: str,
        analysis_type: str,
    ) -> Dict[str, Any]:
        """Build comprehensive parameters for analysis templates"""
        base_params = {
            "step_name": step_name,
            "step_result": step_result,
            "analysis_type": analysis_type,
            "session_context": session.context,
            "topic": session.topic,
        }

        # Add step-specific parameters
        if step_name == "decompose_problem":
            base_params.update(
                {
                    "original_topic": session.topic,
                    "complexity": session.context.get("complexity", "moderate"),
                }
            )
        elif step_name == "collect_evidence":
            base_params.update(
                {
                    "sub_question": self._extract_sub_question_from_context(session),
                    "search_keywords": self._extract_keywords_from_result(step_result),
                }
            )
        elif step_name == "multi_perspective_debate":
            base_params.update(
                {
                    "debate_topic": self._extract_debate_topic(session, step_result),
                    "evidence_context": self._get_evidence_context_summary(session),
                }
            )
        elif step_name in ["critical_evaluation", "bias_detection"]:
            base_params.update(
                {
                    "evaluated_content": self._get_evaluation_target(session),
                    "evaluation_context": session.context.get("focus", "ÁªºÂêàÂàÜÊûê"),
                }
            )
        elif step_name == "reflection":
            base_params.update(
                {
                    "reflection_topic": session.topic,
                    "thinking_history": self._get_previous_steps_summary(session),
                    "current_conclusions": self._extract_current_conclusions(session),
                }
            )

        return base_params

    def _get_quality_threshold(self, step_name: str, flow_type: str) -> float:
        """Get quality threshold for specific step and flow type"""
        # Default thresholds by step type
        default_thresholds = {
            "decompose_problem": 7.0,
            "collect_evidence": 7.5,
            "multi_perspective_debate": 7.0,
            "critical_evaluation": 8.0,
            "bias_detection": 7.5,
            "innovation_thinking": 6.5,
            "reflection": 7.0,
        }

        # Adjust for flow type
        if flow_type == "comprehensive_analysis":
            return default_thresholds.get(step_name, 7.0)
        elif flow_type == "quick_analysis":
            return default_thresholds.get(step_name, 6.0) - 0.5
        else:
            return default_thresholds.get(step_name, 7.0)

    def _generate_improvement_suggestions(
        self, step_name: str, step_result: str, context: Dict[str, Any]
    ) -> str:
        """Generate step-specific improvement suggestions"""
        suggestions = []

        if step_name == "decompose_problem":
            if len(step_result) < 500:
                suggestions.append("ÈóÆÈ¢òÂàÜËß£Â∫îÊõ¥Âä†ËØ¶ÁªÜÔºåÊèê‰æõÊõ¥Â§öÂ≠êÈóÆÈ¢òÂíåÂàÜÊûêËßíÂ∫¶")
            if "priority" not in step_result:
                suggestions.append("Â∫î‰∏∫ÊØè‰∏™Â≠êÈóÆÈ¢òËÆæÂÆö‰ºòÂÖàÁ∫ß")
            if "search_keywords" not in step_result:
                suggestions.append("Â∫î‰∏∫ÊØè‰∏™Â≠êÈóÆÈ¢òÊèê‰æõÊêúÁ¥¢ÂÖ≥ÈîÆËØç")

        elif step_name == "collect_evidence":
            if "http" not in step_result and "www" not in step_result:
                suggestions.append("Â∫îÂåÖÂê´ÂÖ∑‰ΩìÁöÑÁΩëÁªúÊù•Ê∫êÈìæÊé•")
            if step_result.count("Êù•Ê∫ê") < 3:
                suggestions.append("Â∫îÊî∂ÈõÜÊõ¥Â§ö‰∏çÂêåÊù•Ê∫êÁöÑËØÅÊçÆ")
            if "ÂèØ‰ø°Â∫¶" not in step_result:
                suggestions.append("Â∫îÂØπÊØè‰∏™ËØÅÊçÆÊù•Ê∫êËøõË°åÂèØ‰ø°Â∫¶ËØÑ‰º∞")

        elif step_name == "multi_perspective_debate":
            if step_result.count("ËßÇÁÇπ") < 3:
                suggestions.append("Â∫îÂåÖÂê´Êõ¥Â§ö‰∏çÂêåËßíÂ∫¶ÁöÑËßÇÁÇπ")
            if "ÂèçÈ©≥" not in step_result and "Ë¥®Áñë" not in step_result:
                suggestions.append("Â∫îÂåÖÂê´ËßÇÁÇπÈó¥ÁöÑÁõ∏‰∫íË¥®ÁñëÂíåÂèçÈ©≥")

        elif step_name in ["critical_evaluation", "bias_detection"]:
            if "ËØÑÂàÜ" not in step_result:
                suggestions.append("Â∫îÂåÖÂê´ÂÖ∑‰ΩìÁöÑÈáèÂåñËØÑÂàÜ")
            if "ÊîπËøõÂª∫ËÆÆ" not in step_result:
                suggestions.append("Â∫îÊèê‰æõÂÖ∑‰ΩìÁöÑÊîπËøõÂª∫ËÆÆ")

        elif step_name == "reflection":
            if len(step_result) < 400:
                suggestions.append("ÂèçÊÄùÂ∫îÊõ¥Âä†Ê∑±ÂÖ•ÂíåËØ¶ÁªÜ")
            if "Â≠¶Âà∞" not in step_result and "Êî∂Ëé∑" not in step_result:
                suggestions.append("Â∫îÊòéÁ°ÆËØ¥ÊòéÂ≠¶‰π†Êî∂Ëé∑ÂíåÊ¥ûÂØü")

        return (
            "\n".join(f"- {suggestion}" for suggestion in suggestions)
            if suggestions
            else "ÂΩìÂâçÁªìÊûúË¥®ÈáèËâØÂ•ΩÔºåÂª∫ËÆÆ‰øùÊåÅ"
        )

    def _get_next_step_recommendation(
        self, step_name: str, session: SessionState
    ) -> str:
        """Get recommendation for next step based on current step"""
        recommendations = {
            "decompose_problem": "Â¶ÇÊûúË¥®ÈáèËææÊ†áÔºåÂª∫ËÆÆÁªßÁª≠ËØÅÊçÆÊî∂ÈõÜÊ≠•È™§",
            "collect_evidence": "Â¶ÇÊûúËØÅÊçÆÂÖÖÂàÜÔºåÂª∫ËÆÆËøõË°åÂ§öËßíÂ∫¶Ëæ©ËÆ∫ÂàÜÊûê",
            "multi_perspective_debate": "Âª∫ËÆÆËøõË°åÊâπÂà§ÊÄßËØÑ‰º∞ÔºåÊ£ÄÊü•ËÆ∫ËØÅË¥®Èáè",
            "critical_evaluation": "Â¶ÇÊûúËØÑ‰º∞ÈÄöËøáÔºåÂèØ‰ª•ËøõË°åÂÅèËßÅÊ£ÄÊµãÊàñÂàõÊñ∞ÊÄùÁª¥",
            "bias_detection": "Âª∫ËÆÆËøõË°åÂèçÊÄùÊ≠•È™§ÔºåÊÄªÁªìÊÄùÁª¥ËøáÁ®ã",
            "innovation_thinking": "Âª∫ËÆÆËøõË°åÊúÄÁªàÂèçÊÄùÂíåÊÄªÁªì",
            "reflection": "ÂèØ‰ª•Ë∞ÉÁî®complete_thinkingÁîüÊàêÊúÄÁªàÊä•Âëä",
        }
        return recommendations.get(step_name, "Ê†πÊçÆË¥®ÈáèËØÑ‰º∞ÁªìÊûúÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•")

    def _generate_analysis_instructions(
        self, step_name: str, analysis_type: str, quality_threshold: float
    ) -> str:
        """Generate step-specific analysis instructions"""
        base_instruction = f"ËØ∑ÊåâÁÖß{step_name}Ê≠•È™§ÁöÑ‰∏ìÈó®ËØÑ‰º∞Ê†áÂáÜËøõË°åËØ¶ÁªÜÂàÜÊûê"

        quality_instruction = f"Ë¥®ÈáèÈó®ÊéßÊ†áÂáÜ‰∏∫{quality_threshold}/10ÂàÜÔºåËØ∑‰∏•Ê†ºËØÑ‰º∞"

        step_specific = {
            "decompose_problem": "ÈáçÁÇπÂÖ≥Ê≥®ÈóÆÈ¢òÂàÜËß£ÁöÑÂÆåÊï¥ÊÄß„ÄÅÁã¨Á´ãÊÄßÂíåÂèØÊìç‰ΩúÊÄß",
            "collect_evidence": "ÈáçÁÇπËØÑ‰º∞ËØÅÊçÆÊù•Ê∫êÁöÑÂ§öÊ†∑ÊÄß„ÄÅÂèØ‰ø°Â∫¶ÂíåÁõ∏ÂÖ≥ÊÄß",
            "multi_perspective_debate": "ÈáçÁÇπÂàÜÊûêËßÇÁÇπÁöÑÂ§öÊ†∑ÊÄß„ÄÅËÆ∫ËØÅË¥®ÈáèÂíå‰∫íÂä®Ê∑±Â∫¶",
            "critical_evaluation": "ÈáçÁÇπÊ£ÄÊü•ËØÑ‰º∞Ê†áÂáÜÁöÑÂ∫îÁî®ÂíåËØÑÂàÜÁöÑÂáÜÁ°ÆÊÄß",
            "reflection": "ÈáçÁÇπËØÑ‰º∞ÂèçÊÄùÁöÑÊ∑±Â∫¶ÂíåÂÖÉËÆ§Áü•Ë¥®Èáè",
        }

        specific_instruction = step_specific.get(step_name, "ËØ∑ËøõË°åÂÖ®Èù¢ÁöÑË¥®ÈáèÂàÜÊûê")

        return f"{base_instruction}„ÄÇ{quality_instruction}„ÄÇ{specific_instruction}„ÄÇËØ∑Êèê‰æõÂÖ∑‰ΩìÁöÑÊîπËøõÂª∫ËÆÆÂíå‰∏ã‰∏ÄÊ≠•Âª∫ËÆÆ„ÄÇ"

    def _determine_analysis_next_action(
        self, step_name: str, session: SessionState
    ) -> str:
        """Determine next action after analysis"""
        return f"Ê†πÊçÆ{step_name}Ê≠•È™§ÁöÑÂàÜÊûêÁªìÊûúÔºåÂ¶ÇÊûúË¥®ÈáèËææÊ†áÂàôÁªßÁª≠‰∏ã‰∏ÄÊ≠•ÔºåÂê¶ÂàôÈúÄË¶ÅÊîπËøõÂΩìÂâçÊ≠•È™§"

    def _get_analysis_criteria_count(self, step_name: str) -> int:
        """Get number of analysis criteria for the step"""
        criteria_counts = {
            "decompose_problem": 5,
            "collect_evidence": 5,
            "multi_perspective_debate": 5,
            "critical_evaluation": 5,
            "reflection": 5,
        }
        return criteria_counts.get(step_name, 5)

    def _handle_format_validation_failure(
        self, session_id: str, step_name: str, validation_result: Dict[str, Any]
    ) -> MCPToolOutput:
        """Handle format validation failure using error handler"""
        error = MCPFormatValidationError(
            f"Format validation failed for step {step_name}",
            step_name=step_name,
            expected_format=validation_result["expected_format"],
        )

        return self.error_handler.handle_mcp_error(
            tool_name="format_validator",
            error=error,
            session_id=session_id,
            context={
                "step_name": step_name,
                "format_issues": validation_result["issues"],
                "expected_format": validation_result["expected_format"],
            },
        )

    def _get_format_example(self, step_name: str) -> str:
        """Get format example for specific step"""
        examples = {
            "decompose_problem": """
{
  "main_question": "Â¶Ç‰ΩïÊèêÈ´òÊïôËÇ≤Ë¥®ÈáèÔºü",
  "sub_questions": [
    {
      "id": "1",
      "question": "ÂΩìÂâçÊïôËÇ≤‰ΩìÁ≥ªÂ≠òÂú®Âì™‰∫õ‰∏ªË¶ÅÈóÆÈ¢òÔºü",
      "priority": "high",
      "search_keywords": ["ÊïôËÇ≤ÈóÆÈ¢ò", "ÊïôËÇ≤‰ΩìÁ≥ª", "ÊïôÂ≠¶Ë¥®Èáè"],
      "expected_perspectives": ["Â≠¶ÁîüËßÜËßí", "ÊïôÂ∏àËßÜËßí", "ÂÆ∂ÈïøËßÜËßí"]
    }
  ],
  "relationships": ["ÈóÆÈ¢ò1ÊòØÈóÆÈ¢ò2ÁöÑÂâçÊèê"]
}""",
            "collect_evidence": """
ËØÅÊçÆÊù•Ê∫ê1Ôºö
- Ê†áÈ¢òÔºöÊïôËÇ≤Ë¥®ÈáèÁ†îÁ©∂Êä•Âëä
- URLÔºöhttps://example.com/report
- ÂèØ‰ø°Â∫¶Ôºö8/10
- ÂÖ≥ÈîÆÂèëÁé∞Ôºö...

ËØÅÊçÆÊù•Ê∫ê2Ôºö
- Ê†áÈ¢òÔºö‰∏ìÂÆ∂ËÆøË∞à
- URLÔºöhttps://example.com/interview  
- ÂèØ‰ø°Â∫¶Ôºö9/10
- ÂÖ≥ÈîÆÂèëÁé∞Ôºö...""",
            "multi_perspective_debate": """
ÊîØÊåÅÊñπËßÇÁÇπÔºö
- Ê†∏ÂøÉËÆ∫ÁÇπÔºö...
- ÊîØÊåÅËÆ∫ÊçÆÔºö...

ÂèçÂØπÊñπËßÇÁÇπÔºö
- Ê†∏ÂøÉËÆ∫ÁÇπÔºö...
- ÂèçÈ©≥ËÆ∫ÊçÆÔºö...

‰∏≠Á´ãÂàÜÊûêÔºö
- Âπ≥Ë°°ËßÇÁÇπÔºö...
- ÁªºÂêàËØÑ‰º∞Ôºö...""",
        }
        return examples.get(step_name, "ËØ∑ÂèÇËÄÉÊ≠•È™§Ë¶ÅÊ±ÇÁöÑÊ†áÂáÜÊ†ºÂºè")

    def _get_common_format_errors(self, step_name: str) -> str:
        """Get common format errors for specific step"""
        errors = {
            "decompose_problem": "Â∏∏ËßÅÈîôËØØÔºöÂøòËÆ∞JSONÊ†ºÂºè„ÄÅÁº∫Â∞ëÂøÖÈúÄÂ≠óÊÆµ„ÄÅÂ≠êÈóÆÈ¢òÊèèËø∞Ëøá‰∫éÁÆÄÂçï",
            "collect_evidence": "Â∏∏ËßÅÈîôËØØÔºöÁº∫Â∞ëÊù•Ê∫êÈìæÊé•„ÄÅÊ≤°ÊúâÂèØ‰ø°Â∫¶ËØÑ‰º∞„ÄÅËØÅÊçÆËøá‰∫éÁÆÄÂçï",
            "multi_perspective_debate": "Â∏∏ËßÅÈîôËØØÔºöËßÇÁÇπÂçï‰∏Ä„ÄÅÁº∫Â∞ë‰∫íÂä®„ÄÅËÆ∫ÊçÆ‰∏çÂÖÖÂàÜ",
        }
        return errors.get(step_name, "ËØ∑Á°Æ‰øùÊ†ºÂºèÂÆåÊï¥ÂíåËßÑËåÉ")

    # Helper methods for extracting information from session context
    def _extract_sub_question_from_context(self, session: SessionState) -> str:
        """Extract sub-question from session context"""
        # This would extract from previous decomposition results
        return session.context.get("current_sub_question", "Âü∫‰∫éÈóÆÈ¢òÂàÜËß£ÁöÑÂ≠êÈóÆÈ¢ò")

    def _extract_keywords_from_result(self, step_result: str) -> str:
        """Extract keywords from step result"""
        # Simple keyword extraction - in practice this could be more sophisticated
        return "Áõ∏ÂÖ≥ÊêúÁ¥¢ÂÖ≥ÈîÆËØç"

    def _extract_debate_topic(self, session: SessionState, step_result: str) -> str:
        """Extract debate topic from context"""
        return session.context.get("debate_topic", session.topic)

    def _get_evidence_context_summary(self, session: SessionState) -> str:
        """Get summary of evidence collection context"""
        return "Âü∫‰∫éËØÅÊçÆÊî∂ÈõÜÁöÑËÉåÊôØ‰ø°ÊÅØ"

    def _get_evaluation_target(self, session: SessionState) -> str:
        """Get the target content for evaluation"""
        return "ÈúÄË¶ÅËØÑ‰º∞ÁöÑÂÜÖÂÆπ"

    def _extract_current_conclusions(self, session: SessionState) -> str:
        """Extract current conclusions from session"""
        return "Âü∫‰∫éÂâçÈù¢Ê≠•È™§ÂæóÂá∫ÁöÑÂΩìÂâçÁªìËÆ∫"

    def _handle_session_not_found(self, session_id: str) -> MCPToolOutput:
        """Handle case where session is not found using error handler"""
        error = SessionNotFoundError(
            f"Session {session_id} not found", session_id=session_id
        )

        return self.error_handler.handle_mcp_error(
            tool_name="session_manager",
            error=error,
            session_id=session_id,
            context={"session_id": session_id},
        )

    def _handle_flow_completion(self, session_id: str) -> MCPToolOutput:
        """Handle flow completion with safety checks"""
        # SAFETY CHECK: Verify no active for_each iterations before completing
        session = self.session_manager.get_session(session_id)
        if session:
            for step_name, current_count in session.iteration_count.items():
                total_count = session.total_iterations.get(step_name, 0)
                if current_count < total_count and total_count > 0:
                    logger.error(f"üö® CRITICAL: Attempted completion with active for_each: {step_name} at {current_count}/{total_count}")
                    raise RuntimeError(f"Cannot complete flow with active for_each iterations: {step_name} {current_count}/{total_count}")
        
        logger.info("üèÅ CONFIRMED SAFE COMPLETION: All iterations verified complete")
        completion_prompt = self.template_manager.get_template(
            "flow_completion", {"session_id": session_id}
        )

        return MCPToolOutput(
            tool_name=MCPToolName.COMPLETE_THINKING,
            session_id=session_id,
            step="flow_completed",
            prompt_template=completion_prompt,
            instructions="ÊÄùÁª¥ÊµÅÁ®ãÂ∑≤ÂÆåÊàêÔºåÂáÜÂ§áÁîüÊàêÊúÄÁªàÊä•Âëä",
            context={"flow_completed": True},
            next_action="Ë∞ÉÁî®complete_thinkingÁîüÊàêÊúÄÁªàÊä•Âëä",
            metadata={
                "ready_for_completion": True,
                "completion_verified": True,
                "all_iterations_complete": True
            },
        )

    def _handle_error(
        self, tool_name: str, error_message: str, session_id: Optional[str] = None
    ) -> MCPToolOutput:
        """Handle tool execution errors using the error handler"""
        # Create a generic exception for the error handler
        error = MCPToolExecutionError(
            error_message, tool_name=tool_name, session_id=session_id
        )

        # Use the error handler to create appropriate recovery response
        return self.error_handler.handle_mcp_error(
            tool_name=tool_name,
            error=error,
            session_id=session_id,
            context={"error_message": error_message},
        )

    def _validate_completion_eligibility(self, session: SessionState) -> Dict[str, Any]:
        """
        Validate whether the session is actually eligible for completion
        CRITICAL: This prevents HOST from bypassing for_each iterations
        """
        try:
            # Check all for_each steps for incomplete iterations
            for step_name, current_count in session.iteration_count.items():
                total_count = session.total_iterations.get(step_name, 0)
                
                if current_count < total_count and total_count > 0:
                    # Found incomplete for_each - completion not allowed
                    remaining = total_count - current_count
                    
                    return {
                        "allowed": False,
                        "reason": f"Active for_each in {step_name}: {current_count}/{total_count} (Áº∫Â∞ë{remaining}‰∏™)",
                        "current_state": f"{step_name} at {current_count}/{total_count}",
                        "required_step": step_name,
                        "continuation_template": self.template_manager.get_template(
                            "evidence_collection", {"sub_question": f"Á¨¨{current_count + 1}‰∏™Â≠êÈóÆÈ¢ò"}
                        ),
                        "continuation_instruction": f"ÂøÖÈ°ªÂÆåÊàêÂâ©‰Ωô{remaining}‰∏™Â≠êÈóÆÈ¢òÁöÑ{step_name}Â§ÑÁêÜ",
                        "next_action": f"ÁªßÁª≠Â§ÑÁêÜÁ¨¨{current_count + 1}‰∏™Â≠êÈóÆÈ¢ò",
                        "current_iterations": current_count,
                        "total_iterations": total_count,
                        "iteration_status": {
                            "current": current_count,
                            "total": total_count,
                            "remaining": remaining,
                            "step": step_name
                        }
                    }
            
            # All for_each iterations complete - completion allowed
            return {
                "allowed": True,
                "reason": "All for_each iterations completed",
                "validation_passed": True
            }
            
        except Exception as e:
            logger.error(f"Error validating completion eligibility: {e}")
            # Default to blocking completion if validation fails
            return {
                "allowed": False,
                "reason": f"Validation error: {e}",
                "error_occurred": True
            }

    # Enhanced helper methods for complete_thinking tool

    def _calculate_comprehensive_quality_metrics(
        self, session: SessionState
    ) -> Dict[str, Any]:
        """
        Calculate comprehensive quality metrics for the session

        Returns detailed quality analysis including:
        - Average quality score
        - Quality trend analysis
        - Step-by-step quality breakdown
        - Quality distribution
        - Improvement recommendations
        """
        quality_scores = session.quality_scores

        if not quality_scores:
            return {
                "average_quality": 0.0,
                "quality_trend": "no_data",
                "total_steps": session.step_number,
                "high_quality_steps": 0,
                "quality_distribution": {},
                "improvement_areas": ["No quality data available"],
                "overall_assessment": "insufficient_data",
            }

        # Calculate basic metrics
        scores = list(quality_scores.values())
        average_quality = sum(scores) / len(scores)
        high_quality_steps = sum(1 for score in scores if score >= 8.0)

        # Quality trend analysis
        quality_trend = self._analyze_quality_trend(scores)

        # Quality distribution
        quality_distribution = {
            "excellent": sum(1 for score in scores if score >= 9.0),
            "good": sum(1 for score in scores if 7.0 <= score < 9.0),
            "acceptable": sum(1 for score in scores if 5.0 <= score < 7.0),
            "needs_improvement": sum(1 for score in scores if score < 5.0),
        }

        # Identify improvement areas
        improvement_areas = self._identify_improvement_areas(quality_scores)

        # Overall assessment
        overall_assessment = self._determine_overall_assessment(
            average_quality, quality_distribution
        )

        return {
            "average_quality": round(average_quality, 2),
            "quality_trend": quality_trend,
            "total_steps": session.step_number,
            "high_quality_steps": high_quality_steps,
            "quality_distribution": quality_distribution,
            "step_quality_breakdown": quality_scores,
            "improvement_areas": improvement_areas,
            "overall_assessment": overall_assessment,
            "quality_consistency": self._calculate_quality_consistency(scores),
            "best_performing_step": (
                max(quality_scores.items(), key=lambda x: x[1])
                if quality_scores
                else None
            ),
            "lowest_performing_step": (
                min(quality_scores.items(), key=lambda x: x[1])
                if quality_scores
                else None
            ),
        }

    def _analyze_quality_trend(self, scores: List[float]) -> str:
        """Analyze the trend in quality scores"""
        if len(scores) < 2:
            return "insufficient_data"

        # Simple trend analysis
        first_half = scores[: len(scores) // 2]
        second_half = scores[len(scores) // 2 :]

        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)

        diff = second_avg - first_avg

        if diff > 0.5:
            return "improving"
        elif diff < -0.5:
            return "declining"
        else:
            return "stable"

    def _identify_improvement_areas(
        self, quality_scores: Dict[str, float]
    ) -> List[str]:
        """Identify areas that need improvement based on quality scores"""
        improvement_areas = []

        for step_name, score in quality_scores.items():
            if score < 6.0:
                improvement_areas.append(f"{step_name} (ÂæóÂàÜ: {score})")

        if not improvement_areas:
            improvement_areas.append("ÊâÄÊúâÊ≠•È™§Ë¥®ÈáèËâØÂ•Ω")

        return improvement_areas

    def _determine_overall_assessment(
        self, average_quality: float, quality_distribution: Dict[str, int]
    ) -> str:
        """Determine overall quality assessment"""
        if average_quality >= 8.5:
            return "excellent"
        elif average_quality >= 7.0:
            return "good"
        elif average_quality >= 5.0:
            return "acceptable"
        else:
            return "needs_improvement"

    def _calculate_quality_consistency(self, scores: List[float]) -> float:
        """Calculate quality consistency (lower variance = higher consistency)"""
        if len(scores) < 2:
            return 1.0

        mean = sum(scores) / len(scores)
        variance = sum((score - mean) ** 2 for score in scores) / len(scores)

        # Convert to consistency score (0-1, higher is more consistent)
        consistency = max(0, 1 - (variance / 10))  # Normalize variance
        return round(consistency, 3)

    def _generate_detailed_session_summary(
        self, session: SessionState
    ) -> Dict[str, Any]:
        """Generate detailed session summary with step analysis"""

        # Get step details from database
        try:
            steps = self.session_manager.db.get_session_steps(session.session_id)
            results = self.session_manager.db.get_step_results(session.session_id)
        except Exception:
            steps = []
            results = []

        # Organize results by step
        step_results_map = {}
        for result in results:
            step_id = result.get("step_id")
            if step_id not in step_results_map:
                step_results_map[step_id] = []
            step_results_map[step_id].append(result)

        # Build detailed step summary
        detailed_steps = []
        for step in steps:
            step_summary = {
                "step_name": step.get("step_name", "unknown"),
                "step_type": step.get("step_type", "general"),
                "quality_score": step.get("quality_score"),
                "execution_time_ms": step.get("execution_time_ms"),
                "results_count": len(step_results_map.get(step.get("id"), [])),
                "timestamp": step.get("created_at"),
            }
            detailed_steps.append(step_summary)

        return {
            "session_id": session.session_id,
            "topic": session.topic,
            "flow_type": session.flow_type,
            "total_steps": len(detailed_steps),
            "session_duration": self._calculate_session_duration_minutes(session),
            "detailed_steps": detailed_steps,
            "context_summary": session.context,
            "completion_status": "completed",
        }

    def _calculate_session_duration_minutes(self, session: SessionState) -> float:
        """Calculate session duration in minutes"""
        if session.created_at and session.updated_at:
            duration = (session.updated_at - session.created_at).total_seconds() / 60
            return round(duration, 2)
        return 0.0

    def _build_comprehensive_summary_params(
        self,
        session: SessionState,
        quality_metrics: Dict[str, Any],
        session_summary: Dict[str, Any],
        thinking_trace: Dict[str, Any],
        final_insights: Optional[str],
    ) -> Dict[str, Any]:
        """Build comprehensive parameters for the summary template"""

        # Format quality metrics for display
        quality_display = self._format_quality_metrics_for_display(quality_metrics)

        # Format step summary for display
        step_summary_display = self._format_step_summary_for_display(session_summary)

        # Format thinking trace for display
        trace_display = self._format_thinking_trace_for_display(thinking_trace)

        return {
            "topic": session.topic,
            "flow_type": session.flow_type,
            "session_duration": f"{session_summary['session_duration']} ÂàÜÈíü",
            "total_steps": session_summary["total_steps"],
            "step_summary": step_summary_display,
            "thinking_trace": trace_display,
            "quality_metrics": quality_display,
            "final_insights": final_insights or "Êó†È¢ùÂ§ñÊ¥ûÂØü",
            "completion_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "overall_assessment": quality_metrics.get("overall_assessment", "unknown"),
            "average_quality": quality_metrics.get("average_quality", 0),
            "improvement_areas": "\n".join(
                quality_metrics.get("improvement_areas", [])
            ),
            "best_step": (
                quality_metrics.get("best_performing_step", ["Êó†", 0])[0]
                if quality_metrics.get("best_performing_step")
                else "Êó†"
            ),
            "session_context": session.context,
        }

    def _format_quality_metrics_for_display(
        self, quality_metrics: Dict[str, Any]
    ) -> str:
        """Format quality metrics for template display"""
        if not quality_metrics:
            return "Êó†Ë¥®ÈáèÊï∞ÊçÆ"

        lines = [
            f"Âπ≥ÂùáË¥®ÈáèÂæóÂàÜ: {quality_metrics.get('average_quality', 0)}/10",
            f"Ë¥®ÈáèË∂ãÂäø: {quality_metrics.get('quality_trend', 'unknown')}",
            f"È´òË¥®ÈáèÊ≠•È™§Êï∞: {quality_metrics.get('high_quality_steps', 0)}",
            f"ÊÄª‰ΩìËØÑ‰º∞: {quality_metrics.get('overall_assessment', 'unknown')}",
            f"Ë¥®Èáè‰∏ÄËá¥ÊÄß: {quality_metrics.get('quality_consistency', 0)}",
        ]

        if quality_metrics.get("best_performing_step"):
            best_step, best_score = quality_metrics["best_performing_step"]
            lines.append(f"ÊúÄ‰Ω≥Ê≠•È™§: {best_step} ({best_score}/10)")

        return "\n".join(lines)

    def _format_step_summary_for_display(self, session_summary: Dict[str, Any]) -> str:
        """Format step summary for template display"""
        if not session_summary.get("detailed_steps"):
            return "Êó†Ê≠•È™§Êï∞ÊçÆ"

        lines = []
        for step in session_summary["detailed_steps"]:
            step_name = step.get("step_name", "unknown")
            quality = step.get("quality_score", "N/A")
            lines.append(f"- {step_name}: ÂÆåÊàê (Ë¥®Èáè: {quality})")

        return "\n".join(lines)

    def _format_thinking_trace_for_display(self, thinking_trace: Dict[str, Any]) -> str:
        """Format thinking trace for template display"""
        if not thinking_trace or thinking_trace.get("error"):
            return "ÊÄùÁª¥ËΩ®Ëøπ‰∏çÂèØÁî®"

        lines = [
            f"‰ºöËØùID: {thinking_trace.get('session_id', 'unknown')}",
            f"ÊµÅÁ®ãÁ±ªÂûã: {thinking_trace.get('flow_type', 'unknown')}",
            f"ÊÄªÊó∂Èïø: {thinking_trace.get('total_duration', 0)} Áßí",
            f"Ê≠•È™§Êï∞Èáè: {len(thinking_trace.get('steps', []))}",
        ]

        return "\n".join(lines)

    def _generate_completion_instructions(
        self, quality_metrics: Dict[str, Any], session: SessionState
    ) -> str:
        """Generate detailed instructions for final report generation"""
        base_instruction = "ËØ∑ÁîüÊàêËØ¶ÁªÜÁöÑÁªºÂêàÊä•ÂëäÔºåÂåÖÂê´ÊâÄÊúâÂÖ≥ÈîÆÂèëÁé∞ÂíåÊ¥ûÂØü"

        # Add quality-specific guidance
        quality_guidance = []

        overall_assessment = quality_metrics.get("overall_assessment", "unknown")
        if overall_assessment == "excellent":
            quality_guidance.append("Ë¥®Èáè‰ºòÁßÄÔºåÈáçÁÇπÁ™ÅÂá∫Ê†∏ÂøÉÊ¥ûÂØüÂíåÂàõÊñ∞ËßÇÁÇπ")
        elif overall_assessment == "good":
            quality_guidance.append("Ë¥®ÈáèËâØÂ•ΩÔºåÁ°Æ‰øùÊ∂µÁõñÊâÄÊúâÈáçË¶ÅÂèëÁé∞")
        elif overall_assessment == "acceptable":
            quality_guidance.append("Ë¥®ÈáèÂèØÊé•ÂèóÔºåÊ≥®ÊÑèË°•ÂÖÖÂàÜÊûêÊ∑±Â∫¶")
        else:
            quality_guidance.append("Ë¥®ÈáèÈúÄË¶ÅÊîπËøõÔºåÈáçÁÇπÂä†Âº∫ËÆ∫ËØÅÂíåËØÅÊçÆÊîØÊíë")

        # Add step-specific guidance
        if session.step_number > 5:
            quality_guidance.append("ÊµÅÁ®ãËæÉ‰∏∫ÂÆåÊï¥ÔºåÁ°Æ‰øùÂêÑÊ≠•È™§ÁªìÊûúÁöÑÊúâÊú∫Êï¥Âêà")

        # Add improvement area guidance
        improvement_areas = quality_metrics.get("improvement_areas", [])
        if improvement_areas and improvement_areas[0] != "ÊâÄÊúâÊ≠•È™§Ë¥®ÈáèËâØÂ•Ω":
            quality_guidance.append(
                f"ÁâπÂà´ÂÖ≥Ê≥®‰ª•‰∏ãÊîπËøõÈ¢ÜÂüü: {', '.join(improvement_areas[:2])}"
            )

        if quality_guidance:
            return f"{base_instruction}„ÄÇ{' '.join(quality_guidance)}"

        return base_instruction
