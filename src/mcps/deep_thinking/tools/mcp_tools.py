"""
Core MCP tools for the Deep Thinking Engine

These tools follow the zero-cost principle:
- MCP Server provides flow control and prompt templates
- Host LLM performs the actual intelligent processing
- No LLM API calls from the server side
"""

import logging
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..config.exceptions import (
    MCPFormatValidationError,
    MCPToolExecutionError,
    SessionNotFoundError,
)
from ..flows.flow_manager import FlowManager
from ..models.mcp_models import (
    AnalyzeStepInput,
    CompleteThinkingInput,
    MCPToolName,
    MCPToolOutput,
    NextStepInput,
    SessionState,
    StartThinkingInput,
)
from ..sessions.session_manager import SessionManager
from ..templates.template_manager import TemplateManager
from .mcp_error_handler import MCPErrorHandler

logger = logging.getLogger(__name__)


class MCPTools:
    """
    Core MCP tools that return prompt templates for LLM execution

    Following the zero-cost principle:
    - Server handles flow control and template management
    - LLM handles intelligent processing and content generation
    """

    def __init__(
        self,
        session_manager: SessionManager,
        template_manager: TemplateManager,
        flow_manager: FlowManager,
    ):
        self.session_manager = session_manager
        self.template_manager = template_manager
        self.flow_manager = flow_manager
        self.error_handler = MCPErrorHandler(session_manager, template_manager)
        # Track active sessions to prevent state inconsistencies  
        self._active_sessions = {}

    def start_thinking(self, input_data: StartThinkingInput) -> MCPToolOutput:
        """
        Start a new deep thinking session

        Returns a problem decomposition prompt template for the LLM to execute
        """
        try:
            # Create new session
            session_id = str(uuid.uuid4())
            session_state = SessionState(
                session_id=session_id,
                topic=input_data.topic,
                current_step="decompose_problem",
                flow_type=input_data.flow_type,
                context={
                    "complexity": input_data.complexity,
                    "focus": input_data.focus,
                    "original_topic": input_data.topic,
                },
            )

            # Save session state
            self.session_manager.create_session(session_state)

            # Get decomposition prompt template
            template_params = {
                "topic": input_data.topic,
                "complexity": input_data.complexity,
                "focus": input_data.focus,
                "domain_context": input_data.focus or "general analysis",
            }

            prompt_template = self.template_manager.get_template(
                "decomposition", template_params
            )

            return MCPToolOutput(
                tool_name=MCPToolName.START_THINKING,
                session_id=session_id,
                step="decompose_problem",
                prompt_template=prompt_template,
                instructions="è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†è§£ç»“æœï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ",
                context={
                    "session_id": session_id,
                    "topic": input_data.topic,
                    "complexity": input_data.complexity,
                },
                next_action="è°ƒç”¨next_stepå·¥å…·ç»§ç»­æµç¨‹",
                metadata={
                    "flow_type": input_data.flow_type,
                    "expected_output": "JSONæ ¼å¼çš„é—®é¢˜åˆ†è§£ç»“æœ",
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="start_thinking",
                    error=e,
                    session_id=None,
                    context={
                        "topic": input_data.topic,
                        "complexity": input_data.complexity,
                    },
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "start_thinking", e, None
                )

    def next_step(self, input_data: NextStepInput) -> MCPToolOutput:
        """
        Get the next step in the thinking process

        Returns appropriate prompt template based on current flow state
        Implements enhanced flow control and template selection logic
        """
        try:
            # Get current session state
            try:
                session = self.session_manager.get_session(input_data.session_id)
                if session is None:
                    raise SessionNotFoundError(
                        "Session not found", session_id=input_data.session_id
                    )
            except Exception as e:
                # Session not found - use error handler for consistent handling
                return self.error_handler.handle_mcp_error(
                    tool_name="next_step",
                    error=e,
                    session_id=input_data.session_id,
                    context={"step_result": input_data.step_result},
                )

            # Extract quality score from step result if provided
            quality_score = None
            if (
                input_data.quality_feedback
                and "quality_score" in input_data.quality_feedback
            ):
                quality_score = input_data.quality_feedback["quality_score"]

            # Save previous step result with enhanced context
            # IMPORTANT: Don't auto-increment for_each here, let the flow manager decide
            self.session_manager.add_step_result(
                input_data.session_id,
                session.current_step,
                input_data.step_result,
                result_type="output",
                metadata={
                    "step_completion_time": datetime.now().isoformat(),
                    "quality_feedback": input_data.quality_feedback,
                    "step_context": session.context,
                    "for_each_continuation": False,  # Don't auto-increment
                },
                quality_score=quality_score,
            )

            # CRITICAL: Determine next step and handle for_each iteration properly
            next_step_info = self._determine_next_step_with_context(
                session, input_data.step_result, input_data.quality_feedback
            )
            
            # IMPORTANT: If we're continuing for_each, increment the iteration EXACTLY ONCE
            if next_step_info and next_step_info.get("for_each_continuation"):
                # Only increment if we haven't already processed this sub-question
                current_iterations = session.iteration_count.get(session.current_step, 0)
                total_iterations = session.total_iterations.get(session.current_step, 0)
                
                logger.info(f"FOR_EACH CONTINUATION: {session.current_step} at {current_iterations}/{total_iterations}")
                
                # Increment iteration counter for the NEXT sub-question
                if current_iterations < total_iterations:
                    session.iteration_count[session.current_step] = current_iterations + 1
                    new_count = session.iteration_count[session.current_step]
                    logger.info(f"INCREMENTED {session.current_step}: {current_iterations} -> {new_count}/{total_iterations}")
                    
                    # Update session state immediately in both caches
                    self._active_sessions[session.session_id] = session
                    self.session_manager._active_sessions[session.session_id] = session
                else:
                    logger.warning(f"Cannot increment {session.current_step} beyond {total_iterations}")

            if not next_step_info:
                # CRITICAL: Check if we're in the middle of for_each before completing
                session_for_completion_check = self.session_manager.get_session(input_data.session_id)
                if session_for_completion_check:
                    # Check if any step has active for_each iterations
                    for step_name, current_count in session_for_completion_check.iteration_count.items():
                        total_count = session_for_completion_check.total_iterations.get(step_name, 0)
                        if current_count < total_count and total_count > 0:
                            logger.warning(f"ğŸš¨ PREVENTED PREMATURE COMPLETION: {step_name} at {current_count}/{total_count}")
                            logger.warning("ğŸ” Flow manager returned None but for_each is still active!")
                            
                            # Force continue the for_each step
                            return {
                                "step_name": step_name,
                                "template_name": "evidence_collection",  # Default template
                                "instructions": f"ğŸš¨ æ€¥æ•‘æ¨¡å¼: ç»§ç»­å¤„ç†ç¬¬{current_count + 1}ä¸ªå­é—®é¢˜",
                                "for_each_continuation": True,
                            }
                
                # Flow completed
                logger.info("ğŸ LEGITIMATE FLOW COMPLETION: All for_each iterations completed")
                return self._handle_flow_completion(input_data.session_id)

            # Calculate correct step number for metadata based on the next step
            # For for_each continuations, we want to show the step number of the step being iterated
            if next_step_info.get("for_each_continuation"):
                # For for_each iterations, use current session step number which should already be correct
                metadata_step_number = session.step_number
            else:
                # For normal progression, increment from current step
                metadata_step_number = session.step_number + 1

            # Update session state with enhanced tracking
            # Only update step number if not for_each continuation
            if not next_step_info.get("for_each_continuation"):
                self.session_manager.update_session_step(
                    input_data.session_id,
                    next_step_info["step_name"],
                    step_result=input_data.step_result,
                    quality_score=quality_score,
                )
            else:
                # For for_each continuation, add step result without advancing step_number
                # IMPORTANT: Mark that iteration was already incremented above
                self.session_manager.add_step_result(
                    input_data.session_id,
                    session.current_step,
                    input_data.step_result,
                    result_type="output",
                    metadata={
                        "for_each_continuation": True,
                        "should_increment_iteration": False,  # Already incremented above
                        "step_completion_time": datetime.now().isoformat(),
                        "quality_feedback": input_data.quality_feedback,
                        "step_context": session.context,
                        "iteration_already_incremented": True,
                    },
                    quality_score=quality_score,
                )

            # Build enhanced template parameters with full context
            template_params = self._build_enhanced_template_params(
                session, input_data.step_result, next_step_info
            )

            # Get appropriate prompt template with smart selection
            prompt_template = self._get_contextual_template(
                next_step_info["template_name"], template_params, session
            )

            # Build comprehensive context for the next step
            step_context = self._build_step_context(
                session, next_step_info, metadata_step_number
            )

            return MCPToolOutput(
                tool_name=MCPToolName.NEXT_STEP,
                session_id=input_data.session_id,
                step=next_step_info["step_name"],
                prompt_template=prompt_template,
                instructions=self._generate_step_instructions(next_step_info, session),
                context=step_context,
                next_action=self._determine_next_action(next_step_info, session),
                metadata={
                    "step_number": metadata_step_number,
                    "flow_progress": f"{metadata_step_number}/{self.flow_manager.get_total_steps(session.flow_type)}",
                    "flow_type": session.flow_type,
                    "previous_step": session.current_step,
                    "quality_gate_passed": quality_score is None
                    or quality_score >= 0.7,
                    "template_selected": next_step_info["template_name"],
                    "context_enriched": True,
                    "for_each_continuation": next_step_info.get(
                        "for_each_continuation", False
                    ),
                    # Add explicit guidance for HOST
                    "iteration_status": {
                        "current": session.iteration_count.get(next_step_info["step_name"], 0),
                        "total": session.total_iterations.get(next_step_info["step_name"], 0),
                        "is_for_each": next_step_info.get("for_each_continuation", False)
                    }
                },
            )

        except Exception as e:
            try:
                # Provide enriched context for error recovery
                error_context = {
                    "step_result": input_data.step_result,
                    "quality_feedback": input_data.quality_feedback,
                    "session_lookup_failed": True,
                }

                # Try to get partial session info for better recovery
                try:
                    session = self.session_manager.get_session(input_data.session_id)
                    if session:
                        error_context.update(
                            {
                                "current_step": session.current_step,
                                "flow_type": session.flow_type,
                                "step_number": session.step_number,
                                "session_lookup_failed": False,
                            }
                        )
                except Exception:
                    pass  # Keep original context

                return self.error_handler.handle_mcp_error(
                    tool_name="next_step",
                    error=e,
                    session_id=input_data.session_id,
                    context=error_context,
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "next_step", e, input_data.session_id
                )

    def analyze_step(self, input_data: AnalyzeStepInput) -> MCPToolOutput:
        """
        Analyze the quality of a completed step

        Implements comprehensive quality analysis with:
        - Step-specific evaluation criteria
        - Quality gate enforcement
        - Format validation
        - Improvement suggestions generation
        """
        try:
            # Get session state
            session = self.session_manager.get_session(input_data.session_id)
            if not session:
                return self._handle_session_not_found(input_data.session_id)

            # Perform format validation first
            format_validation = self._validate_step_format(
                input_data.step_name, input_data.step_result
            )
            if not format_validation["valid"]:
                return self._handle_format_validation_failure(
                    input_data.session_id, input_data.step_name, format_validation
                )

            # Get step-specific analysis template
            analysis_template_name = self._get_analysis_template_name(
                input_data.step_name
            )

            # Build comprehensive template parameters
            template_params = self._build_analysis_template_params(
                session,
                input_data.step_name,
                input_data.step_result,
                input_data.analysis_type,
            )

            # Get quality threshold for this step
            quality_threshold = self._get_quality_threshold(
                input_data.step_name, session.flow_type
            )

            # Generate improvement suggestions based on step type
            improvement_suggestions = self._generate_improvement_suggestions(
                input_data.step_name, input_data.step_result, session.context
            )

            # Add quality gate information to template params
            template_params.update(
                {
                    "quality_threshold": quality_threshold,
                    "improvement_suggestions": improvement_suggestions,
                    "quality_gate_passed": "å¾…è¯„ä¼°",
                    "quality_level": "å¾…è¯„ä¼°",
                    "next_step_recommendation": self._get_next_step_recommendation(
                        input_data.step_name, session
                    ),
                }
            )

            # Get analysis prompt template
            prompt_template = self.template_manager.get_template(
                analysis_template_name, template_params
            )

            # Generate step-specific instructions
            instructions = self._generate_analysis_instructions(
                input_data.step_name, input_data.analysis_type, quality_threshold
            )

            return MCPToolOutput(
                tool_name=MCPToolName.ANALYZE_STEP,
                session_id=input_data.session_id,
                step=f"analyze_{input_data.step_name}",
                prompt_template=prompt_template,
                instructions=instructions,
                context={
                    "analyzed_step": input_data.step_name,
                    "analysis_type": input_data.analysis_type,
                    "quality_threshold": quality_threshold,
                    "format_validated": True,
                    "step_context": session.context,
                    "improvement_suggestions_available": True,
                },
                next_action=self._determine_analysis_next_action(
                    input_data.step_name, session
                ),
                metadata={
                    "quality_check": True,
                    "step_analyzed": input_data.step_name,
                    "analysis_template": analysis_template_name,
                    "quality_threshold": quality_threshold,
                    "format_validation_passed": True,
                    "analysis_criteria_count": self._get_analysis_criteria_count(
                        input_data.step_name
                    ),
                    "improvement_suggestions_generated": True,
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="analyze_step",
                    error=e,
                    session_id=input_data.session_id,
                    context={
                        "step_name": input_data.step_name,
                        "step_result": input_data.step_result,
                        "analysis_type": input_data.analysis_type,
                    },
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "analyze_step", e, input_data.session_id
                )

    def complete_thinking(self, input_data: CompleteThinkingInput) -> MCPToolOutput:
        """
        Complete the thinking process and generate final report

        Enhanced implementation with:
        - Comprehensive session state update and result aggregation
        - Quality metrics calculation and analysis
        - Final report template with detailed insights
        - Session completion with full trace preservation
        - CRITICAL: Validation to prevent unauthorized completion by HOST
        """
        try:
            # Get session state
            session = self.session_manager.get_session(input_data.session_id)
            if not session:
                return self._handle_session_not_found(input_data.session_id)
            
            # ğŸš¨ CRITICAL: Validate that completion is actually allowed
            completion_validation = self._validate_completion_eligibility(session)
            if not completion_validation["allowed"]:
                logger.warning(f"ğŸš« BLOCKED UNAUTHORIZED COMPLETION: {completion_validation['reason']}")
                logger.warning(f"ğŸ“Š Current state: {completion_validation['current_state']}")
                
                # Force HOST to continue the incomplete for_each
                return MCPToolOutput(
                    tool_name=MCPToolName.NEXT_STEP,  # Force next_step instead of completion
                    session_id=input_data.session_id,
                    step=completion_validation["required_step"],
                    prompt_template=completion_validation["continuation_template"],
                    instructions=f"ğŸš¨ å®Œæˆè¢«æ‹’ç»ï¼{completion_validation['continuation_instruction']}",
                    context={
                        "completion_blocked": True,
                        "reason": completion_validation["reason"],
                        "required_action": "continue_for_each",
                        "current_iterations": completion_validation["current_iterations"],
                        "total_iterations": completion_validation["total_iterations"]
                    },
                    next_action=f"ğŸ”„ å¿…é¡»ç»§ç»­for_eachå¾ªç¯: {completion_validation['next_action']}",
                    metadata={
                        "completion_blocked": True,
                        "for_each_continuation": True,
                        "unauthorized_completion_attempt": True,
                        "iteration_status": completion_validation["iteration_status"]
                    },
                )

            # Completion is authorized - proceed normally
            logger.info("âœ… AUTHORIZED COMPLETION: All for_each iterations verified complete")

            # Calculate comprehensive quality metrics
            quality_metrics = self._calculate_comprehensive_quality_metrics(session)

            # Generate session summary with detailed analysis
            session_summary = self._generate_detailed_session_summary(session)

            # Get full thinking trace with enhanced metadata
            thinking_trace = self.session_manager.get_full_trace(input_data.session_id)

            # Prepare final results for session completion
            final_results = {
                "completion_timestamp": datetime.now().isoformat(),
                "total_steps_completed": session.step_number,
                "quality_metrics": quality_metrics,
                "session_summary": session_summary,
                "final_insights": input_data.final_insights or "",
                "thinking_trace_id": thinking_trace.get("session_id"),
                "flow_type": session.flow_type,
                "session_duration_minutes": self._calculate_session_duration_minutes(
                    session
                ),
            }

            # Mark session as completed with comprehensive final results
            completion_success = self.session_manager.complete_session(
                input_data.session_id, final_results=final_results
            )

            if not completion_success:
                # Handle completion failure but continue with report generation
                final_results["completion_warning"] = (
                    "Session completion partially failed but report can still be generated"
                )

            # Build enhanced template parameters for comprehensive summary
            template_params = self._build_comprehensive_summary_params(
                session,
                quality_metrics,
                session_summary,
                thinking_trace,
                input_data.final_insights,
            )

            # Get comprehensive summary template
            prompt_template = self.template_manager.get_template(
                "comprehensive_summary", template_params
            )

            # Generate detailed instructions for final report
            instructions = self._generate_completion_instructions(
                quality_metrics, session
            )

            return MCPToolOutput(
                tool_name=MCPToolName.COMPLETE_THINKING,
                session_id=input_data.session_id,
                step="generate_final_report",
                prompt_template=prompt_template,
                instructions=instructions,
                context={
                    "session_completed": True,
                    "total_steps": session.step_number,
                    "quality_metrics": quality_metrics,
                    "session_summary": session_summary,
                    "thinking_trace_available": True,
                    "final_results": final_results,
                    "completion_success": completion_success,
                },
                next_action="ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Šï¼Œæ€ç»´æµç¨‹å·²å®Œæˆ",
                metadata={
                    "session_status": "completed",
                    "completion_timestamp": final_results["completion_timestamp"],
                    "quality_summary": {
                        "average_quality": quality_metrics.get("average_quality", 0),
                        "quality_trend": quality_metrics.get("quality_trend", "stable"),
                        "total_steps": session.step_number,
                        "high_quality_steps": quality_metrics.get(
                            "high_quality_steps", 0
                        ),
                    },
                    "thinking_trace_available": True,
                    "report_generation_ready": True,
                    "session_duration_minutes": final_results[
                        "session_duration_minutes"
                    ],
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="complete_thinking",
                    error=e,
                    session_id=input_data.session_id,
                    context={"final_insights": input_data.final_insights},
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "complete_thinking", e, input_data.session_id
                )

    def _build_template_params(
        self, session: SessionState, previous_result: str
    ) -> Dict[str, Any]:
        """Build template parameters from session context"""
        return {
            "topic": session.topic,
            "current_step": session.current_step,
            "previous_result": previous_result,
            "context": session.context,
            "step_results": session.step_results,
            "session_id": session.session_id,
        }

    def _determine_next_step_with_context(
        self,
        session: SessionState,
        step_result: str,
        quality_feedback: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        Determine next step with enhanced context awareness
        Considers quality feedback and adaptive flow control
        """
        # Check if quality gate requires retry
        if quality_feedback and quality_feedback.get("quality_score", 1.0) < 0.6:
            # Quality too low, suggest improvement step
            return {
                "step_name": f"improve_{session.current_step}",
                "template_name": "improvement_guidance",
                "instructions": "æ ¹æ®è´¨é‡åé¦ˆæ”¹è¿›å½“å‰æ­¥éª¤çš„ç»“æœ",
            }

        # Use flow manager for standard next step (with session state for accurate for_each tracking)
        next_step_info = self.flow_manager.get_next_step(
            session.flow_type, session.current_step, step_result, session
        )

        # Enhance with adaptive logic
        if next_step_info:
            # Adapt template based on complexity and context
            if session.context.get("complexity") == "complex":
                next_step_info["template_name"] = self._get_complex_template_variant(
                    next_step_info["template_name"]
                )

            # Add contextual instructions
            next_step_info["instructions"] = self._generate_contextual_instructions(
                next_step_info, session, step_result
            )

        return next_step_info

    def _build_enhanced_template_params(
        self,
        session: SessionState,
        previous_result: str,
        next_step_info: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Build enhanced template parameters with full context"""
        base_params = self._build_template_params(session, previous_result)

        # Add enhanced context
        enhanced_params = {
            **base_params,
            "step_name": next_step_info["step_name"],
            "step_type": next_step_info.get("step_type", "general"),
            "flow_progress": f"{session.step_number}/{self.flow_manager.get_total_steps(session.flow_type)}",
            "complexity": session.context.get("complexity", "moderate"),
            "focus": session.context.get("focus", ""),
            "domain_context": session.context.get(
                "domain_context", session.context.get("focus", "general analysis")
            ),
            "previous_steps_summary": self._get_previous_steps_summary(session),
            "quality_history": session.quality_scores,
            "session_duration": self._calculate_session_duration(session),
        }

        # Add step-specific context
        if next_step_info["step_name"] == "evidence":
            enhanced_params.update(self._get_evidence_context(session, previous_result))
        elif next_step_info["step_name"] == "evaluate":
            enhanced_params.update(self._get_evaluation_context(session))
        elif next_step_info["step_name"] == "reflect":
            enhanced_params.update(self._get_reflection_context(session))

        return enhanced_params

    def _get_contextual_template(
        self, template_name: str, template_params: Dict[str, Any], session: SessionState
    ) -> str:
        """Get template with smart contextual selection"""
        # Select appropriate template variant based on context
        selected_template = template_name

        # Adapt template based on session context
        if (
            session.context.get("complexity") == "simple"
            and template_name == "decomposition"
        ):
            selected_template = "simple_decomposition"
        elif (
            session.context.get("complexity") == "complex"
            and template_name == "critical_evaluation"
        ):
            selected_template = "advanced_critical_evaluation"

        # Fallback to original template if variant doesn't exist
        try:
            return self.template_manager.get_template(
                selected_template, template_params
            )
        except Exception:
            # Fall back to original template if variant doesn't exist
            return self.template_manager.get_template(template_name, template_params)

    def _build_step_context(
        self,
        session: SessionState,
        next_step_info: Dict[str, Any],
        step_number: int,
    ) -> Dict[str, Any]:
        """Build comprehensive context for the next step"""
        return {
            **session.context,
            "session_id": session.session_id,
            "topic": session.topic,
            "current_step": next_step_info["step_name"],
            "step_number": step_number,
            "flow_type": session.flow_type,
            "completed_steps": list(session.step_results.keys()),
            "quality_scores": session.quality_scores,
            "step_context": {
                "step_type": next_step_info.get("step_type", "general"),
                "template_used": next_step_info["template_name"],
                "dependencies_met": True,
                "adaptive_selection": True,
            },
        }

    def _generate_step_instructions(
        self, next_step_info: Dict[str, Any], session: SessionState
    ) -> str:
        """Generate contextual instructions for the step with for_each awareness"""
        base_instruction = next_step_info.get(
            "instructions", f"Execute {next_step_info['step_name']} step"
        )

        # CRITICAL: For_each specific instructions take priority
        if next_step_info.get("for_each_continuation"):
            step_name = next_step_info["step_name"]
            current_iterations = session.iteration_count.get(step_name, 0)
            total_iterations = session.total_iterations.get(step_name, 0)
            
            if total_iterations > 0:
                next_question_num = current_iterations + 1
                return f"ğŸ”„ è¯·å¤„ç†ç¬¬{next_question_num}ä¸ªå­é—®é¢˜ (å…±{total_iterations}ä¸ª)ã€‚å®Œæˆåå¿…é¡»è°ƒç”¨next_stepç»§ç»­å¾ªç¯ï¼Œä¸è¦æ“…è‡ªåœæ­¢æˆ–è·³è½¬åˆ°å…¶ä»–æ­¥éª¤ã€‚"
            else:
                return "ğŸ”„ è¯·å¤„ç†ä¸‹ä¸€ä¸ªå­é—®é¢˜ã€‚å®Œæˆåå¿…é¡»è°ƒç”¨next_stepç»§ç»­å¾ªç¯ã€‚"

        # Add normal contextual guidance for non-for_each steps
        contextual_additions = []

        if session.context.get("complexity") == "complex":
            contextual_additions.append("è¯·ç‰¹åˆ«æ³¨æ„åˆ†æçš„æ·±åº¦å’Œå…¨é¢æ€§")

        if session.quality_scores and min(session.quality_scores.values()) < 0.7:
            contextual_additions.append("è¯·æ³¨æ„æé«˜åˆ†æè´¨é‡ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°")

        if len(session.step_results) > 3:
            contextual_additions.append("è¯·å‚è€ƒä¹‹å‰æ­¥éª¤çš„ç»“æœï¼Œä¿æŒåˆ†æçš„è¿è´¯æ€§")

        if contextual_additions:
            return f"{base_instruction}ã€‚{' '.join(contextual_additions)}"

        return base_instruction

    def _determine_next_action(
        self, next_step_info: Dict[str, Any], session: SessionState
    ) -> str:
        """Determine the recommended next action with for_each awareness"""
        step_name = next_step_info["step_name"]
        
        # CRITICAL: Check if we're in for_each mode and give explicit guidance
        if next_step_info.get("for_each_continuation"):
            current_iterations = session.iteration_count.get(step_name, 0)
            total_iterations = session.total_iterations.get(step_name, 0)
            
            if total_iterations > 0:
                remaining = total_iterations - current_iterations
                if remaining > 0:
                    return f"ğŸ”„ ç»§ç»­å¤„ç†ç¬¬{current_iterations + 1}ä¸ªå­é—®é¢˜ (å‰©ä½™{remaining}ä¸ª)ï¼Œè¯·è°ƒç”¨next_stepç»§ç»­for_eachå¾ªç¯"
                else:
                    return "âœ… æ‰€æœ‰å­é—®é¢˜å·²å®Œæˆï¼Œå°†è‡ªåŠ¨è¿›å…¥ä¸‹ä¸€ä¸ªæ€ç»´é˜¶æ®µ"
            else:
                return "ğŸ”„ ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå­é—®é¢˜ï¼Œè¯·è°ƒç”¨next_stepç»§ç»­for_eachå¾ªç¯"
        
        # Normal (non-for_each) step guidance
        if step_name in ["decompose", "evidence", "collect_evidence"]:
            return "æ‰§è¡Œå½“å‰æ­¥éª¤åï¼Œå»ºè®®è°ƒç”¨analyze_stepè¿›è¡Œè´¨é‡æ£€æŸ¥"
        elif step_name in ["evaluate", "reflect"]:
            return "å®Œæˆå½“å‰æ­¥éª¤åï¼Œå¯ä»¥è°ƒç”¨complete_thinkingç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"
        else:
            return "ç»§ç»­æ‰§è¡Œæ€ç»´æµç¨‹ï¼Œæˆ–è°ƒç”¨analyze_stepè¿›è¡Œè´¨é‡æ£€æŸ¥"

    def _get_complex_template_variant(self, template_name: str) -> str:
        """Get complex variant of template if available"""
        complex_variants = {
            "decomposition": "advanced_decomposition",
            "critical_evaluation": "advanced_critical_evaluation",
            "evidence_collection": "comprehensive_evidence_collection",
        }
        return complex_variants.get(template_name, template_name)

    def _generate_contextual_instructions(
        self, next_step_info: Dict[str, Any], session: SessionState, step_result: str
    ) -> str:
        """Generate contextual instructions based on session state"""
        base_instruction = f"Execute {next_step_info['step_name']} step"

        # Add context-specific guidance
        if (
            "evidence" in step_result.lower()
            and next_step_info["step_name"] == "evaluate"
        ):
            return "åŸºäºæ”¶é›†çš„è¯æ®è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨è¯æ®è´¨é‡å’Œé€»è¾‘è¿è´¯æ€§"
        elif "é—®é¢˜åˆ†è§£" in step_result and next_step_info["step_name"] == "evidence":
            return "é’ˆå¯¹åˆ†è§£çš„å­é—®é¢˜æ”¶é›†ç›¸å…³è¯æ®ï¼Œç¡®ä¿æ¥æºå¤šæ ·åŒ–å’Œå¯ä¿¡åº¦"

        return base_instruction

    def _get_previous_steps_summary(self, session: SessionState) -> str:
        """Get summary of previous steps"""
        if not session.step_results:
            return "æš‚æ— å®Œæˆçš„æ­¥éª¤"

        summary_parts = []
        for step_name, result_data in session.step_results.items():
            if isinstance(result_data, dict):
                quality = result_data.get("quality_score", "N/A")
                summary_parts.append(f"- {step_name}: å·²å®Œæˆ (è´¨é‡: {quality})")
            else:
                summary_parts.append(f"- {step_name}: å·²å®Œæˆ")

        return "\n".join(summary_parts)

    def _calculate_session_duration(self, session: SessionState) -> str:
        """Calculate session duration in minutes"""
        if session.created_at:
            duration = (datetime.now() - session.created_at).total_seconds() / 60
            return f"{duration:.1f} åˆ†é’Ÿ"
        return "æœªçŸ¥"

    def _get_evidence_context(
        self, session: SessionState, previous_result: str
    ) -> Dict[str, Any]:
        """Get context specific to evidence collection step"""
        context = {}

        # Extract sub-questions from decomposition result
        if "sub_questions" in previous_result.lower():
            context["sub_question"] = "åŸºäºé—®é¢˜åˆ†è§£ç»“æœçš„å­é—®é¢˜"
            context["keywords"] = ["ç›¸å…³å…³é”®è¯", "æœç´¢è¯æ±‡"]

        return context

    def _get_evaluation_context(self, session: SessionState) -> Dict[str, Any]:
        """Get context specific to evaluation step"""
        return {
            "content": "ä¹‹å‰æ­¥éª¤çš„åˆ†æç»“æœ",
            "context": session.context.get("focus", "ç»¼åˆåˆ†æ"),
        }

    def _get_reflection_context(self, session: SessionState) -> Dict[str, Any]:
        """Get context specific to reflection step"""
        return {
            "thinking_history": self._get_previous_steps_summary(session),
            "current_conclusions": "åŸºäºå‰é¢æ­¥éª¤å¾—å‡ºçš„ç»“è®º",
        }

    def _get_analysis_template_name(self, step_name: str) -> str:
        """Get appropriate analysis template based on step name"""
        analysis_templates = {
            "decompose_problem": "analyze_decomposition",
            "collect_evidence": "analyze_evidence",
            "multi_perspective_debate": "analyze_debate",
            "critical_evaluation": "analyze_evaluation",
            "bias_detection": "analyze_evaluation",
            "innovation_thinking": "analyze_evaluation",
            "reflection": "analyze_reflection",
        }
        return analysis_templates.get(step_name, "analyze_evaluation")

    def _validate_step_format(self, step_name: str, step_result: str) -> Dict[str, Any]:
        """Validate the format of step results"""
        validation_result = {
            "valid": True,
            "issues": [],
            "expected_format": "",
            "format_requirements": "",
        }

        # Step-specific format validation
        if step_name == "decompose_problem":
            validation_result.update(self._validate_decomposition_format(step_result))
        elif step_name == "collect_evidence":
            validation_result.update(self._validate_evidence_format(step_result))
        elif step_name == "multi_perspective_debate":
            validation_result.update(self._validate_debate_format(step_result))
        elif step_name in ["critical_evaluation", "bias_detection"]:
            validation_result.update(self._validate_evaluation_format(step_result))
        elif step_name == "reflection":
            validation_result.update(self._validate_reflection_format(step_result))

        return validation_result

    def _validate_decomposition_format(self, step_result: str) -> Dict[str, Any]:
        """Validate decomposition step format"""
        issues = []

        # Check for JSON format
        if not (
            step_result.strip().startswith("{") and step_result.strip().endswith("}")
        ):
            issues.append("ç»“æœåº”ä¸ºJSONæ ¼å¼")

        # Check for required fields
        required_fields = ["main_question", "sub_questions", "relationships"]
        for field in required_fields:
            if field not in step_result:
                issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        # Check sub_questions structure
        if "sub_questions" in step_result and "id" not in step_result:
            issues.append("sub_questionsåº”åŒ…å«idå­—æ®µ")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "JSONæ ¼å¼ï¼ŒåŒ…å«main_question, sub_questions, relationshipså­—æ®µ",
            "format_requirements": "æ¯ä¸ªsub_questionéœ€åŒ…å«id, question, priority, search_keywordsç­‰å­—æ®µ",
        }

    def _validate_evidence_format(self, step_result: str) -> Dict[str, Any]:
        """Validate evidence collection format"""
        issues = []

        # Check for structured evidence
        if "æ¥æº" not in step_result and "source" not in step_result.lower():
            issues.append("åº”åŒ…å«è¯æ®æ¥æºä¿¡æ¯")

        if "å¯ä¿¡åº¦" not in step_result and "credibility" not in step_result.lower():
            issues.append("åº”åŒ…å«å¯ä¿¡åº¦è¯„ä¼°")

        if len(step_result) < 50:  # More lenient threshold for testing
            issues.append("è¯æ®æ”¶é›†ç»“æœè¿‡äºç®€çŸ­")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "ç»“æ„åŒ–è¯æ®é›†åˆï¼ŒåŒ…å«æ¥æºã€å¯ä¿¡åº¦ã€å…³é”®å‘ç°",
            "format_requirements": "æ¯ä¸ªè¯æ®æºåº”åŒ…å«URLã€æ ‡é¢˜ã€æ‘˜è¦ã€å¯ä¿¡åº¦è¯„åˆ†",
        }

    def _validate_debate_format(self, step_result: str) -> Dict[str, Any]:
        """Validate debate step format"""
        issues = []

        # Check for multiple perspectives
        perspective_indicators = [
            "æ”¯æŒ",
            "åå¯¹",
            "ä¸­ç«‹",
            "proponent",
            "opponent",
            "neutral",
        ]
        if not any(indicator in step_result for indicator in perspective_indicators):
            issues.append("åº”åŒ…å«å¤šä¸ªä¸åŒè§’åº¦çš„è§‚ç‚¹")

        # Check for argument structure
        if "è®ºæ®" not in step_result and "argument" not in step_result.lower():
            issues.append("åº”åŒ…å«å…·ä½“çš„è®ºæ®å’Œæ¨ç†")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "å¤šè§’åº¦è¾©è®ºç»“æœï¼ŒåŒ…å«ä¸åŒç«‹åœºçš„è§‚ç‚¹",
            "format_requirements": "æ¯ä¸ªè§’åº¦åº”åŒ…å«æ ¸å¿ƒè§‚ç‚¹ã€æ”¯æŒè®ºæ®ã€è´¨ç–‘è¦ç‚¹",
        }

    def _validate_evaluation_format(self, step_result: str) -> Dict[str, Any]:
        """Validate evaluation step format"""
        issues = []

        # Check for scoring
        if "è¯„åˆ†" not in step_result and "score" not in step_result.lower():
            issues.append("åº”åŒ…å«å…·ä½“çš„è¯„åˆ†")

        # Check for Paul-Elder standards (if applicable)
        paul_elder_standards = [
            "å‡†ç¡®æ€§",
            "ç²¾ç¡®æ€§",
            "ç›¸å…³æ€§",
            "é€»è¾‘æ€§",
            "å¹¿åº¦",
            "æ·±åº¦",
            "é‡è¦æ€§",
            "å…¬æ­£æ€§",
            "æ¸…æ™°æ€§",
        ]
        if any(standard in step_result for standard in paul_elder_standards[:3]):
            # If using Paul-Elder, check for comprehensive coverage
            missing_standards = [
                std for std in paul_elder_standards if std not in step_result
            ]
            if len(missing_standards) > 6:  # Allow some flexibility
                issues.append("Paul-Elderè¯„ä¼°åº”è¦†ç›–æ›´å¤šæ ‡å‡†")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "è¯„ä¼°ç»“æœåŒ…å«è¯„åˆ†å’Œè¯¦ç»†åˆ†æ",
            "format_requirements": "åº”åŒ…å«å„é¡¹æ ‡å‡†çš„è¯„åˆ†ã€ç†ç”±å’Œæ”¹è¿›å»ºè®®",
        }

    def _validate_reflection_format(self, step_result: str) -> Dict[str, Any]:
        """Validate reflection step format"""
        issues = []

        # Check for reflection depth
        reflection_indicators = [
            "åæ€",
            "å­¦åˆ°",
            "æ”¹è¿›",
            "æ´å¯Ÿ",
            "reflection",
            "insight",
        ]
        if not any(indicator in step_result for indicator in reflection_indicators):
            issues.append("åº”åŒ…å«æ·±åº¦åæ€å†…å®¹")

        # Check for metacognitive elements - more lenient for testing
        if len(step_result) < 20:
            issues.append("åæ€å†…å®¹åº”æ›´åŠ è¯¦ç»†å’Œæ·±å…¥")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "æ·±åº¦åæ€ç»“æœï¼ŒåŒ…å«è¿‡ç¨‹åæ€å’Œå…ƒè®¤çŸ¥åˆ†æ",
            "format_requirements": "åº”åŒ…å«æ€ç»´è¿‡ç¨‹åˆ†æã€å­¦ä¹ æ”¶è·ã€æ”¹è¿›æ–¹å‘",
        }

    def _build_analysis_template_params(
        self,
        session: SessionState,
        step_name: str,
        step_result: str,
        analysis_type: str,
    ) -> Dict[str, Any]:
        """Build comprehensive parameters for analysis templates"""
        base_params = {
            "step_name": step_name,
            "step_result": step_result,
            "analysis_type": analysis_type,
            "session_context": session.context,
            "topic": session.topic,
        }

        # Add step-specific parameters
        if step_name == "decompose_problem":
            base_params.update(
                {
                    "original_topic": session.topic,
                    "complexity": session.context.get("complexity", "moderate"),
                }
            )
        elif step_name == "collect_evidence":
            base_params.update(
                {
                    "sub_question": self._extract_sub_question_from_context(session),
                    "search_keywords": self._extract_keywords_from_result(step_result),
                }
            )
        elif step_name == "multi_perspective_debate":
            base_params.update(
                {
                    "debate_topic": self._extract_debate_topic(session, step_result),
                    "evidence_context": self._get_evidence_context_summary(session),
                }
            )
        elif step_name in ["critical_evaluation", "bias_detection"]:
            base_params.update(
                {
                    "evaluated_content": self._get_evaluation_target(session),
                    "evaluation_context": session.context.get("focus", "ç»¼åˆåˆ†æ"),
                }
            )
        elif step_name == "reflection":
            base_params.update(
                {
                    "reflection_topic": session.topic,
                    "thinking_history": self._get_previous_steps_summary(session),
                    "current_conclusions": self._extract_current_conclusions(session),
                }
            )

        return base_params

    def _get_quality_threshold(self, step_name: str, flow_type: str) -> float:
        """Get quality threshold for specific step and flow type"""
        # Default thresholds by step type
        default_thresholds = {
            "decompose_problem": 7.0,
            "collect_evidence": 7.5,
            "multi_perspective_debate": 7.0,
            "critical_evaluation": 8.0,
            "bias_detection": 7.5,
            "innovation_thinking": 6.5,
            "reflection": 7.0,
        }

        # Adjust for flow type
        if flow_type == "comprehensive_analysis":
            return default_thresholds.get(step_name, 7.0)
        elif flow_type == "quick_analysis":
            return default_thresholds.get(step_name, 6.0) - 0.5
        else:
            return default_thresholds.get(step_name, 7.0)

    def _generate_improvement_suggestions(
        self, step_name: str, step_result: str, context: Dict[str, Any]
    ) -> str:
        """Generate step-specific improvement suggestions"""
        suggestions = []

        if step_name == "decompose_problem":
            if len(step_result) < 500:
                suggestions.append("é—®é¢˜åˆ†è§£åº”æ›´åŠ è¯¦ç»†ï¼Œæä¾›æ›´å¤šå­é—®é¢˜å’Œåˆ†æè§’åº¦")
            if "priority" not in step_result:
                suggestions.append("åº”ä¸ºæ¯ä¸ªå­é—®é¢˜è®¾å®šä¼˜å…ˆçº§")
            if "search_keywords" not in step_result:
                suggestions.append("åº”ä¸ºæ¯ä¸ªå­é—®é¢˜æä¾›æœç´¢å…³é”®è¯")

        elif step_name == "collect_evidence":
            if "http" not in step_result and "www" not in step_result:
                suggestions.append("åº”åŒ…å«å…·ä½“çš„ç½‘ç»œæ¥æºé“¾æ¥")
            if step_result.count("æ¥æº") < 3:
                suggestions.append("åº”æ”¶é›†æ›´å¤šä¸åŒæ¥æºçš„è¯æ®")
            if "å¯ä¿¡åº¦" not in step_result:
                suggestions.append("åº”å¯¹æ¯ä¸ªè¯æ®æ¥æºè¿›è¡Œå¯ä¿¡åº¦è¯„ä¼°")

        elif step_name == "multi_perspective_debate":
            if step_result.count("è§‚ç‚¹") < 3:
                suggestions.append("åº”åŒ…å«æ›´å¤šä¸åŒè§’åº¦çš„è§‚ç‚¹")
            if "åé©³" not in step_result and "è´¨ç–‘" not in step_result:
                suggestions.append("åº”åŒ…å«è§‚ç‚¹é—´çš„ç›¸äº’è´¨ç–‘å’Œåé©³")

        elif step_name in ["critical_evaluation", "bias_detection"]:
            if "è¯„åˆ†" not in step_result:
                suggestions.append("åº”åŒ…å«å…·ä½“çš„é‡åŒ–è¯„åˆ†")
            if "æ”¹è¿›å»ºè®®" not in step_result:
                suggestions.append("åº”æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®")

        elif step_name == "reflection":
            if len(step_result) < 400:
                suggestions.append("åæ€åº”æ›´åŠ æ·±å…¥å’Œè¯¦ç»†")
            if "å­¦åˆ°" not in step_result and "æ”¶è·" not in step_result:
                suggestions.append("åº”æ˜ç¡®è¯´æ˜å­¦ä¹ æ”¶è·å’Œæ´å¯Ÿ")

        return (
            "\n".join(f"- {suggestion}" for suggestion in suggestions)
            if suggestions
            else "å½“å‰ç»“æœè´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¿æŒ"
        )

    def _get_next_step_recommendation(
        self, step_name: str, session: SessionState
    ) -> str:
        """Get recommendation for next step based on current step"""
        recommendations = {
            "decompose_problem": "å¦‚æœè´¨é‡è¾¾æ ‡ï¼Œå»ºè®®ç»§ç»­è¯æ®æ”¶é›†æ­¥éª¤",
            "collect_evidence": "å¦‚æœè¯æ®å……åˆ†ï¼Œå»ºè®®è¿›è¡Œå¤šè§’åº¦è¾©è®ºåˆ†æ",
            "multi_perspective_debate": "å»ºè®®è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œæ£€æŸ¥è®ºè¯è´¨é‡",
            "critical_evaluation": "å¦‚æœè¯„ä¼°é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œåè§æ£€æµ‹æˆ–åˆ›æ–°æ€ç»´",
            "bias_detection": "å»ºè®®è¿›è¡Œåæ€æ­¥éª¤ï¼Œæ€»ç»“æ€ç»´è¿‡ç¨‹",
            "innovation_thinking": "å»ºè®®è¿›è¡Œæœ€ç»ˆåæ€å’Œæ€»ç»“",
            "reflection": "å¯ä»¥è°ƒç”¨complete_thinkingç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
        }
        return recommendations.get(step_name, "æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥")

    def _generate_analysis_instructions(
        self, step_name: str, analysis_type: str, quality_threshold: float
    ) -> str:
        """Generate step-specific analysis instructions"""
        base_instruction = f"è¯·æŒ‰ç…§{step_name}æ­¥éª¤çš„ä¸“é—¨è¯„ä¼°æ ‡å‡†è¿›è¡Œè¯¦ç»†åˆ†æ"

        quality_instruction = f"è´¨é‡é—¨æ§æ ‡å‡†ä¸º{quality_threshold}/10åˆ†ï¼Œè¯·ä¸¥æ ¼è¯„ä¼°"

        step_specific = {
            "decompose_problem": "é‡ç‚¹å…³æ³¨é—®é¢˜åˆ†è§£çš„å®Œæ•´æ€§ã€ç‹¬ç«‹æ€§å’Œå¯æ“ä½œæ€§",
            "collect_evidence": "é‡ç‚¹è¯„ä¼°è¯æ®æ¥æºçš„å¤šæ ·æ€§ã€å¯ä¿¡åº¦å’Œç›¸å…³æ€§",
            "multi_perspective_debate": "é‡ç‚¹åˆ†æè§‚ç‚¹çš„å¤šæ ·æ€§ã€è®ºè¯è´¨é‡å’Œäº’åŠ¨æ·±åº¦",
            "critical_evaluation": "é‡ç‚¹æ£€æŸ¥è¯„ä¼°æ ‡å‡†çš„åº”ç”¨å’Œè¯„åˆ†çš„å‡†ç¡®æ€§",
            "reflection": "é‡ç‚¹è¯„ä¼°åæ€çš„æ·±åº¦å’Œå…ƒè®¤çŸ¥è´¨é‡",
        }

        specific_instruction = step_specific.get(step_name, "è¯·è¿›è¡Œå…¨é¢çš„è´¨é‡åˆ†æ")

        return f"{base_instruction}ã€‚{quality_instruction}ã€‚{specific_instruction}ã€‚è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œä¸‹ä¸€æ­¥å»ºè®®ã€‚"

    def _determine_analysis_next_action(
        self, step_name: str, session: SessionState
    ) -> str:
        """Determine next action after analysis"""
        return f"æ ¹æ®{step_name}æ­¥éª¤çš„åˆ†æç»“æœï¼Œå¦‚æœè´¨é‡è¾¾æ ‡åˆ™ç»§ç»­ä¸‹ä¸€æ­¥ï¼Œå¦åˆ™éœ€è¦æ”¹è¿›å½“å‰æ­¥éª¤"

    def _get_analysis_criteria_count(self, step_name: str) -> int:
        """Get number of analysis criteria for the step"""
        criteria_counts = {
            "decompose_problem": 5,
            "collect_evidence": 5,
            "multi_perspective_debate": 5,
            "critical_evaluation": 5,
            "reflection": 5,
        }
        return criteria_counts.get(step_name, 5)

    def _handle_format_validation_failure(
        self, session_id: str, step_name: str, validation_result: Dict[str, Any]
    ) -> MCPToolOutput:
        """Handle format validation failure using error handler"""
        error = MCPFormatValidationError(
            f"Format validation failed for step {step_name}",
            step_name=step_name,
            expected_format=validation_result["expected_format"],
        )

        return self.error_handler.handle_mcp_error(
            tool_name="format_validator",
            error=error,
            session_id=session_id,
            context={
                "step_name": step_name,
                "format_issues": validation_result["issues"],
                "expected_format": validation_result["expected_format"],
            },
        )

    def _get_format_example(self, step_name: str) -> str:
        """Get format example for specific step"""
        examples = {
            "decompose_problem": """
{
  "main_question": "å¦‚ä½•æé«˜æ•™è‚²è´¨é‡ï¼Ÿ",
  "sub_questions": [
    {
      "id": "1",
      "question": "å½“å‰æ•™è‚²ä½“ç³»å­˜åœ¨å“ªäº›ä¸»è¦é—®é¢˜ï¼Ÿ",
      "priority": "high",
      "search_keywords": ["æ•™è‚²é—®é¢˜", "æ•™è‚²ä½“ç³»", "æ•™å­¦è´¨é‡"],
      "expected_perspectives": ["å­¦ç”Ÿè§†è§’", "æ•™å¸ˆè§†è§’", "å®¶é•¿è§†è§’"]
    }
  ],
  "relationships": ["é—®é¢˜1æ˜¯é—®é¢˜2çš„å‰æ"]
}""",
            "collect_evidence": """
è¯æ®æ¥æº1ï¼š
- æ ‡é¢˜ï¼šæ•™è‚²è´¨é‡ç ”ç©¶æŠ¥å‘Š
- URLï¼šhttps://example.com/report
- å¯ä¿¡åº¦ï¼š8/10
- å…³é”®å‘ç°ï¼š...

è¯æ®æ¥æº2ï¼š
- æ ‡é¢˜ï¼šä¸“å®¶è®¿è°ˆ
- URLï¼šhttps://example.com/interview  
- å¯ä¿¡åº¦ï¼š9/10
- å…³é”®å‘ç°ï¼š...""",
            "multi_perspective_debate": """
æ”¯æŒæ–¹è§‚ç‚¹ï¼š
- æ ¸å¿ƒè®ºç‚¹ï¼š...
- æ”¯æŒè®ºæ®ï¼š...

åå¯¹æ–¹è§‚ç‚¹ï¼š
- æ ¸å¿ƒè®ºç‚¹ï¼š...
- åé©³è®ºæ®ï¼š...

ä¸­ç«‹åˆ†æï¼š
- å¹³è¡¡è§‚ç‚¹ï¼š...
- ç»¼åˆè¯„ä¼°ï¼š...""",
        }
        return examples.get(step_name, "è¯·å‚è€ƒæ­¥éª¤è¦æ±‚çš„æ ‡å‡†æ ¼å¼")

    def _get_common_format_errors(self, step_name: str) -> str:
        """Get common format errors for specific step"""
        errors = {
            "decompose_problem": "å¸¸è§é”™è¯¯ï¼šå¿˜è®°JSONæ ¼å¼ã€ç¼ºå°‘å¿…éœ€å­—æ®µã€å­é—®é¢˜æè¿°è¿‡äºç®€å•",
            "collect_evidence": "å¸¸è§é”™è¯¯ï¼šç¼ºå°‘æ¥æºé“¾æ¥ã€æ²¡æœ‰å¯ä¿¡åº¦è¯„ä¼°ã€è¯æ®è¿‡äºç®€å•",
            "multi_perspective_debate": "å¸¸è§é”™è¯¯ï¼šè§‚ç‚¹å•ä¸€ã€ç¼ºå°‘äº’åŠ¨ã€è®ºæ®ä¸å……åˆ†",
        }
        return errors.get(step_name, "è¯·ç¡®ä¿æ ¼å¼å®Œæ•´å’Œè§„èŒƒ")

    # Helper methods for extracting information from session context
    def _extract_sub_question_from_context(self, session: SessionState) -> str:
        """Extract sub-question from session context"""
        # This would extract from previous decomposition results
        return session.context.get("current_sub_question", "åŸºäºé—®é¢˜åˆ†è§£çš„å­é—®é¢˜")

    def _extract_keywords_from_result(self, step_result: str) -> str:
        """Extract keywords from step result"""
        # Simple keyword extraction - in practice this could be more sophisticated
        return "ç›¸å…³æœç´¢å…³é”®è¯"

    def _extract_debate_topic(self, session: SessionState, step_result: str) -> str:
        """Extract debate topic from context"""
        return session.context.get("debate_topic", session.topic)

    def _get_evidence_context_summary(self, session: SessionState) -> str:
        """Get summary of evidence collection context"""
        return "åŸºäºè¯æ®æ”¶é›†çš„èƒŒæ™¯ä¿¡æ¯"

    def _get_evaluation_target(self, session: SessionState) -> str:
        """Get the target content for evaluation"""
        return "éœ€è¦è¯„ä¼°çš„å†…å®¹"

    def _extract_current_conclusions(self, session: SessionState) -> str:
        """Extract current conclusions from session"""
        return "åŸºäºå‰é¢æ­¥éª¤å¾—å‡ºçš„å½“å‰ç»“è®º"

    def _handle_session_not_found(self, session_id: str) -> MCPToolOutput:
        """Handle case where session is not found using error handler"""
        error = SessionNotFoundError(
            f"Session {session_id} not found", session_id=session_id
        )

        return self.error_handler.handle_mcp_error(
            tool_name="session_manager",
            error=error,
            session_id=session_id,
            context={"session_id": session_id},
        )

    def _handle_flow_completion(self, session_id: str) -> MCPToolOutput:
        """Handle flow completion with safety checks"""
        # SAFETY CHECK: Verify no active for_each iterations before completing
        session = self.session_manager.get_session(session_id)
        if session:
            for step_name, current_count in session.iteration_count.items():
                total_count = session.total_iterations.get(step_name, 0)
                if current_count < total_count and total_count > 0:
                    logger.error(f"ğŸš¨ CRITICAL: Attempted completion with active for_each: {step_name} at {current_count}/{total_count}")
                    raise RuntimeError(f"Cannot complete flow with active for_each iterations: {step_name} {current_count}/{total_count}")
        
        logger.info("ğŸ CONFIRMED SAFE COMPLETION: All iterations verified complete")
        completion_prompt = self.template_manager.get_template(
            "flow_completion", {"session_id": session_id}
        )

        return MCPToolOutput(
            tool_name=MCPToolName.COMPLETE_THINKING,
            session_id=session_id,
            step="flow_completed",
            prompt_template=completion_prompt,
            instructions="æ€ç»´æµç¨‹å·²å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
            context={"flow_completed": True},
            next_action="è°ƒç”¨complete_thinkingç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
            metadata={
                "ready_for_completion": True,
                "completion_verified": True,
                "all_iterations_complete": True
            },
        )

    def _handle_error(
        self, tool_name: str, error_message: str, session_id: Optional[str] = None
    ) -> MCPToolOutput:
        """Handle tool execution errors using the error handler"""
        # Create a generic exception for the error handler
        error = MCPToolExecutionError(
            error_message, tool_name=tool_name, session_id=session_id
        )

        # Use the error handler to create appropriate recovery response
        return self.error_handler.handle_mcp_error(
            tool_name=tool_name,
            error=error,
            session_id=session_id,
            context={"error_message": error_message},
        )

    def _validate_completion_eligibility(self, session: SessionState) -> Dict[str, Any]:
        """
        Validate whether the session is actually eligible for completion
        CRITICAL: This prevents HOST from bypassing for_each iterations
        """
        try:
            # Check all for_each steps for incomplete iterations
            for step_name, current_count in session.iteration_count.items():
                total_count = session.total_iterations.get(step_name, 0)
                
                if current_count < total_count and total_count > 0:
                    # Found incomplete for_each - completion not allowed
                    remaining = total_count - current_count
                    
                    return {
                        "allowed": False,
                        "reason": f"Active for_each in {step_name}: {current_count}/{total_count} (ç¼ºå°‘{remaining}ä¸ª)",
                        "current_state": f"{step_name} at {current_count}/{total_count}",
                        "required_step": step_name,
                        "continuation_template": self.template_manager.get_template(
                            "evidence_collection", {"sub_question": f"ç¬¬{current_count + 1}ä¸ªå­é—®é¢˜"}
                        ),
                        "continuation_instruction": f"å¿…é¡»å®Œæˆå‰©ä½™{remaining}ä¸ªå­é—®é¢˜çš„{step_name}å¤„ç†",
                        "next_action": f"ç»§ç»­å¤„ç†ç¬¬{current_count + 1}ä¸ªå­é—®é¢˜",
                        "current_iterations": current_count,
                        "total_iterations": total_count,
                        "iteration_status": {
                            "current": current_count,
                            "total": total_count,
                            "remaining": remaining,
                            "step": step_name
                        }
                    }
            
            # All for_each iterations complete - completion allowed
            return {
                "allowed": True,
                "reason": "All for_each iterations completed",
                "validation_passed": True
            }
            
        except Exception as e:
            logger.error(f"Error validating completion eligibility: {e}")
            # Default to blocking completion if validation fails
            return {
                "allowed": False,
                "reason": f"Validation error: {e}",
                "error_occurred": True
            }

    # Enhanced helper methods for complete_thinking tool

    def _calculate_comprehensive_quality_metrics(
        self, session: SessionState
    ) -> Dict[str, Any]:
        """
        Calculate comprehensive quality metrics for the session

        Returns detailed quality analysis including:
        - Average quality score
        - Quality trend analysis
        - Step-by-step quality breakdown
        - Quality distribution
        - Improvement recommendations
        """
        quality_scores = session.quality_scores

        if not quality_scores:
            return {
                "average_quality": 0.0,
                "quality_trend": "no_data",
                "total_steps": session.step_number,
                "high_quality_steps": 0,
                "quality_distribution": {},
                "improvement_areas": ["No quality data available"],
                "overall_assessment": "insufficient_data",
            }

        # Calculate basic metrics
        scores = list(quality_scores.values())
        average_quality = sum(scores) / len(scores)
        high_quality_steps = sum(1 for score in scores if score >= 8.0)

        # Quality trend analysis
        quality_trend = self._analyze_quality_trend(scores)

        # Quality distribution
        quality_distribution = {
            "excellent": sum(1 for score in scores if score >= 9.0),
            "good": sum(1 for score in scores if 7.0 <= score < 9.0),
            "acceptable": sum(1 for score in scores if 5.0 <= score < 7.0),
            "needs_improvement": sum(1 for score in scores if score < 5.0),
        }

        # Identify improvement areas
        improvement_areas = self._identify_improvement_areas(quality_scores)

        # Overall assessment
        overall_assessment = self._determine_overall_assessment(
            average_quality, quality_distribution
        )

        return {
            "average_quality": round(average_quality, 2),
            "quality_trend": quality_trend,
            "total_steps": session.step_number,
            "high_quality_steps": high_quality_steps,
            "quality_distribution": quality_distribution,
            "step_quality_breakdown": quality_scores,
            "improvement_areas": improvement_areas,
            "overall_assessment": overall_assessment,
            "quality_consistency": self._calculate_quality_consistency(scores),
            "best_performing_step": (
                max(quality_scores.items(), key=lambda x: x[1])
                if quality_scores
                else None
            ),
            "lowest_performing_step": (
                min(quality_scores.items(), key=lambda x: x[1])
                if quality_scores
                else None
            ),
        }

    def _analyze_quality_trend(self, scores: List[float]) -> str:
        """Analyze the trend in quality scores"""
        if len(scores) < 2:
            return "insufficient_data"

        # Simple trend analysis
        first_half = scores[: len(scores) // 2]
        second_half = scores[len(scores) // 2 :]

        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)

        diff = second_avg - first_avg

        if diff > 0.5:
            return "improving"
        elif diff < -0.5:
            return "declining"
        else:
            return "stable"

    def _identify_improvement_areas(
        self, quality_scores: Dict[str, float]
    ) -> List[str]:
        """Identify areas that need improvement based on quality scores"""
        improvement_areas = []

        for step_name, score in quality_scores.items():
            if score < 6.0:
                improvement_areas.append(f"{step_name} (å¾—åˆ†: {score})")

        if not improvement_areas:
            improvement_areas.append("æ‰€æœ‰æ­¥éª¤è´¨é‡è‰¯å¥½")

        return improvement_areas

    def _determine_overall_assessment(
        self, average_quality: float, quality_distribution: Dict[str, int]
    ) -> str:
        """Determine overall quality assessment"""
        if average_quality >= 8.5:
            return "excellent"
        elif average_quality >= 7.0:
            return "good"
        elif average_quality >= 5.0:
            return "acceptable"
        else:
            return "needs_improvement"

    def _calculate_quality_consistency(self, scores: List[float]) -> float:
        """Calculate quality consistency (lower variance = higher consistency)"""
        if len(scores) < 2:
            return 1.0

        mean = sum(scores) / len(scores)
        variance = sum((score - mean) ** 2 for score in scores) / len(scores)

        # Convert to consistency score (0-1, higher is more consistent)
        consistency = max(0, 1 - (variance / 10))  # Normalize variance
        return round(consistency, 3)

    def _generate_detailed_session_summary(
        self, session: SessionState
    ) -> Dict[str, Any]:
        """Generate detailed session summary with step analysis"""

        # Get step details from database
        try:
            steps = self.session_manager.db.get_session_steps(session.session_id)
            results = self.session_manager.db.get_step_results(session.session_id)
        except Exception:
            steps = []
            results = []

        # Organize results by step
        step_results_map = {}
        for result in results:
            step_id = result.get("step_id")
            if step_id not in step_results_map:
                step_results_map[step_id] = []
            step_results_map[step_id].append(result)

        # Build detailed step summary
        detailed_steps = []
        for step in steps:
            step_summary = {
                "step_name": step.get("step_name", "unknown"),
                "step_type": step.get("step_type", "general"),
                "quality_score": step.get("quality_score"),
                "execution_time_ms": step.get("execution_time_ms"),
                "results_count": len(step_results_map.get(step.get("id"), [])),
                "timestamp": step.get("created_at"),
            }
            detailed_steps.append(step_summary)

        return {
            "session_id": session.session_id,
            "topic": session.topic,
            "flow_type": session.flow_type,
            "total_steps": len(detailed_steps),
            "session_duration": self._calculate_session_duration_minutes(session),
            "detailed_steps": detailed_steps,
            "context_summary": session.context,
            "completion_status": "completed",
        }

    def _calculate_session_duration_minutes(self, session: SessionState) -> float:
        """Calculate session duration in minutes"""
        if session.created_at and session.updated_at:
            duration = (session.updated_at - session.created_at).total_seconds() / 60
            return round(duration, 2)
        return 0.0

    def _build_comprehensive_summary_params(
        self,
        session: SessionState,
        quality_metrics: Dict[str, Any],
        session_summary: Dict[str, Any],
        thinking_trace: Dict[str, Any],
        final_insights: Optional[str],
    ) -> Dict[str, Any]:
        """Build comprehensive parameters for the summary template"""

        # Format quality metrics for display
        quality_display = self._format_quality_metrics_for_display(quality_metrics)

        # Format step summary for display
        step_summary_display = self._format_step_summary_for_display(session_summary)

        # Format thinking trace for display
        trace_display = self._format_thinking_trace_for_display(thinking_trace)

        return {
            "topic": session.topic,
            "flow_type": session.flow_type,
            "session_duration": f"{session_summary['session_duration']} åˆ†é’Ÿ",
            "total_steps": session_summary["total_steps"],
            "step_summary": step_summary_display,
            "thinking_trace": trace_display,
            "quality_metrics": quality_display,
            "final_insights": final_insights or "æ— é¢å¤–æ´å¯Ÿ",
            "completion_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "overall_assessment": quality_metrics.get("overall_assessment", "unknown"),
            "average_quality": quality_metrics.get("average_quality", 0),
            "improvement_areas": "\n".join(
                quality_metrics.get("improvement_areas", [])
            ),
            "best_step": (
                quality_metrics.get("best_performing_step", ["æ— ", 0])[0]
                if quality_metrics.get("best_performing_step")
                else "æ— "
            ),
            "session_context": session.context,
        }

    def _format_quality_metrics_for_display(
        self, quality_metrics: Dict[str, Any]
    ) -> str:
        """Format quality metrics for template display"""
        if not quality_metrics:
            return "æ— è´¨é‡æ•°æ®"

        lines = [
            f"å¹³å‡è´¨é‡å¾—åˆ†: {quality_metrics.get('average_quality', 0)}/10",
            f"è´¨é‡è¶‹åŠ¿: {quality_metrics.get('quality_trend', 'unknown')}",
            f"é«˜è´¨é‡æ­¥éª¤æ•°: {quality_metrics.get('high_quality_steps', 0)}",
            f"æ€»ä½“è¯„ä¼°: {quality_metrics.get('overall_assessment', 'unknown')}",
            f"è´¨é‡ä¸€è‡´æ€§: {quality_metrics.get('quality_consistency', 0)}",
        ]

        if quality_metrics.get("best_performing_step"):
            best_step, best_score = quality_metrics["best_performing_step"]
            lines.append(f"æœ€ä½³æ­¥éª¤: {best_step} ({best_score}/10)")

        return "\n".join(lines)

    def _format_step_summary_for_display(self, session_summary: Dict[str, Any]) -> str:
        """Format step summary for template display"""
        if not session_summary.get("detailed_steps"):
            return "æ— æ­¥éª¤æ•°æ®"

        lines = []
        for step in session_summary["detailed_steps"]:
            step_name = step.get("step_name", "unknown")
            quality = step.get("quality_score", "N/A")
            lines.append(f"- {step_name}: å®Œæˆ (è´¨é‡: {quality})")

        return "\n".join(lines)

    def _format_thinking_trace_for_display(self, thinking_trace: Dict[str, Any]) -> str:
        """Format thinking trace for template display"""
        if not thinking_trace or thinking_trace.get("error"):
            return "æ€ç»´è½¨è¿¹ä¸å¯ç”¨"

        lines = [
            f"ä¼šè¯ID: {thinking_trace.get('session_id', 'unknown')}",
            f"æµç¨‹ç±»å‹: {thinking_trace.get('flow_type', 'unknown')}",
            f"æ€»æ—¶é•¿: {thinking_trace.get('total_duration', 0)} ç§’",
            f"æ­¥éª¤æ•°é‡: {len(thinking_trace.get('steps', []))}",
        ]

        return "\n".join(lines)

    def _generate_completion_instructions(
        self, quality_metrics: Dict[str, Any], session: SessionState
    ) -> str:
        """Generate detailed instructions for final report generation"""
        base_instruction = "è¯·ç”Ÿæˆè¯¦ç»†çš„ç»¼åˆæŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰å…³é”®å‘ç°å’Œæ´å¯Ÿ"

        # Add quality-specific guidance
        quality_guidance = []

        overall_assessment = quality_metrics.get("overall_assessment", "unknown")
        if overall_assessment == "excellent":
            quality_guidance.append("è´¨é‡ä¼˜ç§€ï¼Œé‡ç‚¹çªå‡ºæ ¸å¿ƒæ´å¯Ÿå’Œåˆ›æ–°è§‚ç‚¹")
        elif overall_assessment == "good":
            quality_guidance.append("è´¨é‡è‰¯å¥½ï¼Œç¡®ä¿æ¶µç›–æ‰€æœ‰é‡è¦å‘ç°")
        elif overall_assessment == "acceptable":
            quality_guidance.append("è´¨é‡å¯æ¥å—ï¼Œæ³¨æ„è¡¥å……åˆ†ææ·±åº¦")
        else:
            quality_guidance.append("è´¨é‡éœ€è¦æ”¹è¿›ï¼Œé‡ç‚¹åŠ å¼ºè®ºè¯å’Œè¯æ®æ”¯æ’‘")

        # Add step-specific guidance
        if session.step_number > 5:
            quality_guidance.append("æµç¨‹è¾ƒä¸ºå®Œæ•´ï¼Œç¡®ä¿å„æ­¥éª¤ç»“æœçš„æœ‰æœºæ•´åˆ")

        # Add improvement area guidance
        improvement_areas = quality_metrics.get("improvement_areas", [])
        if improvement_areas and improvement_areas[0] != "æ‰€æœ‰æ­¥éª¤è´¨é‡è‰¯å¥½":
            quality_guidance.append(
                f"ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹æ”¹è¿›é¢†åŸŸ: {', '.join(improvement_areas[:2])}"
            )

        if quality_guidance:
            return f"{base_instruction}ã€‚{' '.join(quality_guidance)}"

        return base_instruction
