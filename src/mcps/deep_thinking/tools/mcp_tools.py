"""
Core MCP tools for the Deep Thinking Engine

These tools follow the zero-cost principle:
- MCP Server provides flow control and prompt templates
- Host LLM performs the actual intelligent processing
- No LLM API calls from the server side
"""

import json
import logging
import re
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..config.exceptions import (
    MCPFormatValidationError,
    MCPToolExecutionError,
    SessionNotFoundError,
)
from ..flows.flow_manager import FlowManager
from ..models.mcp_models import (
    AnalyzeStepInput,
    CompleteThinkingInput,
    MCPToolName,
    MCPToolOutput,
    NextStepInput,
    SessionState,
    StartThinkingInput,
)
from ..sessions.session_manager import SessionManager
from ..templates.template_manager import TemplateManager
from .mcp_error_handler import MCPErrorHandler

logger = logging.getLogger(__name__)


class MCPTools:
    """
    Core MCP tools that return prompt templates for LLM execution

    Following the zero-cost principle:
    - Server handles flow control and template management
    - LLM handles intelligent processing and content generation
    """

    def __init__(
        self,
        session_manager: SessionManager,
        template_manager: TemplateManager,
        flow_manager: FlowManager,
        config_manager=None,
    ):
        self.session_manager = session_manager
        self.template_manager = template_manager
        self.flow_manager = flow_manager
        self.config_manager = config_manager
        self.error_handler = MCPErrorHandler(session_manager, template_manager)
        # Track active sessions to prevent state inconsistencies
        self._active_sessions = {}

    def start_thinking(self, input_data: StartThinkingInput) -> MCPToolOutput:
        """
        Start a new deep thinking session

        Returns a problem decomposition prompt template for the LLM to execute
        """
        try:
            # Create new session
            session_id = str(uuid.uuid4())
            session_state = SessionState(
                session_id=session_id,
                topic=input_data.topic,
                current_step="decompose_problem",
                flow_type=input_data.flow_type,
                context={
                    "complexity": input_data.complexity,
                    "focus": input_data.focus,
                    "original_topic": input_data.topic,
                },
            )

            # Save session state
            self.session_manager.create_session(session_state)

            # Get decomposition prompt template
            template_params = {
                "topic": input_data.topic,
                "complexity": input_data.complexity,
                "focus": input_data.focus,
                "domain_context": input_data.focus or "general analysis",
            }

            prompt_template = self.template_manager.get_template(
                "decomposition", template_params
            )

            return MCPToolOutput(
                tool_name=MCPToolName.START_THINKING,
                session_id=session_id,
                step="decompose_problem",
                prompt_template=prompt_template,
                instructions="è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†è§£ç»“æœï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ",
                context={
                    "session_id": session_id,
                    "topic": input_data.topic,
                    "complexity": input_data.complexity,
                },
                next_action="è°ƒç”¨next_stepå·¥å…·ç»§ç»­æµç¨‹",
                metadata={
                    "flow_type": input_data.flow_type,
                    "expected_output": "JSONæ ¼å¼çš„é—®é¢˜åˆ†è§£ç»“æœ",
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="start_thinking",
                    error=e,
                    session_id=None,
                    context={
                        "topic": input_data.topic,
                        "complexity": input_data.complexity,
                    },
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "start_thinking", e, None
                )

    def next_step(self, input_data: NextStepInput) -> MCPToolOutput:
        """
        Get the next step in the thinking process

        Returns appropriate prompt template based on current flow state
        Implements enhanced flow control and template selection logic
        """
        try:
            # Get current session state
            try:
                session = self.session_manager.get_session(input_data.session_id)
                if session is None:
                    raise SessionNotFoundError(
                        "Session not found", session_id=input_data.session_id
                    )
            except Exception as e:
                # Session not found - use error handler for consistent handling
                return self.error_handler.handle_mcp_error(
                    tool_name="next_step",
                    error=e,
                    session_id=input_data.session_id,
                    context={"step_result": input_data.step_result},
                )

            # Extract quality score from step result if provided
            quality_score = None
            if (
                input_data.quality_feedback
                and "quality_score" in input_data.quality_feedback
            ):
                quality_score = input_data.quality_feedback["quality_score"]

            # Save previous step result with enhanced context
            # IMPORTANT: Don't auto-increment for_each here, let the flow manager decide
            self.session_manager.add_step_result(
                input_data.session_id,
                session.current_step,
                input_data.step_result,
                result_type="output",
                metadata={
                    "step_completion_time": datetime.now().isoformat(),
                    "quality_feedback": input_data.quality_feedback,
                    "step_context": session.context,
                    "for_each_continuation": False,  # Don't auto-increment
                },
                quality_score=quality_score,
            )

            # CRITICAL: Determine next step and handle for_each iteration properly
            next_step_info = self._determine_next_step_with_context(
                session, input_data.step_result, input_data.quality_feedback
            )

            # IMPORTANT: If we're continuing for_each, increment the iteration EXACTLY ONCE
            if next_step_info and next_step_info.get("for_each_continuation"):
                # Only increment if we haven't already processed this sub-question
                current_iterations = session.iteration_count.get(
                    session.current_step, 0
                )
                total_iterations = session.total_iterations.get(session.current_step, 0)

                logger.info(
                    f"FOR_EACH CONTINUATION: {session.current_step} at {current_iterations}/{total_iterations}"
                )

                # Increment iteration counter for the NEXT sub-question
                if current_iterations < total_iterations:
                    session.iteration_count[session.current_step] = (
                        current_iterations + 1
                    )
                    new_count = session.iteration_count[session.current_step]
                    logger.info(
                        f"INCREMENTED {session.current_step}: {current_iterations} -> {new_count}/{total_iterations}"
                    )

                    # Update session state immediately in both caches
                    self._active_sessions[session.session_id] = session
                    self.session_manager._active_sessions[session.session_id] = session
                else:
                    logger.warning(
                        f"Cannot increment {session.current_step} beyond {total_iterations}"
                    )

            if not next_step_info:
                # CRITICAL: Check if we're in the middle of for_each before completing
                session_for_completion_check = self.session_manager.get_session(
                    input_data.session_id
                )
                if session_for_completion_check:
                    # Check if any step has active for_each iterations
                    for (
                        step_name,
                        current_count,
                    ) in session_for_completion_check.iteration_count.items():
                        total_count = session_for_completion_check.total_iterations.get(
                            step_name, 0
                        )
                        if current_count < total_count and total_count > 0:
                            logger.warning(
                                f"ğŸš¨ PREVENTED PREMATURE COMPLETION: {step_name} at {current_count}/{total_count}"
                            )
                            logger.warning(
                                "ğŸ” Flow manager returned None but for_each is still active!"
                            )

                            # Force continue the for_each step
                            return {
                                "step_name": step_name,
                                "template_name": "evidence_collection",  # Default template
                                "instructions": f"ğŸš¨ æ€¥æ•‘æ¨¡å¼: ç»§ç»­å¤„ç†ç¬¬{current_count + 1}ä¸ªå­é—®é¢˜",
                                "for_each_continuation": True,
                            }

                # Flow completed
                logger.info(
                    "ğŸ LEGITIMATE FLOW COMPLETION: All for_each iterations completed"
                )
                return self._handle_flow_completion(input_data.session_id)

            # Calculate correct step number for metadata based on the next step
            # For for_each continuations, we want to show the step number of the step being iterated
            if next_step_info.get("for_each_continuation"):
                # For for_each iterations, use current session step number which should already be correct
                metadata_step_number = session.step_number
            else:
                # For normal progression, increment from current step
                metadata_step_number = session.step_number + 1

            # Update session state with enhanced tracking
            # Only update step number if not for_each continuation
            if not next_step_info.get("for_each_continuation"):
                self.session_manager.update_session_step(
                    input_data.session_id,
                    next_step_info["step_name"],
                    step_result=input_data.step_result,
                    quality_score=quality_score,
                )
            else:
                # For for_each continuation, add step result without advancing step_number
                # IMPORTANT: Mark that iteration was already incremented above
                self.session_manager.add_step_result(
                    input_data.session_id,
                    session.current_step,
                    input_data.step_result,
                    result_type="output",
                    metadata={
                        "for_each_continuation": True,
                        "should_increment_iteration": False,  # Already incremented above
                        "step_completion_time": datetime.now().isoformat(),
                        "quality_feedback": input_data.quality_feedback,
                        "step_context": session.context,
                        "iteration_already_incremented": True,
                    },
                    quality_score=quality_score,
                )

            # Build enhanced template parameters with full context
            template_params = self._build_enhanced_template_params(
                session, input_data.step_result, next_step_info
            )

            # Get appropriate prompt template with smart selection
            prompt_template = self._get_contextual_template(
                next_step_info["template_name"], template_params, session
            )

            # Build comprehensive context for the next step
            step_context = self._build_step_context(
                session, next_step_info, metadata_step_number
            )

            return MCPToolOutput(
                tool_name=MCPToolName.NEXT_STEP,
                session_id=input_data.session_id,
                step=next_step_info["step_name"],
                prompt_template=prompt_template,
                instructions=self._generate_step_instructions(next_step_info, session),
                context=step_context,
                next_action=self._determine_next_action(next_step_info, session),
                metadata={
                    "step_number": metadata_step_number,
                    "flow_progress": f"{metadata_step_number}/{self.flow_manager.get_total_steps(session.flow_type)}",
                    "flow_type": session.flow_type,
                    "previous_step": session.current_step,
                    "quality_gate_passed": quality_score is None
                    or quality_score >= 0.7,
                    "template_selected": next_step_info["template_name"],
                    "context_enriched": True,
                    "for_each_continuation": next_step_info.get(
                        "for_each_continuation", False
                    ),
                    # Add explicit guidance for HOST
                    "iteration_status": {
                        "current": session.iteration_count.get(
                            next_step_info["step_name"], 0
                        ),
                        "total": session.total_iterations.get(
                            next_step_info["step_name"], 0
                        ),
                        "is_for_each": next_step_info.get(
                            "for_each_continuation", False
                        ),
                    },
                },
            )

        except Exception as e:
            try:
                # Provide enriched context for error recovery
                error_context = {
                    "step_result": input_data.step_result,
                    "quality_feedback": input_data.quality_feedback,
                    "session_lookup_failed": True,
                }

                # Try to get partial session info for better recovery
                try:
                    session = self.session_manager.get_session(input_data.session_id)
                    if session:
                        error_context.update(
                            {
                                "current_step": session.current_step,
                                "flow_type": session.flow_type,
                                "step_number": session.step_number,
                                "session_lookup_failed": False,
                            }
                        )
                except Exception:
                    pass  # Keep original context

                return self.error_handler.handle_mcp_error(
                    tool_name="next_step",
                    error=e,
                    session_id=input_data.session_id,
                    context=error_context,
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "next_step", e, input_data.session_id
                )

    def analyze_step(self, input_data: AnalyzeStepInput) -> MCPToolOutput:
        """
        Analyze the quality of a completed step

        Implements comprehensive quality analysis with:
        - Step-specific evaluation criteria
        - Quality gate enforcement
        - Format validation
        - Improvement suggestions generation
        """
        try:
            # Get session state
            session = self.session_manager.get_session(input_data.session_id)
            if not session:
                return self._handle_session_not_found(input_data.session_id)

            # Perform format validation first
            format_validation = self._validate_step_format(
                input_data.step_name, input_data.step_result
            )
            if not format_validation["valid"]:
                return self._handle_format_validation_failure(
                    input_data.session_id, input_data.step_name, format_validation
                )

            # Get step-specific analysis template
            analysis_template_name = self._get_analysis_template_name(
                input_data.step_name
            )

            # Build comprehensive template parameters
            template_params = self._build_analysis_template_params(
                session,
                input_data.step_name,
                input_data.step_result,
                input_data.analysis_type,
            )

            # Get quality threshold for this step
            quality_threshold = self._get_quality_threshold(
                input_data.step_name, session.flow_type
            )

            # Generate improvement suggestions based on step type
            improvement_suggestions = self._generate_improvement_suggestions(
                input_data.step_name, input_data.step_result, session.context
            )

            # Add quality gate information to template params
            template_params.update(
                {
                    "quality_threshold": quality_threshold,
                    "improvement_suggestions": improvement_suggestions,
                    "quality_gate_passed": "å¾…è¯„ä¼°",
                    "quality_level": "å¾…è¯„ä¼°",
                    "next_step_recommendation": self._get_next_step_recommendation(
                        input_data.step_name, session
                    ),
                }
            )

            # Get analysis prompt template
            prompt_template = self.template_manager.get_template(
                analysis_template_name, template_params
            )

            # Generate step-specific instructions
            instructions = self._generate_analysis_instructions(
                input_data.step_name, input_data.analysis_type, quality_threshold
            )

            return MCPToolOutput(
                tool_name=MCPToolName.ANALYZE_STEP,
                session_id=input_data.session_id,
                step=f"analyze_{input_data.step_name}",
                prompt_template=prompt_template,
                instructions=instructions,
                context={
                    "analyzed_step": input_data.step_name,
                    "analysis_type": input_data.analysis_type,
                    "quality_threshold": quality_threshold,
                    "format_validated": True,
                    "step_context": session.context,
                    "improvement_suggestions_available": True,
                },
                next_action=self._determine_analysis_next_action(
                    input_data.step_name, session
                ),
                metadata={
                    "quality_check": True,
                    "step_analyzed": input_data.step_name,
                    "analysis_template": analysis_template_name,
                    "quality_threshold": quality_threshold,
                    "format_validation_passed": True,
                    "analysis_criteria_count": self._get_analysis_criteria_count(
                        input_data.step_name
                    ),
                    "improvement_suggestions_generated": True,
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="analyze_step",
                    error=e,
                    session_id=input_data.session_id,
                    context={
                        "step_name": input_data.step_name,
                        "step_result": input_data.step_result,
                        "analysis_type": input_data.analysis_type,
                    },
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "analyze_step", e, input_data.session_id
                )

    def complete_thinking(self, input_data: CompleteThinkingInput) -> MCPToolOutput:
        """
        Complete the thinking process and generate final report

        Enhanced implementation with:
        - Comprehensive session state update and result aggregation
        - Quality metrics calculation and analysis
        - Final report template with detailed insights
        - Session completion with full trace preservation
        - CRITICAL: Validation to prevent unauthorized completion by HOST
        """
        try:
            # Get session state
            session = self.session_manager.get_session(input_data.session_id)
            if not session:
                return self._handle_session_not_found(input_data.session_id)

            # ğŸš¨ CRITICAL: Validate that completion is actually allowed
            completion_validation = self._validate_completion_eligibility(session)
            if not completion_validation["allowed"]:
                logger.warning(
                    f"ğŸš« BLOCKED UNAUTHORIZED COMPLETION: {completion_validation['reason']}"
                )
                logger.warning(
                    f"ğŸ“Š Current state: {completion_validation['current_state']}"
                )

                # Force HOST to continue the incomplete for_each
                return MCPToolOutput(
                    tool_name=MCPToolName.NEXT_STEP,  # Force next_step instead of completion
                    session_id=input_data.session_id,
                    step=completion_validation["required_step"],
                    prompt_template=completion_validation["continuation_template"],
                    instructions=f"ğŸš¨ å®Œæˆè¢«æ‹’ç»ï¼{completion_validation['continuation_instruction']}",
                    context={
                        "completion_blocked": True,
                        "reason": completion_validation["reason"],
                        "required_action": "continue_for_each",
                        "current_iterations": completion_validation[
                            "current_iterations"
                        ],
                        "total_iterations": completion_validation["total_iterations"],
                    },
                    next_action=f"ğŸ”„ å¿…é¡»ç»§ç»­for_eachå¾ªç¯: {completion_validation['next_action']}",
                    metadata={
                        "completion_blocked": True,
                        "for_each_continuation": True,
                        "unauthorized_completion_attempt": True,
                        "iteration_status": completion_validation["iteration_status"],
                    },
                )

            # Completion is authorized - proceed normally
            logger.info(
                "âœ… AUTHORIZED COMPLETION: All for_each iterations verified complete"
            )

            # Refresh session data to ensure we have the latest quality scores
            self._refresh_session_quality_data(session)

            # Calculate comprehensive quality metrics
            quality_metrics = self._calculate_comprehensive_quality_metrics(session)

            # Generate session summary with detailed analysis
            session_summary = self._generate_detailed_session_summary(session)

            # Get full thinking trace with enhanced metadata
            thinking_trace = self.session_manager.get_full_trace(input_data.session_id)

            # Prepare final results for session completion
            final_results = {
                "completion_timestamp": datetime.now().isoformat(),
                "total_steps_completed": session_summary["total_steps"],  # Use consistent step count
                "quality_metrics": quality_metrics,
                "session_summary": session_summary,
                "final_insights": input_data.final_insights or "",
                "thinking_trace_id": thinking_trace.get("session_id"),
                "flow_type": session.flow_type,
                "session_duration_minutes": self._calculate_session_duration_minutes(
                    session
                ),
            }

            # Mark session as completed with comprehensive final results
            completion_success = self.session_manager.complete_session(
                input_data.session_id, final_results=final_results
            )

            # Enhanced completion status handling
            if not completion_success:
                logger.warning(f"Session database completion failed for {input_data.session_id}")
                # Check if we have all the data needed for report generation
                if quality_metrics and session_summary and len(session_summary.get("detailed_steps", [])) > 0:
                    logger.info(f"Report generation can proceed with available data for {input_data.session_id}")
                    # For report purposes, consider it successful if we have the essential data
                    completion_success = True
                else:
                    logger.error(f"Insufficient data for report generation in session {input_data.session_id}")
                    final_results["completion_warning"] = (
                        "Session completion partially failed but report can still be generated"
                    )

            # Build enhanced template parameters for comprehensive summary
            template_params = self._build_comprehensive_summary_params(
                session,
                quality_metrics,
                session_summary,
                thinking_trace,
                input_data.final_insights,
            )

            # Get comprehensive summary template
            prompt_template = self.template_manager.get_template(
                "comprehensive_summary", template_params
            )

            # Generate detailed instructions for final report
            instructions = self._generate_completion_instructions(
                quality_metrics, session
            )

            return MCPToolOutput(
                tool_name=MCPToolName.COMPLETE_THINKING,
                session_id=input_data.session_id,
                step="generate_final_report",
                prompt_template=prompt_template,
                instructions=instructions,
                context={
                    "session_completed": True,
                    "total_steps": session_summary["total_steps"],  # Use consistent step count
                    "quality_metrics": quality_metrics,
                    "session_summary": session_summary,
                    "thinking_trace_available": True,
                    "final_results": final_results,
                    "completion_success": completion_success,
                },
                next_action="ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Šï¼Œæ€ç»´æµç¨‹å·²å®Œæˆ",
                metadata={
                    "session_status": "completed",
                    "completion_timestamp": final_results["completion_timestamp"],
                    "quality_summary": {
                        "average_quality": quality_metrics.get("average_quality", 0),
                        "quality_trend": quality_metrics.get("quality_trend", "stable"),
                        "total_steps": session_summary["total_steps"],  # Use consistent step count
                        "high_quality_steps": quality_metrics.get(
                            "high_quality_steps", 0
                        ),
                    },
                    "thinking_trace_available": True,
                    "report_generation_ready": True,
                    "session_duration_minutes": final_results[
                        "session_duration_minutes"
                    ],
                },
            )

        except Exception as e:
            try:
                return self.error_handler.handle_mcp_error(
                    tool_name="complete_thinking",
                    error=e,
                    session_id=input_data.session_id,
                    context={"final_insights": input_data.final_insights},
                )
            except Exception:
                # Fallback if error handler fails
                return self.error_handler._create_fallback_response(
                    "complete_thinking", e, input_data.session_id
                )

    def _build_template_params(
        self, session: SessionState, previous_result: str
    ) -> Dict[str, Any]:
        """Build template parameters from session context"""
        return {
            "topic": session.topic,
            "current_step": session.current_step,
            "previous_result": previous_result,
            "context": session.context,
            "step_results": session.step_results,
            "session_id": session.session_id,
        }

    def _determine_next_step_with_context(
        self,
        session: SessionState,
        step_result: str,
        quality_feedback: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        Determine next step with enhanced context awareness
        Considers quality feedback and adaptive flow control
        """
        # Check if quality gate requires retry
        if quality_feedback and quality_feedback.get("quality_score", 1.0) < 0.6:
            # Quality too low, suggest improvement step
            return {
                "step_name": f"improve_{session.current_step}",
                "template_name": "improvement_guidance",
                "instructions": "æ ¹æ®è´¨é‡åé¦ˆæ”¹è¿›å½“å‰æ­¥éª¤çš„ç»“æœ",
            }

        # Use flow manager for standard next step (with session state for accurate for_each tracking)
        next_step_info = self.flow_manager.get_next_step(
            session.flow_type, session.current_step, step_result, session
        )

        # Enhance with adaptive logic
        if next_step_info:
            # Adapt template based on complexity and context
            if session.context.get("complexity") == "complex":
                next_step_info["template_name"] = self._get_complex_template_variant(
                    next_step_info["template_name"]
                )

            # Add contextual instructions
            next_step_info["instructions"] = self._generate_contextual_instructions(
                next_step_info, session, step_result
            )

        return next_step_info

    def _build_enhanced_template_params(
        self,
        session: SessionState,
        previous_result: str,
        next_step_info: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Build enhanced template parameters with full context"""
        base_params = self._build_template_params(session, previous_result)

        # Add enhanced context
        enhanced_params = {
            **base_params,
            "step_name": next_step_info["step_name"],
            "step_type": next_step_info.get("step_type", "general"),
            "flow_progress": f"{session.step_number}/{self.flow_manager.get_total_steps(session.flow_type)}",
            "complexity": session.context.get("complexity", "moderate"),
            "focus": session.context.get("focus", ""),
            "domain_context": session.context.get(
                "domain_context", session.context.get("focus", "general analysis")
            ),
            "previous_steps_summary": self._get_previous_steps_summary(session),
            "quality_history": session.quality_scores,
            "session_duration": self._calculate_session_duration(session),
        }

        # Add step-specific context
        if next_step_info["step_name"] == "evidence":
            enhanced_params.update(self._get_evidence_context(session, previous_result))
        elif next_step_info["step_name"] == "evaluate":
            enhanced_params.update(self._get_evaluation_context(session))
        elif next_step_info["step_name"] == "reflect":
            enhanced_params.update(self._get_reflection_context(session))

        return enhanced_params

    def _get_contextual_template(
        self, template_name: str, template_params: Dict[str, Any], session: SessionState
    ) -> str:
        """Get template with smart contextual selection"""
        # Select appropriate template variant based on context
        selected_template = template_name

        # Adapt template based on session context
        if (
            session.context.get("complexity") == "simple"
            and template_name == "decomposition"
        ):
            selected_template = "simple_decomposition"
        elif (
            session.context.get("complexity") == "complex"
            and template_name == "critical_evaluation"
        ):
            selected_template = "advanced_critical_evaluation"

        # Fallback to original template if variant doesn't exist
        try:
            return self.template_manager.get_template(
                selected_template, template_params
            )
        except Exception:
            # Fall back to original template if variant doesn't exist
            return self.template_manager.get_template(template_name, template_params)

    def _build_step_context(
        self,
        session: SessionState,
        next_step_info: Dict[str, Any],
        step_number: int,
    ) -> Dict[str, Any]:
        """Build comprehensive context for the next step"""
        return {
            **session.context,
            "session_id": session.session_id,
            "topic": session.topic,
            "current_step": next_step_info["step_name"],
            "step_number": step_number,
            "flow_type": session.flow_type,
            "completed_steps": list(session.step_results.keys()),
            "quality_scores": session.quality_scores,
            "step_context": {
                "step_type": next_step_info.get("step_type", "general"),
                "template_used": next_step_info["template_name"],
                "dependencies_met": True,
                "adaptive_selection": True,
            },
        }

    def _generate_step_instructions(
        self, next_step_info: Dict[str, Any], session: SessionState
    ) -> str:
        """Generate contextual instructions for the step with for_each awareness"""
        base_instruction = next_step_info.get(
            "instructions", f"Execute {next_step_info['step_name']} step"
        )

        # CRITICAL: For_each specific instructions take priority
        if next_step_info.get("for_each_continuation"):
            step_name = next_step_info["step_name"]
            current_iterations = session.iteration_count.get(step_name, 0)
            total_iterations = session.total_iterations.get(step_name, 0)

            if total_iterations > 0:
                next_question_num = current_iterations + 1
                return f"ğŸ”„ è¯·å¤„ç†ç¬¬{next_question_num}ä¸ªå­é—®é¢˜ (å…±{total_iterations}ä¸ª)ã€‚å®Œæˆåå¿…é¡»è°ƒç”¨next_stepç»§ç»­å¾ªç¯ï¼Œä¸è¦æ“…è‡ªåœæ­¢æˆ–è·³è½¬åˆ°å…¶ä»–æ­¥éª¤ã€‚"
            else:
                return "ğŸ”„ è¯·å¤„ç†ä¸‹ä¸€ä¸ªå­é—®é¢˜ã€‚å®Œæˆåå¿…é¡»è°ƒç”¨next_stepç»§ç»­å¾ªç¯ã€‚"

        # Add normal contextual guidance for non-for_each steps
        contextual_additions = []

        if session.context.get("complexity") == "complex":
            contextual_additions.append("è¯·ç‰¹åˆ«æ³¨æ„åˆ†æçš„æ·±åº¦å’Œå…¨é¢æ€§")

        if session.quality_scores and min(session.quality_scores.values()) < 0.7:
            contextual_additions.append("è¯·æ³¨æ„æé«˜åˆ†æè´¨é‡ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°")

        if len(session.step_results) > 3:
            contextual_additions.append("è¯·å‚è€ƒä¹‹å‰æ­¥éª¤çš„ç»“æœï¼Œä¿æŒåˆ†æçš„è¿è´¯æ€§")

        if contextual_additions:
            return f"{base_instruction}ã€‚{' '.join(contextual_additions)}"

        return base_instruction

    def _determine_next_action(
        self, next_step_info: Dict[str, Any], session: SessionState
    ) -> str:
        """Determine the recommended next action with for_each awareness"""
        step_name = next_step_info["step_name"]

        # CRITICAL: Check if we're in for_each mode and give explicit guidance
        if next_step_info.get("for_each_continuation"):
            current_iterations = session.iteration_count.get(step_name, 0)
            total_iterations = session.total_iterations.get(step_name, 0)

            if total_iterations > 0:
                remaining = total_iterations - current_iterations
                if remaining > 0:
                    return f"ğŸ”„ ç»§ç»­å¤„ç†ç¬¬{current_iterations + 1}ä¸ªå­é—®é¢˜ (å‰©ä½™{remaining}ä¸ª)ï¼Œè¯·è°ƒç”¨next_stepç»§ç»­for_eachå¾ªç¯"
                else:
                    return "âœ… æ‰€æœ‰å­é—®é¢˜å·²å®Œæˆï¼Œå°†è‡ªåŠ¨è¿›å…¥ä¸‹ä¸€ä¸ªæ€ç»´é˜¶æ®µ"
            else:
                return "ğŸ”„ ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå­é—®é¢˜ï¼Œè¯·è°ƒç”¨next_stepç»§ç»­for_eachå¾ªç¯"

        # Normal (non-for_each) step guidance
        if step_name in ["decompose", "evidence", "collect_evidence"]:
            return "æ‰§è¡Œå½“å‰æ­¥éª¤åï¼Œå»ºè®®è°ƒç”¨analyze_stepè¿›è¡Œè´¨é‡æ£€æŸ¥"
        elif step_name in ["evaluate", "reflect"]:
            return "å®Œæˆå½“å‰æ­¥éª¤åï¼Œå¯ä»¥è°ƒç”¨complete_thinkingç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"
        else:
            return "ç»§ç»­æ‰§è¡Œæ€ç»´æµç¨‹ï¼Œæˆ–è°ƒç”¨analyze_stepè¿›è¡Œè´¨é‡æ£€æŸ¥"

    def _get_complex_template_variant(self, template_name: str) -> str:
        """Get complex variant of template if available"""
        complex_variants = {
            "decomposition": "advanced_decomposition",
            "critical_evaluation": "advanced_critical_evaluation",
            "evidence_collection": "comprehensive_evidence_collection",
        }
        return complex_variants.get(template_name, template_name)

    def _generate_contextual_instructions(
        self, next_step_info: Dict[str, Any], session: SessionState, step_result: str
    ) -> str:
        """Generate contextual instructions based on session state"""
        base_instruction = f"Execute {next_step_info['step_name']} step"

        # Add context-specific guidance
        if (
            "evidence" in step_result.lower()
            and next_step_info["step_name"] == "evaluate"
        ):
            return "åŸºäºæ”¶é›†çš„è¯æ®è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨è¯æ®è´¨é‡å’Œé€»è¾‘è¿è´¯æ€§"
        elif "é—®é¢˜åˆ†è§£" in step_result and next_step_info["step_name"] == "evidence":
            return "é’ˆå¯¹åˆ†è§£çš„å­é—®é¢˜æ”¶é›†ç›¸å…³è¯æ®ï¼Œç¡®ä¿æ¥æºå¤šæ ·åŒ–å’Œå¯ä¿¡åº¦"

        return base_instruction

    def _get_previous_steps_summary(self, session: SessionState) -> str:
        """Get summary of previous steps"""
        if not session.step_results:
            return "æš‚æ— å®Œæˆçš„æ­¥éª¤"

        summary_parts = []
        for step_name, result_data in session.step_results.items():
            if isinstance(result_data, dict):
                quality = result_data.get("quality_score", "N/A")
                summary_parts.append(f"- {step_name}: å·²å®Œæˆ (è´¨é‡: {quality})")
            else:
                summary_parts.append(f"- {step_name}: å·²å®Œæˆ")

        return "\n".join(summary_parts)

    def _calculate_session_duration(self, session: SessionState) -> str:
        """Calculate session duration in minutes"""
        if session.created_at:
            duration = (datetime.now() - session.created_at).total_seconds() / 60
            return f"{duration:.1f} åˆ†é’Ÿ"
        return "æœªçŸ¥"

    def _get_evidence_context(
        self, session: SessionState, previous_result: str
    ) -> Dict[str, Any]:
        """Get context specific to evidence collection step"""
        context = {}

        # Extract sub-questions from decomposition result
        if "sub_questions" in previous_result.lower():
            context["sub_question"] = "åŸºäºé—®é¢˜åˆ†è§£ç»“æœçš„å­é—®é¢˜"
            context["keywords"] = ["ç›¸å…³å…³é”®è¯", "æœç´¢è¯æ±‡"]

        return context

    def _get_evaluation_context(self, session: SessionState) -> Dict[str, Any]:
        """Get context specific to evaluation step"""
        return {
            "content": "ä¹‹å‰æ­¥éª¤çš„åˆ†æç»“æœ",
            "context": session.context.get("focus", "ç»¼åˆåˆ†æ"),
        }

    def _get_reflection_context(self, session: SessionState) -> Dict[str, Any]:
        """Get context specific to reflection step"""
        return {
            "thinking_history": self._get_previous_steps_summary(session),
            "current_conclusions": "åŸºäºå‰é¢æ­¥éª¤å¾—å‡ºçš„ç»“è®º",
        }

    def _get_analysis_template_name(self, step_name: str) -> str:
        """Get appropriate analysis template based on step name"""
        analysis_templates = {
            "decompose_problem": "analyze_decomposition",
            "collect_evidence": "analyze_evidence",
            "multi_perspective_debate": "analyze_debate",
            "critical_evaluation": "analyze_evaluation",
            "bias_detection": "analyze_evaluation",
            "innovation_thinking": "analyze_evaluation",
            "reflection": "analyze_reflection",
        }
        return analysis_templates.get(step_name, "analyze_evaluation")

    def _validate_step_format(self, step_name: str, step_result: str) -> Dict[str, Any]:
        """Validate the format of step results"""
        validation_result = {
            "valid": True,
            "issues": [],
            "expected_format": "",
            "format_requirements": "",
        }

        # Step-specific format validation
        if step_name == "decompose_problem":
            validation_result.update(self._validate_decomposition_format(step_result))
        elif step_name == "collect_evidence":
            validation_result.update(self._validate_evidence_format(step_result))
        elif step_name == "multi_perspective_debate":
            validation_result.update(self._validate_debate_format(step_result))
        elif step_name in ["critical_evaluation", "bias_detection"]:
            validation_result.update(self._validate_evaluation_format(step_result))
        elif step_name == "reflection":
            validation_result.update(self._validate_reflection_format(step_result))

        return validation_result

    def _validate_decomposition_format(self, step_result: str) -> Dict[str, Any]:
        """Validate decomposition step format"""
        issues = []

        # Check for JSON format
        if not (
            step_result.strip().startswith("{") and step_result.strip().endswith("}")
        ):
            issues.append("ç»“æœåº”ä¸ºJSONæ ¼å¼")

        # Check for required fields
        required_fields = ["main_question", "sub_questions", "relationships"]
        for field in required_fields:
            if field not in step_result:
                issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        # Check sub_questions structure
        if "sub_questions" in step_result and "id" not in step_result:
            issues.append("sub_questionsåº”åŒ…å«idå­—æ®µ")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "JSONæ ¼å¼ï¼ŒåŒ…å«main_question, sub_questions, relationshipså­—æ®µ",
            "format_requirements": "æ¯ä¸ªsub_questionéœ€åŒ…å«id, question, priority, search_keywordsç­‰å­—æ®µ",
        }

    def _validate_evidence_format(self, step_result: str) -> Dict[str, Any]:
        """Validate evidence collection format"""
        issues = []

        # Check for structured evidence
        if "æ¥æº" not in step_result and "source" not in step_result.lower():
            issues.append("åº”åŒ…å«è¯æ®æ¥æºä¿¡æ¯")

        if "å¯ä¿¡åº¦" not in step_result and "credibility" not in step_result.lower():
            issues.append("åº”åŒ…å«å¯ä¿¡åº¦è¯„ä¼°")

        if len(step_result) < 50:  # More lenient threshold for testing
            issues.append("è¯æ®æ”¶é›†ç»“æœè¿‡äºç®€çŸ­")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "ç»“æ„åŒ–è¯æ®é›†åˆï¼ŒåŒ…å«æ¥æºã€å¯ä¿¡åº¦ã€å…³é”®å‘ç°",
            "format_requirements": "æ¯ä¸ªè¯æ®æºåº”åŒ…å«URLã€æ ‡é¢˜ã€æ‘˜è¦ã€å¯ä¿¡åº¦è¯„åˆ†",
        }

    def _validate_debate_format(self, step_result: str) -> Dict[str, Any]:
        """Validate debate step format"""
        issues = []

        # Check for multiple perspectives
        perspective_indicators = [
            "æ”¯æŒ",
            "åå¯¹",
            "ä¸­ç«‹",
            "proponent",
            "opponent",
            "neutral",
        ]
        if not any(indicator in step_result for indicator in perspective_indicators):
            issues.append("åº”åŒ…å«å¤šä¸ªä¸åŒè§’åº¦çš„è§‚ç‚¹")

        # Check for argument structure
        if "è®ºæ®" not in step_result and "argument" not in step_result.lower():
            issues.append("åº”åŒ…å«å…·ä½“çš„è®ºæ®å’Œæ¨ç†")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "å¤šè§’åº¦è¾©è®ºç»“æœï¼ŒåŒ…å«ä¸åŒç«‹åœºçš„è§‚ç‚¹",
            "format_requirements": "æ¯ä¸ªè§’åº¦åº”åŒ…å«æ ¸å¿ƒè§‚ç‚¹ã€æ”¯æŒè®ºæ®ã€è´¨ç–‘è¦ç‚¹",
        }

    def _validate_evaluation_format(self, step_result: str) -> Dict[str, Any]:
        """Validate evaluation step format"""
        issues = []

        # Check for scoring
        if "è¯„åˆ†" not in step_result and "score" not in step_result.lower():
            issues.append("åº”åŒ…å«å…·ä½“çš„è¯„åˆ†")

        # Check for Paul-Elder standards (if applicable)
        paul_elder_standards = [
            "å‡†ç¡®æ€§",
            "ç²¾ç¡®æ€§",
            "ç›¸å…³æ€§",
            "é€»è¾‘æ€§",
            "å¹¿åº¦",
            "æ·±åº¦",
            "é‡è¦æ€§",
            "å…¬æ­£æ€§",
            "æ¸…æ™°æ€§",
        ]
        if any(standard in step_result for standard in paul_elder_standards[:3]):
            # If using Paul-Elder, check for comprehensive coverage
            missing_standards = [
                std for std in paul_elder_standards if std not in step_result
            ]
            if len(missing_standards) > 6:  # Allow some flexibility
                issues.append("Paul-Elderè¯„ä¼°åº”è¦†ç›–æ›´å¤šæ ‡å‡†")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "è¯„ä¼°ç»“æœåŒ…å«è¯„åˆ†å’Œè¯¦ç»†åˆ†æ",
            "format_requirements": "åº”åŒ…å«å„é¡¹æ ‡å‡†çš„è¯„åˆ†ã€ç†ç”±å’Œæ”¹è¿›å»ºè®®",
        }

    def _validate_reflection_format(self, step_result: str) -> Dict[str, Any]:
        """Validate reflection step format"""
        issues = []

        # Check for reflection depth
        reflection_indicators = [
            "åæ€",
            "å­¦åˆ°",
            "æ”¹è¿›",
            "æ´å¯Ÿ",
            "reflection",
            "insight",
        ]
        if not any(indicator in step_result for indicator in reflection_indicators):
            issues.append("åº”åŒ…å«æ·±åº¦åæ€å†…å®¹")

        # Check for metacognitive elements - more lenient for testing
        if len(step_result) < 20:
            issues.append("åæ€å†…å®¹åº”æ›´åŠ è¯¦ç»†å’Œæ·±å…¥")

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "expected_format": "æ·±åº¦åæ€ç»“æœï¼ŒåŒ…å«è¿‡ç¨‹åæ€å’Œå…ƒè®¤çŸ¥åˆ†æ",
            "format_requirements": "åº”åŒ…å«æ€ç»´è¿‡ç¨‹åˆ†æã€å­¦ä¹ æ”¶è·ã€æ”¹è¿›æ–¹å‘",
        }

    def _build_analysis_template_params(
        self,
        session: SessionState,
        step_name: str,
        step_result: str,
        analysis_type: str,
    ) -> Dict[str, Any]:
        """Build comprehensive parameters for analysis templates"""
        base_params = {
            "step_name": step_name,
            "step_result": step_result,
            "analysis_type": analysis_type,
            "session_context": session.context,
            "topic": session.topic,
        }

        # Add step-specific parameters
        if step_name == "decompose_problem":
            base_params.update(
                {
                    "original_topic": session.topic,
                    "complexity": session.context.get("complexity", "moderate"),
                }
            )
        elif step_name == "collect_evidence":
            base_params.update(
                {
                    "sub_question": self._extract_sub_question_from_context(session),
                    "search_keywords": self._extract_keywords_from_result(step_result),
                }
            )
        elif step_name == "multi_perspective_debate":
            base_params.update(
                {
                    "debate_topic": self._extract_debate_topic(session, step_result),
                    "evidence_context": self._get_evidence_context_summary(session),
                }
            )
        elif step_name in ["critical_evaluation", "bias_detection"]:
            base_params.update(
                {
                    "evaluated_content": self._get_evaluation_target(session),
                    "evaluation_context": session.context.get("focus", "ç»¼åˆåˆ†æ"),
                }
            )
        elif step_name == "reflection":
            base_params.update(
                {
                    "reflection_topic": session.topic,
                    "thinking_history": self._get_previous_steps_summary(session),
                    "current_conclusions": self._extract_current_conclusions(session),
                }
            )

        return base_params

    def _get_quality_threshold(self, step_name: str, flow_type: str) -> float:
        """Get quality threshold for specific step and flow type"""
        # Default thresholds by step type
        default_thresholds = {
            "decompose_problem": 7.0,
            "collect_evidence": 7.5,
            "multi_perspective_debate": 7.0,
            "critical_evaluation": 8.0,
            "bias_detection": 7.5,
            "innovation_thinking": 6.5,
            "reflection": 7.0,
        }

        # Adjust for flow type
        if flow_type == "comprehensive_analysis":
            return default_thresholds.get(step_name, 7.0)
        elif flow_type == "quick_analysis":
            return default_thresholds.get(step_name, 6.0) - 0.5
        else:
            return default_thresholds.get(step_name, 7.0)

    def _generate_improvement_suggestions(
        self, step_name: str, step_result: str, context: Dict[str, Any]
    ) -> str:
        """Generate step-specific improvement suggestions"""
        suggestions = []

        if step_name == "decompose_problem":
            if len(step_result) < 500:
                suggestions.append("é—®é¢˜åˆ†è§£åº”æ›´åŠ è¯¦ç»†ï¼Œæä¾›æ›´å¤šå­é—®é¢˜å’Œåˆ†æè§’åº¦")
            if "priority" not in step_result:
                suggestions.append("åº”ä¸ºæ¯ä¸ªå­é—®é¢˜è®¾å®šä¼˜å…ˆçº§")
            if "search_keywords" not in step_result:
                suggestions.append("åº”ä¸ºæ¯ä¸ªå­é—®é¢˜æä¾›æœç´¢å…³é”®è¯")

        elif step_name == "collect_evidence":
            if "http" not in step_result and "www" not in step_result:
                suggestions.append("åº”åŒ…å«å…·ä½“çš„ç½‘ç»œæ¥æºé“¾æ¥")
            if step_result.count("æ¥æº") < 3:
                suggestions.append("åº”æ”¶é›†æ›´å¤šä¸åŒæ¥æºçš„è¯æ®")
            if "å¯ä¿¡åº¦" not in step_result:
                suggestions.append("åº”å¯¹æ¯ä¸ªè¯æ®æ¥æºè¿›è¡Œå¯ä¿¡åº¦è¯„ä¼°")

        elif step_name == "multi_perspective_debate":
            if step_result.count("è§‚ç‚¹") < 3:
                suggestions.append("åº”åŒ…å«æ›´å¤šä¸åŒè§’åº¦çš„è§‚ç‚¹")
            if "åé©³" not in step_result and "è´¨ç–‘" not in step_result:
                suggestions.append("åº”åŒ…å«è§‚ç‚¹é—´çš„ç›¸äº’è´¨ç–‘å’Œåé©³")

        elif step_name in ["critical_evaluation", "bias_detection"]:
            if "è¯„åˆ†" not in step_result:
                suggestions.append("åº”åŒ…å«å…·ä½“çš„é‡åŒ–è¯„åˆ†")
            if "æ”¹è¿›å»ºè®®" not in step_result:
                suggestions.append("åº”æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®")

        elif step_name == "reflection":
            if len(step_result) < 400:
                suggestions.append("åæ€åº”æ›´åŠ æ·±å…¥å’Œè¯¦ç»†")
            if "å­¦åˆ°" not in step_result and "æ”¶è·" not in step_result:
                suggestions.append("åº”æ˜ç¡®è¯´æ˜å­¦ä¹ æ”¶è·å’Œæ´å¯Ÿ")

        return (
            "\n".join(f"- {suggestion}" for suggestion in suggestions)
            if suggestions
            else "å½“å‰ç»“æœè´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¿æŒ"
        )

    def _get_next_step_recommendation(
        self, step_name: str, session: SessionState
    ) -> str:
        """Get recommendation for next step based on current step"""
        recommendations = {
            "decompose_problem": "å¦‚æœè´¨é‡è¾¾æ ‡ï¼Œå»ºè®®ç»§ç»­è¯æ®æ”¶é›†æ­¥éª¤",
            "collect_evidence": "å¦‚æœè¯æ®å……åˆ†ï¼Œå»ºè®®è¿›è¡Œå¤šè§’åº¦è¾©è®ºåˆ†æ",
            "multi_perspective_debate": "å»ºè®®è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œæ£€æŸ¥è®ºè¯è´¨é‡",
            "critical_evaluation": "å¦‚æœè¯„ä¼°é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œåè§æ£€æµ‹æˆ–åˆ›æ–°æ€ç»´",
            "bias_detection": "å»ºè®®è¿›è¡Œåæ€æ­¥éª¤ï¼Œæ€»ç»“æ€ç»´è¿‡ç¨‹",
            "innovation_thinking": "å»ºè®®è¿›è¡Œæœ€ç»ˆåæ€å’Œæ€»ç»“",
            "reflection": "å¯ä»¥è°ƒç”¨complete_thinkingç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
        }
        return recommendations.get(step_name, "æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥")

    def _generate_analysis_instructions(
        self, step_name: str, analysis_type: str, quality_threshold: float
    ) -> str:
        """Generate step-specific analysis instructions"""
        base_instruction = f"è¯·æŒ‰ç…§{step_name}æ­¥éª¤çš„ä¸“é—¨è¯„ä¼°æ ‡å‡†è¿›è¡Œè¯¦ç»†åˆ†æ"

        quality_instruction = f"è´¨é‡é—¨æ§æ ‡å‡†ä¸º{quality_threshold}/10åˆ†ï¼Œè¯·ä¸¥æ ¼è¯„ä¼°"

        step_specific = {
            "decompose_problem": "é‡ç‚¹å…³æ³¨é—®é¢˜åˆ†è§£çš„å®Œæ•´æ€§ã€ç‹¬ç«‹æ€§å’Œå¯æ“ä½œæ€§",
            "collect_evidence": "é‡ç‚¹è¯„ä¼°è¯æ®æ¥æºçš„å¤šæ ·æ€§ã€å¯ä¿¡åº¦å’Œç›¸å…³æ€§",
            "multi_perspective_debate": "é‡ç‚¹åˆ†æè§‚ç‚¹çš„å¤šæ ·æ€§ã€è®ºè¯è´¨é‡å’Œäº’åŠ¨æ·±åº¦",
            "critical_evaluation": "é‡ç‚¹æ£€æŸ¥è¯„ä¼°æ ‡å‡†çš„åº”ç”¨å’Œè¯„åˆ†çš„å‡†ç¡®æ€§",
            "reflection": "é‡ç‚¹è¯„ä¼°åæ€çš„æ·±åº¦å’Œå…ƒè®¤çŸ¥è´¨é‡",
        }

        specific_instruction = step_specific.get(step_name, "è¯·è¿›è¡Œå…¨é¢çš„è´¨é‡åˆ†æ")

        return f"{base_instruction}ã€‚{quality_instruction}ã€‚{specific_instruction}ã€‚è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œä¸‹ä¸€æ­¥å»ºè®®ã€‚"

    def _determine_analysis_next_action(
        self, step_name: str, session: SessionState
    ) -> str:
        """Determine next action after analysis"""
        return f"æ ¹æ®{step_name}æ­¥éª¤çš„åˆ†æç»“æœï¼Œå¦‚æœè´¨é‡è¾¾æ ‡åˆ™ç»§ç»­ä¸‹ä¸€æ­¥ï¼Œå¦åˆ™éœ€è¦æ”¹è¿›å½“å‰æ­¥éª¤"

    def _get_analysis_criteria_count(self, step_name: str) -> int:
        """Get number of analysis criteria for the step"""
        criteria_counts = {
            "decompose_problem": 5,
            "collect_evidence": 5,
            "multi_perspective_debate": 5,
            "critical_evaluation": 5,
            "reflection": 5,
        }
        return criteria_counts.get(step_name, 5)

    def _handle_format_validation_failure(
        self, session_id: str, step_name: str, validation_result: Dict[str, Any]
    ) -> MCPToolOutput:
        """Handle format validation failure using error handler"""
        error = MCPFormatValidationError(
            f"Format validation failed for step {step_name}",
            step_name=step_name,
            expected_format=validation_result["expected_format"],
        )

        return self.error_handler.handle_mcp_error(
            tool_name="format_validator",
            error=error,
            session_id=session_id,
            context={
                "step_name": step_name,
                "format_issues": validation_result["issues"],
                "expected_format": validation_result["expected_format"],
            },
        )

    def _get_format_example(self, step_name: str) -> str:
        """Get format example for specific step"""
        examples = {
            "decompose_problem": """
{
  "main_question": "å¦‚ä½•æé«˜æ•™è‚²è´¨é‡ï¼Ÿ",
  "sub_questions": [
    {
      "id": "1",
      "question": "å½“å‰æ•™è‚²ä½“ç³»å­˜åœ¨å“ªäº›ä¸»è¦é—®é¢˜ï¼Ÿ",
      "priority": "high",
      "search_keywords": ["æ•™è‚²é—®é¢˜", "æ•™è‚²ä½“ç³»", "æ•™å­¦è´¨é‡"],
      "expected_perspectives": ["å­¦ç”Ÿè§†è§’", "æ•™å¸ˆè§†è§’", "å®¶é•¿è§†è§’"]
    }
  ],
  "relationships": ["é—®é¢˜1æ˜¯é—®é¢˜2çš„å‰æ"]
}""",
            "collect_evidence": """
è¯æ®æ¥æº1ï¼š
- æ ‡é¢˜ï¼šæ•™è‚²è´¨é‡ç ”ç©¶æŠ¥å‘Š
- URLï¼šhttps://example.com/report
- å¯ä¿¡åº¦ï¼š8/10
- å…³é”®å‘ç°ï¼š...

è¯æ®æ¥æº2ï¼š
- æ ‡é¢˜ï¼šä¸“å®¶è®¿è°ˆ
- URLï¼šhttps://example.com/interview  
- å¯ä¿¡åº¦ï¼š9/10
- å…³é”®å‘ç°ï¼š...""",
            "multi_perspective_debate": """
æ”¯æŒæ–¹è§‚ç‚¹ï¼š
- æ ¸å¿ƒè®ºç‚¹ï¼š...
- æ”¯æŒè®ºæ®ï¼š...

åå¯¹æ–¹è§‚ç‚¹ï¼š
- æ ¸å¿ƒè®ºç‚¹ï¼š...
- åé©³è®ºæ®ï¼š...

ä¸­ç«‹åˆ†æï¼š
- å¹³è¡¡è§‚ç‚¹ï¼š...
- ç»¼åˆè¯„ä¼°ï¼š...""",
        }
        return examples.get(step_name, "è¯·å‚è€ƒæ­¥éª¤è¦æ±‚çš„æ ‡å‡†æ ¼å¼")

    def _get_common_format_errors(self, step_name: str) -> str:
        """Get common format errors for specific step"""
        errors = {
            "decompose_problem": "å¸¸è§é”™è¯¯ï¼šå¿˜è®°JSONæ ¼å¼ã€ç¼ºå°‘å¿…éœ€å­—æ®µã€å­é—®é¢˜æè¿°è¿‡äºç®€å•",
            "collect_evidence": "å¸¸è§é”™è¯¯ï¼šç¼ºå°‘æ¥æºé“¾æ¥ã€æ²¡æœ‰å¯ä¿¡åº¦è¯„ä¼°ã€è¯æ®è¿‡äºç®€å•",
            "multi_perspective_debate": "å¸¸è§é”™è¯¯ï¼šè§‚ç‚¹å•ä¸€ã€ç¼ºå°‘äº’åŠ¨ã€è®ºæ®ä¸å……åˆ†",
        }
        return errors.get(step_name, "è¯·ç¡®ä¿æ ¼å¼å®Œæ•´å’Œè§„èŒƒ")

    # Helper methods for extracting information from session context
    def _extract_sub_question_from_context(self, session: SessionState) -> str:
        """Extract sub-question from session context"""
        # This would extract from previous decomposition results
        return session.context.get("current_sub_question", "åŸºäºé—®é¢˜åˆ†è§£çš„å­é—®é¢˜")

    def _extract_keywords_from_result(self, step_result: str) -> str:
        """Extract keywords from step result"""
        # Simple keyword extraction - in practice this could be more sophisticated
        return "ç›¸å…³æœç´¢å…³é”®è¯"

    def _extract_debate_topic(self, session: SessionState, step_result: str) -> str:
        """Extract debate topic from context"""
        return session.context.get("debate_topic", session.topic)

    def _get_evidence_context_summary(self, session: SessionState) -> str:
        """Get summary of evidence collection context"""
        return "åŸºäºè¯æ®æ”¶é›†çš„èƒŒæ™¯ä¿¡æ¯"

    def _get_evaluation_target(self, session: SessionState) -> str:
        """Get the target content for evaluation"""
        return "éœ€è¦è¯„ä¼°çš„å†…å®¹"

    def _extract_current_conclusions(self, session: SessionState) -> str:
        """Extract current conclusions from session"""
        return "åŸºäºå‰é¢æ­¥éª¤å¾—å‡ºçš„å½“å‰ç»“è®º"

    def _handle_session_not_found(self, session_id: str) -> MCPToolOutput:
        """Handle case where session is not found using error handler"""
        error = SessionNotFoundError(
            f"Session {session_id} not found", session_id=session_id
        )

        return self.error_handler.handle_mcp_error(
            tool_name="session_manager",
            error=error,
            session_id=session_id,
            context={"session_id": session_id},
        )

    def _handle_flow_completion(self, session_id: str) -> MCPToolOutput:
        """Handle flow completion with safety checks"""
        # SAFETY CHECK: Verify no active for_each iterations before completing
        session = self.session_manager.get_session(session_id)
        if session:
            for step_name, current_count in session.iteration_count.items():
                total_count = session.total_iterations.get(step_name, 0)
                if current_count < total_count and total_count > 0:
                    logger.error(
                        f"ğŸš¨ CRITICAL: Attempted completion with active for_each: {step_name} at {current_count}/{total_count}"
                    )
                    raise RuntimeError(
                        f"Cannot complete flow with active for_each iterations: {step_name} {current_count}/{total_count}"
                    )

        logger.info("ğŸ CONFIRMED SAFE COMPLETION: All iterations verified complete")
        completion_prompt = self.template_manager.get_template(
            "flow_completion", {"session_id": session_id}
        )

        return MCPToolOutput(
            tool_name=MCPToolName.COMPLETE_THINKING,
            session_id=session_id,
            step="flow_completed",
            prompt_template=completion_prompt,
            instructions="æ€ç»´æµç¨‹å·²å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
            context={"flow_completed": True},
            next_action="è°ƒç”¨complete_thinkingç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
            metadata={
                "ready_for_completion": True,
                "completion_verified": True,
                "all_iterations_complete": True,
            },
        )

    def _handle_error(
        self, tool_name: str, error_message: str, session_id: Optional[str] = None
    ) -> MCPToolOutput:
        """Handle tool execution errors using the error handler"""
        # Create a generic exception for the error handler
        error = MCPToolExecutionError(
            error_message, tool_name=tool_name, session_id=session_id
        )

        # Use the error handler to create appropriate recovery response
        return self.error_handler.handle_mcp_error(
            tool_name=tool_name,
            error=error,
            session_id=session_id,
            context={"error_message": error_message},
        )

    def _validate_completion_eligibility(self, session: SessionState) -> Dict[str, Any]:
        """
        Validate whether the session is actually eligible for completion
        CRITICAL: This prevents HOST from bypassing for_each iterations
        """
        try:
            # Check all for_each steps for incomplete iterations
            for step_name, current_count in session.iteration_count.items():
                total_count = session.total_iterations.get(step_name, 0)

                if current_count < total_count and total_count > 0:
                    # Found incomplete for_each - completion not allowed
                    remaining = total_count - current_count

                    return {
                        "allowed": False,
                        "reason": f"Active for_each in {step_name}: {current_count}/{total_count} (ç¼ºå°‘{remaining}ä¸ª)",
                        "current_state": f"{step_name} at {current_count}/{total_count}",
                        "required_step": step_name,
                        "continuation_template": self.template_manager.get_template(
                            "evidence_collection",
                            {"sub_question": f"ç¬¬{current_count + 1}ä¸ªå­é—®é¢˜"},
                        ),
                        "continuation_instruction": f"å¿…é¡»å®Œæˆå‰©ä½™{remaining}ä¸ªå­é—®é¢˜çš„{step_name}å¤„ç†",
                        "next_action": f"ç»§ç»­å¤„ç†ç¬¬{current_count + 1}ä¸ªå­é—®é¢˜",
                        "current_iterations": current_count,
                        "total_iterations": total_count,
                        "iteration_status": {
                            "current": current_count,
                            "total": total_count,
                            "remaining": remaining,
                            "step": step_name,
                        },
                    }

            # All for_each iterations complete - completion allowed
            return {
                "allowed": True,
                "reason": "All for_each iterations completed",
                "validation_passed": True,
            }

        except Exception as e:
            logger.error(f"Error validating completion eligibility: {e}")
            # Default to blocking completion if validation fails
            return {
                "allowed": False,
                "reason": f"Validation error: {e}",
                "error_occurred": True,
            }

    # Enhanced helper methods for complete_thinking tool

    def _calculate_comprehensive_quality_metrics(
        self, session: SessionState
    ) -> Dict[str, Any]:
        """
        Calculate comprehensive quality metrics for the session

        Returns detailed quality analysis including:
        - Average quality score
        - Quality trend analysis
        - Step-by-step quality breakdown
        - Quality distribution
        - Improvement recommendations
        """
        quality_scores = session.quality_scores
        
        # Get actual step count from database for consistency with session summary
        try:
            steps = self.session_manager.db.get_session_steps(session.session_id)
            actual_step_count = len(steps)
        except Exception:
            actual_step_count = session.step_number

        if not quality_scores:
            return {
                "average_quality": 0.0,
                "quality_trend": "no_data",
                "total_steps": actual_step_count,
                "high_quality_steps": 0,
                "quality_distribution": {},
                "improvement_areas": ["No quality data available"],
                "overall_assessment": "insufficient_data",
            }

        # Calculate basic metrics
        scores = list(quality_scores.values())
        average_quality = sum(scores) / len(scores)
        high_quality_steps = sum(1 for score in scores if score >= 8.0)

        # Quality trend analysis
        quality_trend = self._analyze_quality_trend(scores)

        # Quality distribution
        quality_distribution = {
            "excellent": sum(1 for score in scores if score >= 9.0),
            "good": sum(1 for score in scores if 7.0 <= score < 9.0),
            "acceptable": sum(1 for score in scores if 5.0 <= score < 7.0),
            "needs_improvement": sum(1 for score in scores if score < 5.0),
        }

        # Identify improvement areas
        improvement_areas = self._identify_improvement_areas(quality_scores)

        # Overall assessment
        overall_assessment = self._determine_overall_assessment(
            average_quality, quality_distribution
        )

        return {
            "average_quality": round(average_quality, 2),
            "quality_trend": quality_trend,
            "total_steps": actual_step_count,
            "high_quality_steps": high_quality_steps,
            "quality_distribution": quality_distribution,
            "step_quality_breakdown": quality_scores,
            "improvement_areas": improvement_areas,
            "overall_assessment": overall_assessment,
            "quality_consistency": self._calculate_quality_consistency(scores),
            "best_performing_step": (
                max(quality_scores.items(), key=lambda x: x[1])
                if quality_scores
                else None
            ),
            "lowest_performing_step": (
                min(quality_scores.items(), key=lambda x: x[1])
                if quality_scores
                else None
            ),
        }

    def _analyze_quality_trend(self, scores: List[float]) -> str:
        """Analyze the trend in quality scores"""
        if len(scores) < 2:
            return "insufficient_data"

        # Simple trend analysis
        first_half = scores[: len(scores) // 2]
        second_half = scores[len(scores) // 2 :]

        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)

        diff = second_avg - first_avg

        if diff > 0.5:
            return "improving"
        elif diff < -0.5:
            return "declining"
        else:
            return "stable"

    def _identify_improvement_areas(
        self, quality_scores: Dict[str, float]
    ) -> List[str]:
        """Identify areas that need improvement based on quality scores"""
        improvement_areas = []

        for step_name, score in quality_scores.items():
            if score < 6.0:
                improvement_areas.append(f"{step_name} (å¾—åˆ†: {score})")

        if not improvement_areas:
            improvement_areas.append("æ‰€æœ‰æ­¥éª¤è´¨é‡è‰¯å¥½")

        return improvement_areas

    def _determine_overall_assessment(
        self, average_quality: float, quality_distribution: Dict[str, int]
    ) -> str:
        """Determine overall quality assessment"""
        if average_quality >= 8.5:
            return "excellent"
        elif average_quality >= 7.0:
            return "good"
        elif average_quality >= 5.0:
            return "acceptable"
        else:
            return "needs_improvement"

    def _calculate_quality_consistency(self, scores: List[float]) -> float:
        """Calculate quality consistency (lower variance = higher consistency)"""
        if len(scores) < 2:
            return 1.0

        mean = sum(scores) / len(scores)
        variance = sum((score - mean) ** 2 for score in scores) / len(scores)

        # Convert to consistency score (0-1, higher is more consistent)
        consistency = max(0, 1 - (variance / 10))  # Normalize variance
        return round(consistency, 3)

    def _generate_detailed_session_summary(
        self, session: SessionState
    ) -> Dict[str, Any]:
        """Generate detailed session summary with step analysis"""

        # Get step details from database
        try:
            steps = self.session_manager.db.get_session_steps(session.session_id)
            results = self.session_manager.db.get_step_results(session.session_id)
        except Exception:
            steps = []
            results = []

        # Organize results by step
        step_results_map = {}
        for result in results:
            step_id = result.get("step_id")
            if step_id not in step_results_map:
                step_results_map[step_id] = []
            step_results_map[step_id].append(result)

        # Build detailed step summary with proper quality score synchronization
        detailed_steps = []
        for step in steps:
            step_name = step.get("step_name", "unknown")

            # CRITICAL FIX: Use session.quality_scores as primary source
            # Fall back to database value only if session doesn't have it
            quality_score = session.quality_scores.get(step_name) or step.get(
                "quality_score"
            )

            step_summary = {
                "step_name": step_name,
                "step_type": step.get("step_type", "general"),
                "quality_score": quality_score,
                "execution_time_ms": step.get("execution_time_ms"),
                "results_count": len(step_results_map.get(step.get("id"), [])),
                "timestamp": step.get("created_at"),
            }
            detailed_steps.append(step_summary)

        return {
            "session_id": session.session_id,
            "topic": session.topic,
            "flow_type": session.flow_type,
            "total_steps": len(detailed_steps),
            "session_duration": self._calculate_session_duration_minutes(session),
            "detailed_steps": detailed_steps,
            "context_summary": session.context,
            "completion_status": "completed",
        }

    def _calculate_session_duration_minutes(self, session: SessionState) -> float:
        """Calculate session duration in minutes"""
        if session.created_at and session.updated_at:
            duration = (session.updated_at - session.created_at).total_seconds() / 60
            return round(duration, 2)
        return 0.0

    def _extract_detailed_step_contents(self, session_id: str) -> Dict[str, Any]:
        """
        Extract detailed contents from all steps for comprehensive summary
        Returns step contents grouped by step with actual analysis content
        """
        try:
            # Get all step results from database
            results = self.session_manager.db.get_step_results(session_id)
            steps = self.session_manager.db.get_session_steps(session_id)

            # Create a mapping of step_id to step_name
            step_id_to_name = {}
            for step in steps:
                step_id_to_name[step.get("id")] = step.get("step_name", "unknown")

            # Group results by step
            step_contents = {}
            for result in results:
                step_id = result.get("step_id")
                step_name = step_id_to_name.get(step_id, f"step_{step_id}")

                if step_name not in step_contents:
                    step_contents[step_name] = []

                content_data = {
                    "content": result.get("content", ""),
                    "result_type": result.get("result_type", "output"),
                    "metadata": result.get("metadata", {}),
                    "quality_indicators": result.get("quality_indicators", {}),
                    "timestamp": result.get("created_at", ""),
                }
                step_contents[step_name].append(content_data)

            return step_contents
        except Exception as e:
            logger.error(
                f"Error extracting detailed step contents for {session_id}: {e}"
            )
            return {}

    def _build_step_analysis_summary(
        self, detailed_step_contents: Dict[str, Any]
    ) -> str:
        """
        Build a comprehensive analysis summary from step contents (NO TRUNCATION)
        Provides complete content for modern LLMs with large context windows
        Now with intelligent JSON parsing and formatting for better readability
        """
        if not detailed_step_contents:
            return "æ— è¯¦ç»†åˆ†æå†…å®¹"

        summary_parts = []
        total_chars = 0

        for step_name, contents in detailed_step_contents.items():
            if not contents:
                continue

            # Get the main analysis content (prioritize 'output' type)
            raw_content = ""
            for content_data in contents:
                if content_data.get("result_type") == "output" and content_data.get(
                    "content"
                ):
                    raw_content = content_data.get(
                        "content", ""
                    )  # FULL CONTENT - no truncation
                    break

            if not raw_content and contents:
                # Fallback to any content
                raw_content = contents[0].get(
                    "content", ""
                )  # FULL CONTENT - no truncation

            if raw_content:
                # Format the content intelligently
                formatted_content = self._format_step_content(step_name, raw_content)
                step_content_length = len(formatted_content)
                total_chars += step_content_length

                # Add content statistics for transparency
                content_info = f"*[å†…å®¹é•¿åº¦: {step_content_length:,} å­—ç¬¦]*\n\n"
                summary_parts.append(f"### {step_name}\n{content_info}{formatted_content}")

        # Add summary statistics
        summary_header = (
            f"*æ€»åˆ†æå†…å®¹: {total_chars:,} å­—ç¬¦ï¼Œ{len(summary_parts)} ä¸ªæ­¥éª¤*\n\n"
        )
        full_content = "\n\n".join(summary_parts) if summary_parts else "æ— æœ‰æ•ˆåˆ†æå†…å®¹"

        return summary_header + full_content

    def _format_step_content(self, step_name: str, raw_content: str) -> str:
        """
        Format step content intelligently - parse JSON if possible, otherwise return as-is
        Converts raw data into human-readable analysis content
        """
        try:
            # Try to parse as JSON first
            import json
            data = json.loads(raw_content)
            
            # Format based on step type
            if step_name == "decompose_problem":
                return self._format_decompose_content(data)
            elif step_name == "collect_evidence":
                return self._format_evidence_content(data)
            elif step_name in ["evaluate", "critical_evaluation"]:
                return self._format_evaluation_content(data)
            elif step_name == "reflect":
                return self._format_reflection_content(data)
            elif step_name == "multi_perspective_debate":
                return self._format_debate_content(data)
            else:
                # Generic JSON formatting
                return self._format_generic_json(data)
                
        except (json.JSONDecodeError, TypeError):
            # If it's not JSON, return the content as-is
            return raw_content
            
    def _format_decompose_content(self, data: dict) -> str:
        """Format decomposition step content"""
        formatted = []
        
        if "main_question" in data:
            formatted.append(f"**æ ¸å¿ƒé—®é¢˜**: {data['main_question']}")
            formatted.append("")
        
        if "complexity_level" in data:
            formatted.append(f"**å¤æ‚åº¦çº§åˆ«**: {data['complexity_level']}")
            formatted.append("")
            
        if "sub_questions" in data:
            formatted.append("**å­é—®é¢˜åˆ†è§£**:")
            for i, sq in enumerate(data["sub_questions"], 1):
                formatted.append(f"{i}. **{sq.get('id', f'SQ{i}')}**: {sq.get('question', 'æœªçŸ¥é—®é¢˜')}")
                if sq.get('priority'):
                    formatted.append(f"   - ä¼˜å…ˆçº§: {sq['priority']}")
                if sq.get('search_keywords'):
                    keywords = ", ".join(sq['search_keywords'])
                    formatted.append(f"   - å…³é”®è¯: {keywords}")
                formatted.append("")
        
        if "relationships" in data:
            formatted.append("**é—®é¢˜å…³è”æ€§**:")
            for rel in data["relationships"]:
                formatted.append(f"- {rel.get('from')} â†’ {rel.get('to')}: {rel.get('description', 'ç›¸å…³')}")
            formatted.append("")
            
        if "coverage_analysis" in data:
            ca = data["coverage_analysis"]
            if "key_aspects_covered" in ca:
                aspects = ", ".join(ca["key_aspects_covered"])
                formatted.append(f"**è¦†ç›–åˆ†æ**: {aspects}")
                formatted.append("")
        
        return "\n".join(formatted)
    
    def _format_evidence_content(self, data: dict) -> str:
        """Format evidence collection content"""
        formatted = []
        
        if "sub_question" in data:
            formatted.append(f"**ç ”ç©¶é—®é¢˜**: {data['sub_question']}")
            formatted.append("")
        
        if "evidence_collection" in data:
            formatted.append("**è¯æ®æ”¶é›†ç»“æœ**:")
            for i, evidence in enumerate(data["evidence_collection"], 1):
                formatted.append(f"\n**è¯æ® {i}: {evidence.get('source_name', 'æœªçŸ¥æ¥æº')}**")
                if evidence.get('credibility_score'):
                    formatted.append(f"- å¯ä¿¡åº¦: {evidence['credibility_score']}/10")
                if evidence.get('key_findings'):
                    formatted.append("- å…³é”®å‘ç°:")
                    for finding in evidence['key_findings']:
                        formatted.append(f"  â€¢ {finding}")
                if evidence.get('quantitative_data'):
                    formatted.append("- é‡åŒ–æ•°æ®:")
                    for qdata in evidence['quantitative_data']:
                        formatted.append(f"  â€¢ {qdata}")
                formatted.append("")
        
        if "evidence_synthesis" in data:
            es = data["evidence_synthesis"]
            if "main_findings" in es:
                formatted.append("**ç»¼åˆå‘ç°**:")
                for finding in es["main_findings"]:
                    formatted.append(f"â€¢ {finding}")
                formatted.append("")
            
            if "practical_recommendations" in es:
                formatted.append("**å®è·µå»ºè®®**:")
                for rec in es["practical_recommendations"]:
                    formatted.append(f"â€¢ {rec}")
                formatted.append("")
        
        return "\n".join(formatted)
    
    def _format_evaluation_content(self, data: dict) -> str:
        """Format evaluation content"""
        formatted = []
        
        if isinstance(data, dict):
            if "æ‰§è¡Œæ‘˜è¦" in data or "executive_summary" in data:
                summary = data.get("æ‰§è¡Œæ‘˜è¦") or data.get("executive_summary")
                formatted.append(f"**æ‰§è¡Œæ‘˜è¦**\n{summary}")
                formatted.append("")
            
            if "è¯æ®å¯ä¿¡åº¦çŸ©é˜µ" in data:
                formatted.append("**è¯æ®å¯ä¿¡åº¦çŸ©é˜µ**:")
                matrix = data["è¯æ®å¯ä¿¡åº¦çŸ©é˜µ"]
                for key, value in matrix.items():
                    formatted.append(f"â€¢ {key}: {value}")
                formatted.append("")
            
            if "æ‰¹åˆ¤æ€§æ´å¯Ÿ" in data:
                insights = data["æ‰¹åˆ¤æ€§æ´å¯Ÿ"]
                if "æ ¸å¿ƒä¼˜åŠ¿" in insights:
                    formatted.append("**æ ¸å¿ƒä¼˜åŠ¿**:")
                    for advantage in insights["æ ¸å¿ƒä¼˜åŠ¿"]:
                        formatted.append(f"â€¢ {advantage}")
                    formatted.append("")
                
                if "å…³é”®å¼±ç‚¹" in insights:
                    formatted.append("**å…³é”®å¼±ç‚¹**:")
                    for weakness in insights["å…³é”®å¼±ç‚¹"]:
                        formatted.append(f"â€¢ {weakness}")
                    formatted.append("")
            
            if "æˆ˜ç•¥å»ºè®®" in data:
                formatted.append("**æˆ˜ç•¥å»ºè®®**:")
                suggestions = data["æˆ˜ç•¥å»ºè®®"]
                if "ç«‹å³è¡ŒåŠ¨å»ºè®®" in suggestions:
                    formatted.append("ç«‹å³è¡ŒåŠ¨å»ºè®®:")
                    for action in suggestions["ç«‹å³è¡ŒåŠ¨å»ºè®®"]:
                        if isinstance(action, dict):
                            formatted.append(f"â€¢ {action.keys()}")
                        else:
                            formatted.append(f"â€¢ {action}")
                formatted.append("")
        else:
            formatted.append(str(data))
        
        return "\n".join(formatted)
    
    def _format_reflection_content(self, data: dict) -> str:
        """Format reflection content"""
        formatted = []
        
        if "æ•´ä½“è¯„ä¼°" in data:
            formatted.append(f"**æ•´ä½“è¯„ä¼°**: {data['æ•´ä½“è¯„ä¼°']}")
            formatted.append("")
        
        if "ä¸»è¦ä¼˜åŠ¿" in data:
            formatted.append("**ä¸»è¦ä¼˜åŠ¿**:")
            for advantage in data["ä¸»è¦ä¼˜åŠ¿"]:
                formatted.append(f"â€¢ {advantage}")
            formatted.append("")
        
        if "å…³é”®ä¸è¶³" in data:
            formatted.append("**å…³é”®ä¸è¶³**:")
            for weakness in data["å…³é”®ä¸è¶³"]:
                formatted.append(f"â€¢ {weakness}")
            formatted.append("")
        
        if "ç¡®å®šæ€§åˆ†æ" in data:
            ca = data["ç¡®å®šæ€§åˆ†æ"]
            for level, items in ca.items():
                formatted.append(f"**{level}**:")
                for item in items:
                    formatted.append(f"â€¢ {item}")
                formatted.append("")
        
        if "æ”¹è¿›å»ºè®®" in data:
            formatted.append("**æ”¹è¿›å»ºè®®**:")
            if "å³åˆ»æ”¹è¿›" in data["æ”¹è¿›å»ºè®®"]:
                formatted.append("å³åˆ»æ”¹è¿›:")
                for improvement in data["æ”¹è¿›å»ºè®®"]["å³åˆ»æ”¹è¿›"]:
                    formatted.append(f"â€¢ {improvement}")
            if "é•¿æœŸæå‡" in data["æ”¹è¿›å»ºè®®"]:
                formatted.append("é•¿æœŸæå‡:")
                for improvement in data["æ”¹è¿›å»ºè®®"]["é•¿æœŸæå‡"]:
                    formatted.append(f"â€¢ {improvement}")
            formatted.append("")
        
        return "\n".join(formatted)
    
    def _format_debate_content(self, data: dict) -> str:
        """Format debate content"""
        formatted = []
        
        if "debate_summary" in data:
            formatted.append(f"**è¾©è®ºæ€»ç»“**: {data['debate_summary']}")
            formatted.append("")
        
        if "perspectives" in data:
            for i, perspective in enumerate(data["perspectives"], 1):
                stance = perspective.get("stance", f"è§‚ç‚¹{i}")
                formatted.append(f"**{stance}æ–¹è§‚ç‚¹**:")
                if "main_arguments" in perspective:
                    for arg in perspective["main_arguments"]:
                        formatted.append(f"â€¢ {arg}")
                formatted.append("")
        
        return "\n".join(formatted)
    
    def _format_generic_json(self, data: dict) -> str:
        """Generic JSON formatting fallback"""
        formatted = []
        
        def format_value(value, indent=0):
            prefix = "  " * indent
            if isinstance(value, dict):
                for k, v in value.items():
                    if isinstance(v, (dict, list)):
                        formatted.append(f"{prefix}**{k}**:")
                        format_value(v, indent + 1)
                    else:
                        formatted.append(f"{prefix}**{k}**: {v}")
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        formatted.append(f"{prefix}{i+1}.")
                        format_value(item, indent + 1)
                    else:
                        formatted.append(f"{prefix}â€¢ {item}")
            else:
                formatted.append(f"{prefix}{value}")
        
        format_value(data)
        return "\n".join(formatted)

    def _calculate_auto_quality_score(
        self, content: str, metadata: Dict[str, Any] = None
    ) -> float:
        """
        Calculate automatic quality score based on content characteristics
        """
        if not content or len(content.strip()) < 10:
            return 1.0

        score = 5.0  # Base score

        # Content length factor (more comprehensive analysis gets higher score)
        length_factor = min(len(content) / 1000.0, 3.0)  # Max 3 points for length
        score += length_factor

        # Structure and organization factor
        if "##" in content or "###" in content:  # Has headers
            score += 0.5
        if content.count("\n") > 5:  # Well-structured with line breaks
            score += 0.5

        # Content quality indicators
        quality_keywords = [
            "åˆ†æ",
            "ç»“è®º",
            "å»ºè®®",
            "è¯„ä¼°",
            "æ€»ç»“",
            "æ´å¯Ÿ",
            "å‘ç°",
            "ç­–ç•¥",
        ]
        keyword_count = sum(1 for keyword in quality_keywords if keyword in content)
        score += min(keyword_count * 0.2, 1.0)  # Max 1 point for keywords

        # Metadata quality indicators
        if metadata:
            if metadata.get("citations"):
                score += 0.5  # Has citations
            if metadata.get("evidence_count", 0) > 0:
                score += 0.5  # Has evidence

        return min(score, 10.0)  # Cap at 10.0

    def get_export_directory(self) -> Path:
        """
        Get the export directory using priority chain configuration
        
        Priority order:
        1. DEEP_THINKING_EXPORT_DIR environment variable
        2. Configuration file export.base_directory
        3. DEEP_THINKING_DATA_DIR/exports environment variable  
        4. XDG_DATA_HOME/deep-thinking/exports (Linux/Mac standard)
        5. User home ~/.deep-thinking/exports (fallback)
        6. Temp directory (final fallback)
        
        Returns:
            Path object for export directory
        """
        import os
        import tempfile
        
        # 1. Check primary environment variable
        if export_dir := os.getenv('DEEP_THINKING_EXPORT_DIR'):
            path = Path(export_dir)
            logger.info(f"Using export directory from DEEP_THINKING_EXPORT_DIR: {path}")
            return path
            
        # 2. Check configuration file (if available)
        try:
            if hasattr(self, 'config_manager') and self.config_manager:
                config_dir = self.config_manager.get('export.base_directory')
                if config_dir:
                    path = Path(os.path.expanduser(config_dir))
                    logger.info(f"Using export directory from config: {path}")
                    return path
        except Exception as e:
            logger.debug(f"Could not read config for export directory: {e}")
            
        # 3. Check data directory environment variable
        if data_dir := os.getenv('DEEP_THINKING_DATA_DIR'):
            path = Path(data_dir) / 'exports'
            logger.info(f"Using export directory from DEEP_THINKING_DATA_DIR: {path}")
            return path
            
        # 4. XDG Base Directory standard (Linux/Mac)
        if xdg_data := os.getenv('XDG_DATA_HOME'):
            path = Path(xdg_data) / 'deep-thinking' / 'exports'
            logger.info(f"Using XDG standard export directory: {path}")
            return path
            
        # 5. User home directory (most common fallback)
        try:
            path = Path.home() / '.deep-thinking' / 'exports'
            logger.info(f"Using user home export directory: {path}")
            return path
        except Exception as e:
            logger.warning(f"Could not access user home directory: {e}")
            
        # 6. Temp directory (final fallback)
        path = Path(tempfile.gettempdir()) / 'deep-thinking-exports'
        logger.warning(f"Using temporary export directory: {path}")
        return path

    def _validate_export_directory(self, export_dir: Path) -> Dict[str, Any]:
        """
        Validate and prepare export directory
        
        Args:
            export_dir: Path to validate
            
        Returns:
            Dict with validation results and warnings
        """
        result = {
            "valid": True,
            "created": False,
            "warnings": []
        }
        
        try:
            # Check if directory is inside a git repository (warn if so)
            current_path = export_dir.absolute()
            for parent in current_path.parents:
                if (parent / '.git').exists():
                    # Check if it's the same as current working directory
                    if parent == Path.cwd():
                        result["warnings"].append(
                            f"Export directory {export_dir} is inside the current git repository. "
                            "Consider setting DEEP_THINKING_EXPORT_DIR to avoid git status pollution."
                        )
                    break
            
            # Create directory if it doesn't exist
            if not export_dir.exists():
                export_dir.mkdir(parents=True, exist_ok=True)
                result["created"] = True
                logger.info(f"Created export directory: {export_dir}")
            
            # Check write permissions
            test_file = export_dir / '.write_test'
            try:
                test_file.write_text("test")
                test_file.unlink()
            except Exception as e:
                result["valid"] = False
                result["warnings"].append(f"No write permission for directory {export_dir}: {e}")
                
        except Exception as e:
            result["valid"] = False
            result["warnings"].append(f"Failed to validate/create export directory {export_dir}: {e}")
            logger.error(f"Export directory validation failed: {e}")
            
        return result

    def _generate_export_filename(self, session, custom_title: Optional[str] = None) -> str:
        """
        Generate export filename based on configuration
        
        Args:
            session: Session state object
            custom_title: Optional concise title from Host (20 chars or less recommended)
            
        Returns:
            Generated filename string
        """
        try:
            # Get configuration with defaults - get the entire config tree
            if not self.config_manager:
                logger.debug("Config manager not available, using defaults")
                config = {}
            else:
                config = self.config_manager.config_data  # Direct access to full config
                logger.debug(f"Config data keys: {list(config.keys()) if config else 'None'}")
            
            export_config = config.get("export", {})
            logger.debug(f"Export config: {export_config}")
            file_naming = export_config.get("file_naming", {})
            logger.debug(f"File naming config: {file_naming}")
            
            pattern = file_naming.get("pattern", "{topic}_{timestamp}.md")
            max_topic_length = file_naming.get("max_topic_length", 20)
            include_session_id = file_naming.get("include_session_id", False)
            sanitize_topic = file_naming.get("sanitize_topic", True)
            
            logger.debug(f"File naming config - pattern: {pattern}, max_length: {max_topic_length}")
            logger.debug(f"Custom title provided: {custom_title}")
            
            # Generate components
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Use custom title if provided, otherwise extract from session topic
            if custom_title:
                topic = custom_title[:max_topic_length].strip()
            else:
                topic = session.topic[:max_topic_length].strip() if hasattr(session, 'topic') and session.topic else "untitled"
            
            # Sanitize topic if enabled
            if sanitize_topic:
                # More inclusive sanitization that preserves Unicode characters (Chinese, etc.)
                # Remove only problematic filename characters
                # Keep alphanumeric (including Unicode), spaces, hyphens, underscores
                topic = re.sub(r'[<>:"/\\|?*]', '', topic)  # Remove Windows forbidden chars
                topic = re.sub(r'[\x00-\x1f\x7f]', '', topic)  # Remove control characters
                topic = topic.replace(" ", "_").strip() if topic.strip() else "untitled"
            
            # Prepare replacement variables
            replacements = {
                "timestamp": timestamp,
                "topic": topic,
                "session_id": session.session_id if hasattr(session, 'session_id') else "unknown",
                "session_id_short": (session.session_id[:8] if hasattr(session, 'session_id') else "unknown"),
            }
            
            # Apply pattern with replacements
            logger.debug(f"Applying pattern '{pattern}' with replacements: {replacements}")
            filename = pattern.format(**replacements)
            
            # Ensure .md extension if not present
            if not filename.endswith('.md'):
                filename += '.md'
                
            logger.info(f"âœ… Generated filename: {filename} from pattern: {pattern}")
            logger.info(f"âœ… Custom title: {custom_title}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ FILENAME GENERATION FAILED: {e}")
            logger.error(f"Custom title was: {custom_title}")
            # Fallback to simple timestamp-based name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            logger.error(f"ğŸ”„ Using fallback filename: {timestamp}_analysis.md")
            return f"{timestamp}_analysis.md"

    def export_session_to_markdown(
        self, session_id: str, export_path: Optional[str] = None, custom_title: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Export complete session analysis to Markdown file with intelligent directory management

        This function provides full, untruncated access to all analysis content
        for offline reading and further LLM processing. Uses configurable export
        directory to avoid polluting git repositories.

        Args:
            session_id: The session to export
            export_path: Optional custom file path (defaults to auto-generated name in export directory)
            custom_title: Optional concise title from Host (overrides topic extraction)

        Returns:
            Dict with export status and file information
        """
        try:
            # Get session and validate
            session = self.session_manager.get_session(session_id)
            if not session:
                return {
                    "success": False,
                    "error": f"Session {session_id} not found",
                    "file_path": None,
                }

            # Get all detailed content
            detailed_step_contents = self._extract_detailed_step_contents(session_id)
            quality_metrics = self._calculate_comprehensive_quality_metrics(session)
            session_summary = self._generate_detailed_session_summary(session)
            thinking_trace = self.session_manager.get_full_trace(session_id)

            # Get export directory and validate
            if export_path:
                # Custom path provided - use as-is
                file_path = Path(export_path)
                export_dir = file_path.parent
                logger.info(f"Using custom export path: {file_path}")
            else:
                # Auto-generate filename in configured export directory
                export_dir = self.get_export_directory()
                
                # Validate and prepare directory
                validation = self._validate_export_directory(export_dir)
                if not validation["valid"]:
                    return {
                        "success": False,
                        "error": f"Export directory validation failed: {'; '.join(validation['warnings'])}",
                        "file_path": None,
                    }
                
                # Log warnings but continue
                for warning in validation["warnings"]:
                    logger.warning(warning)
                
                # Generate filename using configuration
                filename = self._generate_export_filename(session, custom_title)
                file_path = export_dir / filename
                
                logger.info(f"Auto-generated export path: {file_path}")

            # Build comprehensive Markdown content
            markdown_content = self._build_markdown_report(
                session=session,
                detailed_step_contents=detailed_step_contents,
                quality_metrics=quality_metrics,
                session_summary=session_summary,
                thinking_trace=thinking_trace,
            )

            # Ensure parent directory exists (in case of custom path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Write to file
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # Calculate file statistics
            file_size = file_path.stat().st_size
            content_lines = markdown_content.count("\n") + 1
            content_words = len(markdown_content.split())

            logger.info(f"Successfully exported session {session_id} to {file_path}")
            logger.info(
                f"Export stats: {file_size:,} bytes, {content_lines:,} lines, {content_words:,} words"
            )

            return {
                "success": True,
                "file_path": str(file_path.absolute()),
                "file_size_bytes": file_size,
                "content_lines": content_lines,
                "content_words": content_words,
                "session_id": session_id,
                "export_timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"Error exporting session {session_id} to Markdown: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_path": export_path,
                "session_id": session_id,
            }

    def _build_markdown_report(
        self,
        session: SessionState,
        detailed_step_contents: Dict[str, Any],
        quality_metrics: Dict[str, Any],
        session_summary: Dict[str, Any],
        thinking_trace: Dict[str, Any],
    ) -> str:
        """
        Build comprehensive Markdown report with complete content
        """
        lines = []

        # Header with metadata
        lines.extend(
            [
                "# æ·±åº¦æ€è€ƒä¼šè¯å®Œæ•´æŠ¥å‘Š",
                "",
                f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"**ä¼šè¯ID**: `{session.session_id}`",
                "**å¯¼å‡ºç‰ˆæœ¬**: å®Œæ•´æ— æˆªæ–­ç‰ˆæœ¬",
                "",
                "---",
                "",
            ]
        )

        # Session metadata
        lines.extend(
            [
                "## ğŸ“Š ä¼šè¯å…ƒæ•°æ®",
                "",
                "| å±æ€§ | å€¼ |",
                "|------|-----|",
                f"| **ä¸»é¢˜** | {session.topic} |",
                f"| **æ€è€ƒæµç¨‹** | {session.flow_type} |",
                f"| **ä¼šè¯æ—¶é•¿** | {session_summary.get('session_duration', 0):.2f} åˆ†é’Ÿ |",
                f"| **æ‰§è¡Œæ­¥éª¤** | {session_summary.get('total_steps', 0)} ä¸ª |",
                f"| **å¹³å‡è´¨é‡å¾—åˆ†** | {quality_metrics.get('average_quality', 0):.2f}/10 |",
                f"| **è´¨é‡è¶‹åŠ¿** | {quality_metrics.get('quality_trend', 'unknown')} |",
                "",
            ]
        )

        # Quality metrics details
        if quality_metrics.get("step_quality_breakdown"):
            lines.extend(
                [
                    "### ğŸ“ˆ è¯¦ç»†è´¨é‡è¯„åˆ†",
                    "",
                    "| æ­¥éª¤ | è´¨é‡å¾—åˆ† |",
                    "|------|----------|",
                ]
            )
            for step_name, score in quality_metrics["step_quality_breakdown"].items():
                lines.append(f"| {step_name} | {score:.3f}/10 |")
            lines.append("")

        # Complete analysis content
        lines.extend(
            [
                "## ğŸ” å®Œæ•´åˆ†æè¿‡ç¨‹",
                "",
                "> ä»¥ä¸‹æ˜¯å®Œæ•´çš„ã€æœªç»æˆªæ–­çš„åˆ†æå†…å®¹ï¼Œå¯ä¾›è¿›ä¸€æ­¥çš„LLMåˆ†æå’Œæ´å¯Ÿæå–ã€‚",
                "",
            ]
        )

        # Add each step's complete content
        for step_name, contents in detailed_step_contents.items():
            if not contents:
                continue

            lines.extend([f"### ğŸ“‹ {step_name}", ""])

            # Add step metadata
            main_content = ""
            metadata = {}
            for content_data in contents:
                if content_data.get("result_type") == "output":
                    main_content = content_data.get("content", "")
                    metadata = content_data.get("metadata", {})
                    break

            if not main_content and contents:
                main_content = contents[0].get("content", "")
                metadata = contents[0].get("metadata", {})

            # Add content statistics
            if main_content:
                char_count = len(main_content)
                word_count = len(main_content.split())
                lines.extend(
                    [
                        f"**å†…å®¹ç»Ÿè®¡**: {char_count:,} å­—ç¬¦, {word_count:,} è¯",
                        f"**æ—¶é—´æˆ³**: {contents[0].get('timestamp', 'Unknown')}",
                        "",
                    ]
                )

                # Add metadata if available
                if metadata:
                    lines.extend(
                        [
                            "**å…ƒæ•°æ®**:",
                            "```json",
                            f"{json.dumps(metadata, ensure_ascii=False, indent=2)}",
                            "```",
                            "",
                        ]
                    )

                # Add the complete content
                lines.extend(["**å®Œæ•´å†…å®¹**:", "", main_content, "", "---", ""])

        # Session context
        if session.context:
            lines.extend(
                [
                    "## ğŸ·ï¸ ä¼šè¯ä¸Šä¸‹æ–‡",
                    "",
                    "```json",
                    f"{json.dumps(session.context, ensure_ascii=False, indent=2)}",
                    "```",
                    "",
                ]
            )

        # Footer
        lines.extend(
            [
                "---",
                "",
                "## ğŸ“‹ ä½¿ç”¨è¯´æ˜",
                "",
                "1. **å®Œæ•´å†…å®¹**: æœ¬æŠ¥å‘ŠåŒ…å«æ‰€æœ‰åŸå§‹åˆ†æå†…å®¹ï¼Œæ— ä»»ä½•æˆªæ–­",
                "2. **LLMåˆ†æ**: å¯ç›´æ¥å°†æ­¤æ–‡ä»¶æä¾›ç»™LLMè¿›è¡ŒäºŒæ¬¡åˆ†æå’Œæ´å¯Ÿæå–",
                "3. **å¼•ç”¨æ–¹å¼**: åœ¨æ–°çš„å¯¹è¯ä¸­å¼•ç”¨æ­¤æ–‡ä»¶è¿›è¡Œæ·±åº¦è®¨è®º",
                "4. **æ ¼å¼**: æ ‡å‡†Markdownæ ¼å¼ï¼Œæ”¯æŒæ‰€æœ‰Markdownæ¸²æŸ“å™¨",
                "",
                "*æ­¤æŠ¥å‘Šç”±æ·±åº¦æ€è€ƒå¼•æ“è‡ªåŠ¨ç”Ÿæˆï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚*",
            ]
        )

        return "\n".join(lines)

    def _refresh_session_quality_data(self, session: SessionState) -> None:
        """
        Refresh session quality data from database to ensure completeness
        This ensures we have the most up-to-date quality scores for reporting
        """
        try:
            # Get all steps and their results from database
            steps = self.session_manager.db.get_session_steps(session.session_id)

            # Update session quality scores with any missing data
            for step in steps:
                step_name = step.get("step_name")
                quality_score = step.get("quality_score")

                if step_name and quality_score is not None:
                    # Update session quality scores if not already present
                    if step_name not in session.quality_scores:
                        session.quality_scores[step_name] = quality_score
                        logger.info(
                            f"Refreshed quality score for {step_name}: {quality_score:.1f}"
                        )

            # Update the session cache
            self.session_manager._active_sessions[session.session_id] = session

        except Exception as e:
            logger.warning(f"Error refreshing session quality data: {e}")

    def _build_comprehensive_summary_params(
        self,
        session: SessionState,
        quality_metrics: Dict[str, Any],
        session_summary: Dict[str, Any],
        thinking_trace: Dict[str, Any],
        final_insights: Optional[str],
    ) -> Dict[str, Any]:
        """Build comprehensive parameters for the summary template with detailed content"""

        # Extract detailed step contents from database
        detailed_step_contents = self._extract_detailed_step_contents(
            session.session_id
        )

        # Build comprehensive step analysis summary
        step_analysis = self._build_step_analysis_summary(detailed_step_contents)

        # Format quality metrics for display
        quality_display = self._format_quality_metrics_for_display(quality_metrics)

        # Format step summary for display
        step_summary_display = self._format_step_summary_for_display(session_summary)

        # Format thinking trace for display
        trace_display = self._format_thinking_trace_for_display(thinking_trace)

        return {
            "topic": session.topic,
            "flow_type": session.flow_type,
            "session_duration": f"{session_summary['session_duration']} åˆ†é’Ÿ",
            "total_steps": session_summary["total_steps"],
            "step_summary": step_summary_display,
            "thinking_trace": trace_display,
            "quality_metrics": quality_display,
            "final_insights": final_insights or "æ— é¢å¤–æ´å¯Ÿ",
            "completion_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "overall_assessment": quality_metrics.get("overall_assessment", "unknown"),
            "average_quality": quality_metrics.get("average_quality", 0),
            "improvement_areas": "\n".join(
                quality_metrics.get("improvement_areas", [])
            ),
            "best_step": (
                quality_metrics.get("best_performing_step", ["æ— ", 0])[0]
                if quality_metrics.get("best_performing_step")
                else "æ— "
            ),
            "session_context": session.context,
            # Enhanced content fields for comprehensive summary
            "detailed_step_contents": detailed_step_contents,
            "step_analysis_summary": step_analysis,
            "comprehensive_analysis": step_analysis,  # Alias for template compatibility
        }

    def _format_quality_metrics_for_display(
        self, quality_metrics: Dict[str, Any]
    ) -> str:
        """Format quality metrics for template display"""
        if not quality_metrics:
            return "æ— è´¨é‡æ•°æ®"

        lines = [
            f"å¹³å‡è´¨é‡å¾—åˆ†: {quality_metrics.get('average_quality', 0)}/10",
            f"è´¨é‡è¶‹åŠ¿: {quality_metrics.get('quality_trend', 'unknown')}",
            f"é«˜è´¨é‡æ­¥éª¤æ•°: {quality_metrics.get('high_quality_steps', 0)}",
            f"æ€»ä½“è¯„ä¼°: {quality_metrics.get('overall_assessment', 'unknown')}",
            f"è´¨é‡ä¸€è‡´æ€§: {quality_metrics.get('quality_consistency', 0)}",
        ]

        if quality_metrics.get("best_performing_step"):
            best_step, best_score = quality_metrics["best_performing_step"]
            lines.append(f"æœ€ä½³æ­¥éª¤: {best_step} ({best_score}/10)")

        return "\n".join(lines)

    def _format_step_summary_for_display(self, session_summary: Dict[str, Any]) -> str:
        """Format step summary for template display"""
        if not session_summary.get("detailed_steps"):
            return "æ— æ­¥éª¤æ•°æ®"

        lines = []
        for step in session_summary["detailed_steps"]:
            step_name = step.get("step_name", "unknown")
            quality = step.get("quality_score", "N/A")
            lines.append(f"- {step_name}: å®Œæˆ (è´¨é‡: {quality})")

        return "\n".join(lines)

    def _format_thinking_trace_for_display(self, thinking_trace: Dict[str, Any]) -> str:
        """Format thinking trace for template display"""
        if not thinking_trace or thinking_trace.get("error"):
            return "æ€ç»´è½¨è¿¹ä¸å¯ç”¨"

        lines = [
            f"ä¼šè¯ID: {thinking_trace.get('session_id', 'unknown')}",
            f"æµç¨‹ç±»å‹: {thinking_trace.get('flow_type', 'unknown')}",
            f"æ€»æ—¶é•¿: {thinking_trace.get('total_duration', 0)} ç§’",
            f"æ­¥éª¤æ•°é‡: {len(thinking_trace.get('steps', []))}",
        ]

        return "\n".join(lines)

    def _generate_completion_instructions(
        self, quality_metrics: Dict[str, Any], session: SessionState
    ) -> str:
        """Generate detailed instructions for final report generation"""
        base_instruction = "è¯·ç”Ÿæˆè¯¦ç»†çš„ç»¼åˆæŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰å…³é”®å‘ç°å’Œæ´å¯Ÿ"

        # Add quality-specific guidance
        quality_guidance = []

        overall_assessment = quality_metrics.get("overall_assessment", "unknown")
        if overall_assessment == "excellent":
            quality_guidance.append("è´¨é‡ä¼˜ç§€ï¼Œé‡ç‚¹çªå‡ºæ ¸å¿ƒæ´å¯Ÿå’Œåˆ›æ–°è§‚ç‚¹")
        elif overall_assessment == "good":
            quality_guidance.append("è´¨é‡è‰¯å¥½ï¼Œç¡®ä¿æ¶µç›–æ‰€æœ‰é‡è¦å‘ç°")
        elif overall_assessment == "acceptable":
            quality_guidance.append("è´¨é‡å¯æ¥å—ï¼Œæ³¨æ„è¡¥å……åˆ†ææ·±åº¦")
        else:
            quality_guidance.append("è´¨é‡éœ€è¦æ”¹è¿›ï¼Œé‡ç‚¹åŠ å¼ºè®ºè¯å’Œè¯æ®æ”¯æ’‘")

        # Add step-specific guidance
        if session.step_number > 5:
            quality_guidance.append("æµç¨‹è¾ƒä¸ºå®Œæ•´ï¼Œç¡®ä¿å„æ­¥éª¤ç»“æœçš„æœ‰æœºæ•´åˆ")

        # Add improvement area guidance
        improvement_areas = quality_metrics.get("improvement_areas", [])
        if improvement_areas and improvement_areas[0] != "æ‰€æœ‰æ­¥éª¤è´¨é‡è‰¯å¥½":
            quality_guidance.append(
                f"ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹æ”¹è¿›é¢†åŸŸ: {', '.join(improvement_areas[:2])}"
            )

        if quality_guidance:
            return f"{base_instruction}ã€‚{' '.join(quality_guidance)}"

        return base_instruction
