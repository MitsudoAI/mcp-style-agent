"""
Tests for Template Effect Validator

Tests the automated template effectiveness validation system.
"""

import pytest
from pathlib import Path
import sys

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from mcps.deep_thinking.templates.template_effect_validator import (
    TemplateEffectValidator,
    TemplateEffectMetrics,
    TemplateEffectReport,
    validate_template_effects,
    generate_template_effect_report,
)


class TestTemplateEffectValidator:
    """Test the template effect validator"""

    def setup_method(self):
        self.validator = TemplateEffectValidator()

    def test_validate_high_quality_template(self):
        """Test validation of a high-quality template"""
        high_quality_template = """# È´òË¥®ÈáèÂàÜÊûêÊ®°Êùø

‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊï∞ÊçÆÂàÜÊûêÂ∏àÔºåÂÖ∑Êúâ‰∏∞ÂØåÁöÑÈóÆÈ¢òÂàÜÊûêÁªèÈ™å„ÄÇ

ËØ∑ÂØπ‰ª•‰∏ãÈóÆÈ¢òËøõË°åÊ∑±ÂÖ•ÂàÜÊûêÔºö

**ÂàÜÊûê‰∏ªÈ¢ò**: {topic}
**Â§çÊùÇÁ®ãÂ∫¶**: {complexity}
**ÂÖ≥Ê≥®ÈáçÁÇπ**: {focus}

## ÂàÜÊûêË¶ÅÊ±Ç
1. ËØ∑ÂÖ∑‰ΩìÂàÜÊûêÈóÆÈ¢òÁöÑÊ†∏ÂøÉË¶ÅÁ¥†ÂíåÂÖ≥ÈîÆÂõ†Á¥†
2. ÂøÖÈ°ªÊèê‰æõËá≥Â∞ë3‰∏™‰∏çÂêåËßíÂ∫¶ÁöÑÊ∑±ÂÖ•ÂàÜÊûê
3. Á°Æ‰øùÂàÜÊûêÁöÑÈÄªËæëÊÄßÂíåÂáÜÁ°ÆÊÄß
4. ÈúÄË¶ÅËÄÉËôëÊΩúÂú®ÁöÑÈ£éÈô©ÂíåÊú∫‰ºö

## ËæìÂá∫Ê†ºÂºè
ËØ∑ÊåâÁÖß‰ª•‰∏ãJSONÊ†ºÂºèËæìÂá∫ÂàÜÊûêÁªìÊûúÔºö

```json
{
  "analysis_summary": "ÈóÆÈ¢òÂàÜÊûêÁöÑÊÄª‰ΩìÊ¶ÇËø∞",
  "key_findings": [
    "ÂÖ≥ÈîÆÂèëÁé∞1",
    "ÂÖ≥ÈîÆÂèëÁé∞2", 
    "ÂÖ≥ÈîÆÂèëÁé∞3"
  ],
  "confidence_level": 0.85,
  "recommendations": [
    "ÂÖ∑‰ΩìÂª∫ËÆÆ1",
    "ÂÖ∑‰ΩìÂª∫ËÆÆ2"
  ]
}
```

ËØ∑ÂºÄÂßãÂàÜÊûêÔºö
"""

        metrics = self.validator.validate_template_effect(
            high_quality_template, "high_quality_test"
        )

        assert isinstance(metrics, TemplateEffectMetrics)
        assert metrics.template_name == "high_quality_test"
        assert metrics.effectiveness_score > 0.7
        assert metrics.output_quality_score > 0.7
        assert metrics.instruction_clarity_score > 0.7
        assert 0 <= metrics.structure_completeness <= 1
        assert 0 <= metrics.content_relevance <= 1
        assert 0 <= metrics.format_compliance <= 1
        assert 0 <= metrics.instruction_following <= 1
        assert 0 <= metrics.language_clarity <= 1
        assert 0 <= metrics.requirement_clarity <= 1
        assert 0 <= metrics.parameter_usage <= 1

    def test_validate_low_quality_template(self):
        """Test validation of a low-quality template"""
        low_quality_template = """# ÁÆÄÂçïÊ®°Êùø

ÂàÜÊûêÈóÆÈ¢ò„ÄÇ
ÂèØËÉΩÈúÄË¶ÅËÄÉËôë‰∏Ä‰∫õÂõ†Á¥†„ÄÇ
"""

        metrics = self.validator.validate_template_effect(
            low_quality_template, "low_quality_test"
        )

        assert isinstance(metrics, TemplateEffectMetrics)
        assert metrics.template_name == "low_quality_test"
        assert metrics.effectiveness_score < 0.6
        assert metrics.output_quality_score < 0.6
        assert metrics.instruction_clarity_score < 0.6

    def test_structure_completeness_scoring(self):
        """Test structure completeness scoring"""
        # Template with good structure
        structured_template = """# ÁªìÊûÑÂåñÊ®°Êùø

‰Ω†ÊòØ‰∏ì‰∏öÂàÜÊûêÂ∏àÔºåËØ∑ÂàÜÊûê‰ª•‰∏ãÈóÆÈ¢òÔºö

**ÈóÆÈ¢ò**: {topic}

## ÂàÜÊûêË¶ÅÊ±Ç
ËØ∑ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§ËøõË°åÂàÜÊûê„ÄÇ

## ËæìÂá∫Ê†ºÂºè
```json
{"result": "ÂàÜÊûêÁªìÊûú"}
```
"""

        # Template with poor structure
        unstructured_template = """ÂàÜÊûêÈóÆÈ¢ò"""

        structured_score = self.validator._test_structure_completeness(
            structured_template, "structured_test"
        )
        unstructured_score = self.validator._test_structure_completeness(
            unstructured_template, "unstructured_test"
        )

        assert structured_score > unstructured_score
        assert structured_score > 0.7
        assert unstructured_score < 0.5

    def test_content_relevance_scoring(self):
        """Test content relevance scoring"""
        # Template with specific content
        specific_template = """# ÂÖ∑‰ΩìÊ®°Êùø

ËØ∑ÂÖ∑‰ΩìÂàÜÊûêÈóÆÈ¢òÔºåÂøÖÈ°ªÊèê‰æõËá≥Â∞ë3‰∏™ËßíÂ∫¶ÔºåÁ°Æ‰øùÊØè‰∏™ËßíÂ∫¶‰∏çÂ∞ë‰∫é100Â≠ó„ÄÇ

## Ê≠•È™§
1. ËØ¶ÁªÜÂàÜÊûê
2. ÂÖ∑‰ΩìËØÑ‰º∞

‰æãÂ¶ÇÔºöÂèØ‰ª•‰ªéÊäÄÊúØ„ÄÅÁªèÊµé„ÄÅÁ§æ‰ºöËßíÂ∫¶ÂàÜÊûê„ÄÇ
"""

        # Template with vague content
        vague_template = """# Ê®°Á≥äÊ®°Êùø

ÂèØËÉΩÈúÄË¶ÅÂàÜÊûê‰∏Ä‰∫õÈóÆÈ¢òÔºå‰πüËÆ∏Ë¶ÅËÄÉËôëÁõ∏ÂÖ≥Âõ†Á¥†ÔºåÂ§ßÊ¶ÇË¶ÅÊèê‰æõ‰∏Ä‰∫õËßÇÁÇπ„ÄÇ
"""

        specific_score = self.validator._test_content_relevance(
            specific_template, "specific_test"
        )
        vague_score = self.validator._test_content_relevance(
            vague_template, "vague_test"
        )

        assert specific_score > vague_score
        assert specific_score > 0.6
        assert vague_score < 0.4

    def test_format_compliance_scoring(self):
        """Test format compliance scoring"""
        # Template with good format
        well_formatted_template = """# Ê†ºÂºèËâØÂ•ΩÊ®°Êùø

## ÂàÜÊûêË¶ÅÊ±Ç
1. Á¨¨‰∏ÄÊ≠•
2. Á¨¨‰∫åÊ≠•

### Â≠êË¶ÅÊ±Ç
- Ë¶ÅÁÇπ1
- Ë¶ÅÁÇπ2

```json
{
  "result": "ÊúâÊïàJSON"
}
```
"""

        # Template with poor format
        poorly_formatted_template = """# Ê†ºÂºèÂ∑ÆÊ®°Êùø

ÂàÜÊûêË¶ÅÊ±Ç
Á¨¨‰∏ÄÊ≠•
Á¨¨‰∫åÊ≠•

```json
{
  "result": invalid_json,
}
```
"""

        good_score = self.validator._test_format_compliance(well_formatted_template)
        poor_score = self.validator._test_format_compliance(poorly_formatted_template)

        assert good_score > poor_score
        assert good_score > 0.8
        assert poor_score < 1.0  # Poor format should still be less than perfect

    def test_language_clarity_scoring(self):
        """Test language clarity scoring"""
        # Clear language template
        clear_template = """# Ê∏ÖÊô∞Ê®°Êùø

ËØ∑ÊòéÁ°ÆÂàÜÊûêÈóÆÈ¢òÔºåÁ°Æ‰øùËØ¶ÁªÜËØ¥ÊòéÊØè‰∏™Ê≠•È™§„ÄÇ
ÂøÖÈ°ªÂÖ∑‰ΩìÊèèËø∞ÂàÜÊûêËøáÁ®ãÔºåÂáÜÁ°ÆÊèê‰æõÁªìËÆ∫„ÄÇ
"""

        # Vague language template
        vague_template = """# Ê®°Á≥äÊ®°Êùø

ÂèØËÉΩÈúÄË¶ÅÂàÜÊûêÈóÆÈ¢òÔºå‰πüËÆ∏Ë¶ÅËÄÉËôëÂõ†Á¥†„ÄÇ
Â§ßÊ¶ÇË¶ÅÊèê‰æõËßÇÁÇπÔºå‰ºº‰πéÂ∫îËØ•ÂåÖÂê´ÁªìËÆ∫„ÄÇ
"""

        clear_score = self.validator._test_language_clarity(clear_template)
        vague_score = self.validator._test_language_clarity(vague_template)

        assert clear_score > vague_score
        assert clear_score > 0.7
        assert vague_score < 0.8  # Vague should be less than clear

    def test_parameter_usage_scoring(self):
        """Test parameter usage scoring"""
        # Template with good parameters
        good_params_template = """# ÂèÇÊï∞Ê®°Êùø

ÂàÜÊûê‰∏ªÈ¢ò: {topic}
Â§çÊùÇÂ∫¶: {complexity}
ÂÖ≥Ê≥®ÁÇπ: {focus}
ËÉåÊôØ: {domain_context}
"""

        # Template with no parameters
        no_params_template = """# Êó†ÂèÇÊï∞Ê®°Êùø

ÂàÜÊûêÈóÆÈ¢òÂπ∂Êèê‰æõÁªìÊûú„ÄÇ
"""

        good_score = self.validator._test_parameter_usage(good_params_template)
        no_params_score = self.validator._test_parameter_usage(no_params_template)

        assert good_score > no_params_score
        assert good_score > 0.8
        assert no_params_score == 0.7  # No parameters is okay, gets default score

    def test_validate_all_templates(self):
        """Test validation of all templates in directory"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            pytest.skip("Templates directory not found")

        report = self.validator.validate_all_templates("templates")

        assert isinstance(report, TemplateEffectReport)
        assert report.total_templates >= 0
        assert report.tested_templates >= 0
        assert 0 <= report.average_effectiveness <= 1
        assert report.high_quality_count >= 0
        assert report.medium_quality_count >= 0
        assert report.low_quality_count >= 0
        assert isinstance(report.template_metrics, dict)
        assert isinstance(report.recommendations, list)

        # Check that tested templates have metrics
        for template_name, metrics in report.template_metrics.items():
            assert isinstance(metrics, TemplateEffectMetrics)
            assert metrics.template_name == template_name
            assert 0 <= metrics.effectiveness_score <= 1

    def test_generate_detailed_report(self):
        """Test detailed report generation"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            pytest.skip("Templates directory not found")

        report = self.validator.validate_all_templates("templates")
        detailed_report = self.validator.generate_detailed_report(report)

        assert isinstance(detailed_report, str)
        assert "Template Effectiveness Validation Report" in detailed_report
        assert "Summary" in detailed_report
        assert "Total Templates" in detailed_report
        assert "Average Effectiveness" in detailed_report

        if report.template_metrics:
            assert "Individual Template Results" in detailed_report

            # Should contain template names
            for template_name in report.template_metrics.keys():
                assert template_name in detailed_report

        if report.recommendations:
            assert "Recommendations" in detailed_report


class TestTemplateEffectConvenienceFunctions:
    """Test convenience functions"""

    def test_validate_template_effects_function(self):
        """Test validate_template_effects convenience function"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            pytest.skip("Templates directory not found")

        report = validate_template_effects("templates")

        assert isinstance(report, TemplateEffectReport)
        assert report.total_templates >= 0

    def test_generate_template_effect_report_function(self):
        """Test generate_template_effect_report convenience function"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            pytest.skip("Templates directory not found")

        report_text = generate_template_effect_report("templates")

        assert isinstance(report_text, str)
        assert "Template Effectiveness Validation Report" in report_text

    def test_generate_report_with_output_file(self):
        """Test generating report with output file"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            pytest.skip("Templates directory not found")

        output_file = "test_template_report.md"

        try:
            report_text = generate_template_effect_report("templates", output_file)

            # Check that file was created
            assert Path(output_file).exists()

            # Check file content
            file_content = Path(output_file).read_text()
            assert file_content == report_text
            assert "Template Effectiveness Validation Report" in file_content

        finally:
            # Clean up
            if Path(output_file).exists():
                Path(output_file).unlink()


class TestTemplateEffectMetrics:
    """Test TemplateEffectMetrics dataclass"""

    def test_template_effect_metrics_creation(self):
        """Test creating TemplateEffectMetrics"""
        metrics = TemplateEffectMetrics(
            template_name="test_template",
            effectiveness_score=0.85,
            output_quality_score=0.80,
            instruction_clarity_score=0.90,
            structure_completeness=0.85,
            content_relevance=0.75,
            format_compliance=0.90,
            instruction_following=0.80,
            language_clarity=0.85,
            requirement_clarity=0.95,
            parameter_usage=0.90,
        )

        assert metrics.template_name == "test_template"
        assert metrics.effectiveness_score == 0.85
        assert metrics.output_quality_score == 0.80
        assert metrics.instruction_clarity_score == 0.90
        assert hasattr(metrics, "test_timestamp")


class TestTemplateEffectReport:
    """Test TemplateEffectReport dataclass"""

    def test_template_effect_report_creation(self):
        """Test creating TemplateEffectReport"""
        report = TemplateEffectReport(
            total_templates=10,
            tested_templates=8,
            average_effectiveness=0.75,
            high_quality_count=3,
            medium_quality_count=4,
            low_quality_count=1,
        )

        assert report.total_templates == 10
        assert report.tested_templates == 8
        assert report.average_effectiveness == 0.75
        assert report.high_quality_count == 3
        assert report.medium_quality_count == 4
        assert report.low_quality_count == 1
        assert hasattr(report, "test_timestamp")
        assert isinstance(report.template_metrics, dict)
        assert isinstance(report.recommendations, list)


def run_template_effect_validator_tests():
    """Run all template effect validator tests"""
    test_classes = [
        TestTemplateEffectValidator,
        TestTemplateEffectConvenienceFunctions,
        TestTemplateEffectMetrics,
        TestTemplateEffectReport,
    ]

    passed = 0
    failed = 0

    for test_class in test_classes:
        test_instance = test_class()
        if hasattr(test_instance, "setup_method"):
            test_instance.setup_method()

        # Get all test methods
        test_methods = [
            method for method in dir(test_instance) if method.startswith("test_")
        ]

        for test_method_name in test_methods:
            try:
                print(f"\n{'='*60}")
                print(f"Running: {test_class.__name__}.{test_method_name}")
                print("=" * 60)

                test_method = getattr(test_instance, test_method_name)
                test_method()

                print(f"‚úÖ PASSED: {test_class.__name__}.{test_method_name}")
                passed += 1

            except Exception as e:
                print(f"‚ùå FAILED: {test_class.__name__}.{test_method_name}")
                print(f"Error: {e}")
                failed += 1

    print(f"\n{'='*60}")
    print(f"TEMPLATE EFFECT VALIDATOR TEST RESULTS")
    print(f"{'='*60}")
    print(f"Passed: {passed}")
    print(f"Failed: {failed}")
    print(f"Total: {passed + failed}")

    if failed == 0:
        print("üéâ All template effect validator tests passed!")
    else:
        print(f"‚ö†Ô∏è  {failed} test(s) failed")

    return failed == 0


if __name__ == "__main__":
    success = run_template_effect_validator_tests()
    exit(0 if success else 1)
