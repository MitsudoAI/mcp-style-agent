"""
Integration tests for template quality validation with template manager

Tests the integration between template validation and the existing template management system.
"""

import sys
from pathlib import Path

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from mcps.deep_thinking.templates.template_validator import (
    TemplateValidator,
    ValidationSeverity,
)
from mcps.deep_thinking.templates.template_manager import TemplateManager


class TestTemplateQualityIntegration:
    """Test integration between template validation and template manager"""

    def setup_method(self):
        self.validator = TemplateValidator()
        self.template_manager = TemplateManager("templates")

    def test_validate_template_manager_templates(self):
        """Test validation of templates from template manager"""
        # Get all templates from manager
        template_names = self.template_manager.list_templates()

        assert len(template_names) > 0, "Template manager should have templates"

        validation_results = {}

        for template_name in template_names:
            try:
                # Get template content from manager
                template_content = self.template_manager.get_template(
                    template_name, params={}, use_default_if_missing=True
                )

                # Validate the template
                result = self.validator.validate_template(
                    template_content, template_name
                )
                validation_results[template_name] = result

                # Basic assertions
                assert result.template_name == template_name
                assert isinstance(result.overall_score, float)
                assert 0 <= result.overall_score <= 1
                assert isinstance(result.is_valid, bool)

                print(
                    f"âœ“ {template_name}: Score {result.overall_score:.2f}, "
                    f"Valid: {result.is_valid}, Issues: {len(result.issues)}"
                )

            except Exception as e:
                print(f"âœ— Error validating {template_name}: {e}")
                assert False, f"Template validation failed for {template_name}: {e}"

        # Ensure we validated some templates
        assert len(validation_results) > 0

        # Check that most templates have reasonable quality
        good_templates = sum(
            1 for r in validation_results.values() if r.overall_score >= 0.7
        )
        total_templates = len(validation_results)

        print(
            f"\nQuality Summary: {good_templates}/{total_templates} templates have score >= 0.7"
        )

        # At least 70% of templates should have good quality
        assert (
            good_templates / total_templates >= 0.7
        ), f"Too many low-quality templates: {good_templates}/{total_templates}"

    def test_template_parameter_validation_integration(self):
        """Test that template validation works with parameter replacement"""
        # Test with decomposition template which has many parameters
        template_name = "decomposition"

        # Get template with parameters
        test_params = {
            "topic": "å¦‚ä½•æé«˜å­¦ä¹ æ•ˆç‡",
            "complexity": "moderate",
            "focus": "å­¦ç”Ÿç¾¤ä½“",
            "domain_context": "æ•™è‚²å¿ƒç†å­¦",
        }

        try:
            # Get template content with parameters replaced
            template_with_params = self.template_manager.get_template(
                template_name, params=test_params
            )

            # Validate the template content (original, not with params replaced)
            original_template = self.template_manager.cache.get(template_name, "")
            result = self.validator.validate_template(original_template, template_name)

            # Should be valid and have good score
            assert result.is_valid, f"Template {template_name} should be valid"
            assert (
                result.overall_score >= 0.8
            ), f"Template {template_name} should have high quality"

            # Should detect parameters
            assert (
                result.metrics["parameter_count"] > 0
            ), "Should detect parameters in template"

            print(f"âœ“ Parameter integration test passed for {template_name}")
            print(f"  Original template score: {result.overall_score:.2f}")
            print(f"  Parameters detected: {result.metrics['parameter_count']}")
            print(f"  Template with params length: {len(template_with_params)} chars")

        except Exception as e:
            assert False, f"Parameter integration test failed: {e}"

    def test_template_validation_with_file_system(self):
        """Test validation of templates from file system"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            print("Templates directory not found, skipping file system test")
            return

        # Validate all templates in directory
        results = self.validator.validate_all_templates(templates_dir)

        assert len(results) > 0, "Should find template files"

        # Compare with template manager templates
        manager_templates = set(self.template_manager.list_templates())
        file_templates = set(results.keys())

        # Should have significant overlap
        common_templates = manager_templates.intersection(file_templates)
        assert (
            len(common_templates) > 0
        ), "Should have common templates between manager and files"

        print(f"âœ“ File system validation found {len(results)} templates")
        print(f"  Common with manager: {len(common_templates)}")
        print(f"  Manager only: {manager_templates - file_templates}")
        print(f"  Files only: {file_templates - manager_templates}")

    def test_validation_report_generation(self):
        """Test generation of validation reports"""
        templates_dir = Path("templates")

        if not templates_dir.exists():
            print("Templates directory not found, skipping report test")
            return

        # Generate validation results
        results = self.validator.validate_all_templates(templates_dir)

        # Generate report
        report = self.validator.generate_validation_report(results)

        # Basic report checks
        assert "Template Quality Validation Report" in report
        assert "Summary" in report
        assert "Individual Template Results" in report

        # Should contain template names
        for template_name in results.keys():
            assert template_name in report

        # Should contain scores and metrics
        assert "Average Quality Score" in report
        assert "Total Templates" in report

        print(f"âœ“ Generated validation report ({len(report)} characters)")
        print("Report preview:")
        print(report[:500] + "..." if len(report) > 500 else report)

    def test_template_improvement_suggestions(self):
        """Test that validation provides useful improvement suggestions"""
        # Create a deliberately poor template
        poor_template = """åˆ†æé—®é¢˜ã€‚

å¯èƒ½éœ€è¦è€ƒè™‘ä¸€äº›å› ç´ ã€‚
ä¹Ÿè®¸è¦æä¾›ç»“æœã€‚
"""

        result = self.validator.validate_template(poor_template, "poor_test")

        # Should have low score and many issues
        assert result.overall_score < 0.5, "Poor template should have low score"
        assert len(result.issues) > 3, "Poor template should have many issues"
        assert len(result.suggestions) > 0, "Should provide improvement suggestions"

        # Should have critical or error issues
        serious_issues = [
            i
            for i in result.issues
            if i.severity in [ValidationSeverity.CRITICAL, ValidationSeverity.ERROR]
        ]
        assert len(serious_issues) > 0, "Poor template should have serious issues"

        print(f"âœ“ Poor template validation:")
        print(f"  Score: {result.overall_score:.2f}")
        print(f"  Issues: {len(result.issues)}")
        print(f"  Suggestions: {result.suggestions[:2]}")  # Show first 2 suggestions

    def test_template_quality_metrics(self):
        """Test that quality metrics are meaningful"""
        # Test with a good template
        good_template = """# é«˜è´¨é‡åˆ†ææ¨¡æ¿

ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„é—®é¢˜åˆ†æç»éªŒã€‚

è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±å…¥åˆ†æï¼š

**åˆ†æä¸»é¢˜**: {topic}
**å¤æ‚ç¨‹åº¦**: {complexity}
**å…³æ³¨é‡ç‚¹**: {focus}
**é¢†åŸŸèƒŒæ™¯**: {domain_context}

## åˆ†æè¦æ±‚
1. è¯·å…·ä½“åˆ†æé—®é¢˜çš„æ ¸å¿ƒè¦ç´ å’Œå…³é”®å› ç´ 
2. å¿…é¡»æä¾›è‡³å°‘3ä¸ªä¸åŒè§’åº¦çš„æ·±å…¥åˆ†æ
3. ç¡®ä¿åˆ†æçš„é€»è¾‘æ€§å’Œå‡†ç¡®æ€§
4. éœ€è¦è€ƒè™‘æ½œåœ¨çš„é£é™©å’Œæœºä¼š

## è¾“å‡ºæ ¼å¼
è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

```json
{
  "analysis_summary": "é—®é¢˜åˆ†æçš„æ€»ä½“æ¦‚è¿°",
  "key_findings": [
    "å…³é”®å‘ç°1",
    "å…³é”®å‘ç°2", 
    "å…³é”®å‘ç°3"
  ],
  "detailed_analysis": {
    "core_elements": "æ ¸å¿ƒè¦ç´ åˆ†æ",
    "risk_assessment": "é£é™©è¯„ä¼°",
    "opportunities": "æœºä¼šè¯†åˆ«"
  },
  "confidence_level": 0.85,
  "recommendations": [
    "å…·ä½“å»ºè®®1",
    "å…·ä½“å»ºè®®2"
  ]
}
```

è¯·å¼€å§‹åˆ†æï¼š
"""

        result = self.validator.validate_template(good_template, "good_test")

        # Should have high score and few issues
        assert result.overall_score > 0.8, "Good template should have high score"
        assert result.is_valid, "Good template should be valid"

        # Check metrics
        metrics = result.metrics
        assert metrics["parameter_count"] > 0, "Should detect parameters"
        assert metrics["word_count"] > 50, "Should count words"
        assert metrics["line_count"] > 10, "Should count lines"
        assert metrics["format_score"] > 0.8, "Should have good format score"
        assert metrics["content_score"] > 0.8, "Should have good content score"
        assert (
            metrics["effectiveness_score"] > 0.7
        ), "Should have good effectiveness score"

        print(f"âœ“ Good template metrics:")
        print(f"  Overall score: {result.overall_score:.2f}")
        print(f"  Format: {metrics['format_score']:.2f}")
        print(f"  Content: {metrics['content_score']:.2f}")
        print(f"  Effectiveness: {metrics['effectiveness_score']:.2f}")
        print(f"  Parameters: {metrics['parameter_count']}")
        print(f"  Size: {metrics['word_count']} words, {metrics['line_count']} lines")


def run_integration_tests():
    """Run all integration tests"""
    test_instance = TestTemplateQualityIntegration()
    test_instance.setup_method()

    tests = [
        test_instance.test_validate_template_manager_templates,
        test_instance.test_template_parameter_validation_integration,
        test_instance.test_template_validation_with_file_system,
        test_instance.test_validation_report_generation,
        test_instance.test_template_improvement_suggestions,
        test_instance.test_template_quality_metrics,
    ]

    passed = 0
    failed = 0

    for test in tests:
        try:
            print(f"\n{'='*60}")
            print(f"Running: {test.__name__}")
            print("=" * 60)
            test()
            print(f"âœ… PASSED: {test.__name__}")
            passed += 1
        except Exception as e:
            print(f"âŒ FAILED: {test.__name__}")
            print(f"Error: {e}")
            failed += 1

    print(f"\n{'='*60}")
    print(f"INTEGRATION TEST RESULTS")
    print(f"{'='*60}")
    print(f"Passed: {passed}")
    print(f"Failed: {failed}")
    print(f"Total: {passed + failed}")

    if failed == 0:
        print("ğŸ‰ All integration tests passed!")
    else:
        print(f"âš ï¸  {failed} test(s) failed")

    return failed == 0


if __name__ == "__main__":
    success = run_integration_tests()
    exit(0 if success else 1)
