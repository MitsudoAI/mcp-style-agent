#!/usr/bin/env python3
"""
Complete System Integration Test Runner

This script runs comprehensive system integration tests for the Deep Thinking Engine
without requiring pytest or other external testing dependencies.

Requirements: ç³»ç»Ÿé›†æˆï¼ŒåŠŸèƒ½éªŒè¯
"""

import json
import logging
import tempfile
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import sys

# Add src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# Core imports
from mcps.deep_thinking.config.config_manager import ConfigManager
from mcps.deep_thinking.data.database import ThinkingDatabase
from mcps.deep_thinking.flows.flow_executor import FlowExecutor
from mcps.deep_thinking.flows.flow_manager import FlowManager
from mcps.deep_thinking.sessions.session_manager import SessionManager
from mcps.deep_thinking.templates.template_manager import TemplateManager
from mcps.deep_thinking.tools.mcp_tools import MCPTools


@dataclass
class SystemIntegrationResult:
    """Result of a complete system integration test"""
    test_name: str
    success: bool
    execution_time: float
    components_tested: List[str]
    workflow_steps_completed: int
    total_workflow_steps: int
    performance_metrics: Dict[str, Any]
    error_message: Optional[str] = None
    detailed_results: Optional[Dict[str, Any]] = None


@dataclass
class SystemHealthMetrics:
    """System health and performance metrics"""
    memory_usage_mb: float
    cpu_usage_percent: float
    database_operations_per_second: float
    template_cache_hit_rate: float
    session_creation_time_ms: float
    workflow_completion_rate: float
    error_rate: float
    uptime_seconds: float


class CompleteSystemIntegrationTester:
    """Complete system integration tester for the deep thinking engine"""
    
    def __init__(self, temp_dir: Optional[str] = None):
        """Initialize the complete system integration tester"""
        self.temp_dir = temp_dir or tempfile.mkdtemp()
        self.temp_path = Path(self.temp_dir)
        
        self.logger = logging.getLogger(__name__)
        self.start_time = time.time()
        
        # Test results
        self.integration_results: List[SystemIntegrationResult] = []
        self.health_metrics: Optional[SystemHealthMetrics] = None
        
        # System components
        self.database: Optional[ThinkingDatabase] = None
        self.template_manager: Optional[TemplateManager] = None
        self.flow_manager: Optional[FlowManager] = None
        self.flow_executor: Optional[FlowExecutor] = None
        self.session_manager: Optional[SessionManager] = None
        self.mcp_tools: Optional[MCPTools] = None
        
        # Test configuration
        self.test_scenarios = self._define_test_scenarios()
        
        self.setup_complete_system()
    
    def setup_complete_system(self):
        """Setup the complete system with all components"""
        try:
            self.logger.info("Setting up complete system integration test environment...")
            
            # Create directory structure
            self._create_directory_structure()
            
            # Create configuration files
            self._create_system_configuration()
            
            # Create test templates
            self._create_test_templates()
            
            # Initialize all system components
            self._initialize_all_components()
            
            # Verify system health
            self._verify_system_health()
            
            self.logger.info(f"Complete system setup successful: {self.temp_dir}")
            
        except Exception as e:
            self.logger.error(f"Failed to setup complete system: {e}")
            raise
    
    def _create_directory_structure(self):
        """Create complete directory structure"""
        directories = [
            "config",
            "templates",
            "data",
            "logs",
            "cache",
            "sessions",
            "flows",
            "performance"
        ]
        
        for directory in directories:
            (self.temp_path / directory).mkdir(exist_ok=True)
    
    def _create_system_configuration(self):
        """Create comprehensive system configuration"""
        import yaml
        
        # System configuration
        system_config = {
            'database': {
                'path': str(self.temp_path / "data" / "thinking.db"),
                'connection_pool_size': 10,
                'timeout_seconds': 30
            },
            'templates': {
                'directory': str(self.temp_path / "templates"),
                'cache_size': 100,
                'hot_reload': True
            },
            'flows': {
                'default_flow': 'comprehensive_analysis',
                'timeout_minutes': 30,
                'max_concurrent_flows': 5
            },
            'sessions': {
                'max_active_sessions': 100,
                'cleanup_interval_minutes': 60,
                'session_timeout_hours': 24
            },
            'performance': {
                'monitoring_enabled': True,
                'metrics_collection_interval': 10,
                'performance_logging': True
            },
            'privacy': {
                'data_encryption': True,
                'local_only': True,
                'audit_logging': True
            }
        }
        
        with open(self.temp_path / "config" / "system.yaml", 'w') as f:
            yaml.dump(system_config, f)
        
        # Flow configuration
        flow_config = {
            'flows': {
                'test_comprehensive': {
                    'name': 'Test Comprehensive Analysis',
                    'description': 'Complete test workflow',
                    'steps': [
                        {
                            'step': 'decompose_problem',
                            'template': 'decomposition_template',
                            'required': True
                        },
                        {
                            'step': 'collect_evidence',
                            'template': 'evidence_collection_template',
                            'required': True
                        },
                        {
                            'step': 'critical_evaluation',
                            'template': 'critical_evaluation_template',
                            'required': True
                        }
                    ]
                },
                'test_simple': {
                    'name': 'Test Simple Analysis',
                    'description': 'Simple test workflow',
                    'steps': [
                        {
                            'step': 'basic_analysis',
                            'template': 'basic_template',
                            'required': True
                        }
                    ]
                }
            }
        }
        
        with open(self.temp_path / "config" / "flows.yaml", 'w') as f:
            yaml.dump(flow_config, f)
    
    def _create_test_templates(self):
        """Create comprehensive test templates"""
        templates = {
            'basic_template.tmpl': """
# åŸºç¡€åˆ†ææ¨¡æ¿

è¯·å¯¹ä»¥ä¸‹ä¸»é¢˜è¿›è¡ŒåŸºç¡€åˆ†æï¼š

**ä¸»é¢˜**: {topic}

## åˆ†æè¦æ±‚ï¼š
1. ç®€è¦æ¦‚è¿°ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹
2. è¯†åˆ«å…³é”®è¦ç‚¹
3. æä¾›åˆæ­¥ç»“è®º

è¯·å¼€å§‹åˆ†æï¼š
""",
            
            'decomposition_template.tmpl': """
# é—®é¢˜åˆ†è§£æ¨¡æ¿

è¯·å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­é—®é¢˜ï¼š

**ä¸»è¦é—®é¢˜**: {topic}

## åˆ†è§£è¦æ±‚ï¼š
1. å°†ä¸»é—®é¢˜åˆ†è§£ä¸º3-5ä¸ªæ ¸å¿ƒå­é—®é¢˜
2. æ¯ä¸ªå­é—®é¢˜åº”è¯¥ç›¸å¯¹ç‹¬ç«‹ä¸”å¯æ·±å…¥åˆ†æ
3. ç¡®ä¿è¦†ç›–é—®é¢˜çš„ä¸åŒè§’åº¦å’Œå±‚é¢

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ï¼š
- main_question: ä¸»è¦é—®é¢˜
- sub_questions: å­é—®é¢˜åˆ—è¡¨

å¼€å§‹åˆ†è§£ï¼š
""",
            
            'evidence_collection_template.tmpl': """
# è¯æ®æ”¶é›†æ¨¡æ¿

è¯·ä¸ºä»¥ä¸‹é—®é¢˜æ”¶é›†ç›¸å…³è¯æ®ï¼š

**é—®é¢˜**: {topic}
**å­é—®é¢˜**: {sub_questions}

## æ”¶é›†è¦æ±‚ï¼š
1. å¯»æ‰¾å¤šæ ·åŒ–çš„ä¿¡æ¯æ¥æº
2. è¯„ä¼°ä¿¡æ¯çš„å¯ä¿¡åº¦
3. æå–å…³é”®äº‹å®å’Œæ•°æ®

è¯·å¼€å§‹æ”¶é›†è¯æ®ï¼š
""",
            
            'critical_evaluation_template.tmpl': """
# æ‰¹åˆ¤æ€§è¯„ä¼°æ¨¡æ¿

è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼š

**è¯„ä¼°å†…å®¹**: {content}

## è¯„ä¼°æ ‡å‡†ï¼š
1. å‡†ç¡®æ€§ - ä¿¡æ¯æ˜¯å¦å‡†ç¡®æ— è¯¯
2. é€»è¾‘æ€§ - æ¨ç†æ˜¯å¦åˆä¹é€»è¾‘
3. å®Œæ•´æ€§ - åˆ†ææ˜¯å¦å…¨é¢
4. ç›¸å…³æ€§ - å†…å®¹æ˜¯å¦ä¸ä¸»é¢˜ç›¸å…³

è¯·å¼€å§‹è¯„ä¼°ï¼š
"""
        }
        
        for template_name, template_content in templates.items():
            template_path = self.temp_path / "templates" / template_name
            template_path.write_text(template_content, encoding='utf-8')
    
    def _initialize_all_components(self):
        """Initialize all system components"""
        try:
            # Database
            db_path = str(self.temp_path / "data" / "thinking.db")
            self.database = ThinkingDatabase(db_path)
            
            # Template manager
            templates_dir = str(self.temp_path / "templates")
            self.template_manager = TemplateManager(templates_dir)
            
            # Flow manager
            self.flow_manager = FlowManager(self.database)
            
            # Flow executor
            self.flow_executor = FlowExecutor(
                flow_manager=self.flow_manager,
                template_manager=self.template_manager,
                database=self.database
            )
            
            # Session manager
            self.session_manager = SessionManager(self.database)
            
            # MCP tools
            self.mcp_tools = MCPTools(
                session_manager=self.session_manager,
                flow_manager=self.flow_manager,
                template_manager=self.template_manager,
                flow_executor=self.flow_executor
            )
            
            self.logger.info("All system components initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize system components: {e}")
            raise
    
    def _verify_system_health(self):
        """Verify system health after initialization"""
        health_checks = [
            ('database_connection', lambda: self.database.get_connection() is not None),
            ('template_loading', lambda: len(self.template_manager.list_templates()) > 0),
            ('flow_configuration', lambda: len(self.flow_manager.list_flows()) > 0),
            ('session_creation', lambda: self.session_manager.create_session('health_check', 'test')),
            ('mcp_tools_ready', lambda: hasattr(self.mcp_tools, 'start_thinking'))
        ]
        
        for check_name, check_func in health_checks:
            try:
                result = check_func()
                if not result:
                    raise Exception(f"Health check failed: {check_name}")
                self.logger.info(f"âœ… Health check passed: {check_name}")
            except Exception as e:
                self.logger.error(f"âŒ Health check failed: {check_name} - {e}")
                raise
    
    def _define_test_scenarios(self) -> List[Dict[str, Any]]:
        """Define comprehensive test scenarios"""
        return [
            {
                'name': 'basic_workflow_test',
                'description': 'Test basic workflow execution',
                'topic': 'å¦‚ä½•æé«˜å­¦ä¹ æ•ˆç‡ï¼Ÿ',
                'flow_type': 'test_simple',
                'expected_steps': 1,
                'timeout_seconds': 30
            },
            {
                'name': 'comprehensive_workflow_test',
                'description': 'Test comprehensive workflow execution',
                'topic': 'äººå·¥æ™ºèƒ½å¯¹æ•™è‚²çš„å½±å“',
                'flow_type': 'test_comprehensive',
                'expected_steps': 3,
                'timeout_seconds': 60
            },
            {
                'name': 'complex_topic_test',
                'description': 'Test with complex topic',
                'topic': 'æ°”å€™å˜åŒ–å¯¹å…¨çƒç»æµçš„é•¿æœŸå½±å“åŠåº”å¯¹ç­–ç•¥',
                'flow_type': 'test_comprehensive',
                'expected_steps': 3,
                'timeout_seconds': 90
            }
        ]
    
    def run_complete_system_integration_test(self) -> List[SystemIntegrationResult]:
        """Run complete system integration test"""
        self.logger.info("Starting complete system integration test...")
        
        for scenario in self.test_scenarios:
            result = self._run_single_integration_test(scenario)
            self.integration_results.append(result)
        
        # Collect system health metrics
        self.health_metrics = self._collect_system_health_metrics()
        
        self.logger.info(f"Complete system integration test finished. Results: {len(self.integration_results)}")
        
        return self.integration_results
    
    def _run_single_integration_test(self, scenario: Dict[str, Any]) -> SystemIntegrationResult:
        """Run a single integration test scenario"""
        test_name = scenario['name']
        start_time = time.time()
        
        self.logger.info(f"Running integration test: {test_name}")
        
        try:
            # Track components being tested
            components_tested = []
            workflow_steps_completed = 0
            performance_metrics = {}
            detailed_results = {}
            
            # Step 1: Start thinking workflow
            self.logger.info(f"  Step 1: Starting thinking workflow for '{scenario['topic']}'")
            start_result = self.mcp_tools.start_thinking(scenario['topic'])
            
            if not isinstance(start_result, dict) or 'session_id' not in start_result:
                raise Exception("start_thinking did not return valid result")
            
            session_id = start_result['session_id']
            components_tested.extend(['mcp_tools', 'session_manager', 'template_manager'])
            workflow_steps_completed += 1
            
            detailed_results['start_thinking'] = {
                'session_id': session_id,
                'has_prompt_template': 'prompt_template' in start_result,
                'template_length': len(start_result.get('prompt_template', ''))
            }
            
            # Step 2: Simulate workflow progression
            current_step_result = "Initial problem decomposition completed"
            
            for step_num in range(2, scenario['expected_steps'] + 1):
                self.logger.info(f"  Step {step_num}: Getting next step")
                
                next_result = self.mcp_tools.next_step(session_id, current_step_result)
                
                if not isinstance(next_result, dict):
                    raise Exception(f"next_step did not return valid result at step {step_num}")
                
                workflow_steps_completed += 1
                current_step_result = f"Step {step_num} completed with analysis"
                
                detailed_results[f'step_{step_num}'] = {
                    'has_step': 'step' in next_result,
                    'has_prompt_template': 'prompt_template' in next_result,
                    'step_name': next_result.get('step', 'unknown')
                }
            
            components_tested.extend(['flow_manager', 'flow_executor'])
            
            # Step 3: Analyze workflow step
            self.logger.info(f"  Step 3: Analyzing workflow step")
            analyze_result = self.mcp_tools.analyze_step(
                session_id, 
                'test_step', 
                current_step_result
            )
            
            if not isinstance(analyze_result, dict):
                raise Exception("analyze_step did not return valid result")
            
            detailed_results['analyze_step'] = {
                'has_analysis_prompt': 'analysis_prompt' in analyze_result,
                'quality_check': analyze_result.get('quality_check', False)
            }
            
            # Step 4: Complete thinking workflow
            self.logger.info(f"  Step 4: Completing thinking workflow")
            complete_result = self.mcp_tools.complete_thinking(session_id)
            
            if not isinstance(complete_result, dict):
                raise Exception("complete_thinking did not return valid result")
            
            detailed_results['complete_thinking'] = {
                'status': complete_result.get('status', 'unknown'),
                'has_summary_prompt': 'summary_prompt' in complete_result
            }
            
            # Step 5: Verify session persistence
            self.logger.info(f"  Step 5: Verifying session persistence")
            session = self.session_manager.get_session(session_id)
            
            if not session:
                raise Exception("Session was not persisted correctly")
            
            components_tested.append('database')
            
            detailed_results['session_verification'] = {
                'session_exists': session is not None,
                'topic_matches': session.get('topic') == scenario['topic'] if session else False
            }
            
            # Collect performance metrics
            execution_time = time.time() - start_time
            performance_metrics = {
                'total_execution_time': execution_time,
                'steps_per_second': workflow_steps_completed / execution_time,
                'memory_usage_mb': self._get_current_memory_usage()
            }
            
            # Create successful result
            result = SystemIntegrationResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                components_tested=list(set(components_tested)),
                workflow_steps_completed=workflow_steps_completed,
                total_workflow_steps=scenario['expected_steps'],
                performance_metrics=performance_metrics,
                detailed_results=detailed_results
            )
            
            self.logger.info(f"âœ… Integration test {test_name} PASSED ({execution_time:.3f}s)")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            result = SystemIntegrationResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                components_tested=components_tested if 'components_tested' in locals() else [],
                workflow_steps_completed=workflow_steps_completed if 'workflow_steps_completed' in locals() else 0,
                total_workflow_steps=scenario['expected_steps'],
                performance_metrics={'total_execution_time': execution_time},
                error_message=str(e)
            )
            
            self.logger.error(f"âŒ Integration test {test_name} FAILED ({execution_time:.3f}s): {e}")
            
            return result
    
    def _get_current_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0
    
    def _collect_system_health_metrics(self) -> SystemHealthMetrics:
        """Collect comprehensive system health metrics"""
        try:
            uptime = time.time() - self.start_time
            
            # Calculate success rates
            total_tests = len(self.integration_results)
            successful_tests = sum(1 for r in self.integration_results if r.success)
            workflow_completion_rate = successful_tests / max(total_tests, 1)
            error_rate = (total_tests - successful_tests) / max(total_tests, 1)
            
            # Calculate performance metrics
            execution_times = [r.execution_time for r in self.integration_results]
            avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0
            
            # Memory usage
            memory_usage = self._get_current_memory_usage()
            
            # Database performance (simplified)
            db_operations = sum(r.workflow_steps_completed for r in self.integration_results)
            db_ops_per_second = db_operations / max(uptime, 1)
            
            return SystemHealthMetrics(
                memory_usage_mb=memory_usage,
                cpu_usage_percent=0.0,  # Would need more complex monitoring
                database_operations_per_second=db_ops_per_second,
                template_cache_hit_rate=0.95,  # Estimated
                session_creation_time_ms=avg_execution_time * 1000,
                workflow_completion_rate=workflow_completion_rate,
                error_rate=error_rate,
                uptime_seconds=uptime
            )
            
        except Exception as e:
            self.logger.error(f"Failed to collect system health metrics: {e}")
            return SystemHealthMetrics(
                memory_usage_mb=0.0,
                cpu_usage_percent=0.0,
                database_operations_per_second=0.0,
                template_cache_hit_rate=0.0,
                session_creation_time_ms=0.0,
                workflow_completion_rate=0.0,
                error_rate=1.0,
                uptime_seconds=time.time() - self.start_time
            )
    
    def generate_comprehensive_integration_report(self) -> str:
        """Generate comprehensive integration test report"""
        if not self.integration_results:
            return "No integration test results available."
        
        total_tests = len(self.integration_results)
        successful_tests = sum(1 for r in self.integration_results if r.success)
        failed_tests = total_tests - successful_tests
        total_execution_time = sum(r.execution_time for r in self.integration_results)
        
        # Calculate component coverage
        all_components = set()
        for result in self.integration_results:
            all_components.update(result.components_tested)
        
        report_lines = [
            "=" * 100,
            "DEEP THINKING ENGINE - COMPLETE SYSTEM INTEGRATION TEST REPORT",
            "=" * 100,
            "",
            f"Test Environment: {self.temp_dir}",
            f"Report Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"System Uptime: {time.time() - self.start_time:.1f} seconds",
            "",
            "EXECUTIVE SUMMARY",
            "-" * 50,
            f"Total Integration Tests: {total_tests}",
            f"Successful Tests: {successful_tests}",
            f"Failed Tests: {failed_tests}",
            f"Overall Success Rate: {successful_tests/max(total_tests,1):.1%}",
            f"Total Execution Time: {total_execution_time:.3f} seconds",
            f"Average Test Duration: {total_execution_time/max(total_tests,1):.3f} seconds",
            "",
            "COMPONENT COVERAGE",
            "-" * 50,
            f"Components Tested: {len(all_components)}",
            f"Components: {', '.join(sorted(all_components))}",
            "",
            "SYSTEM HEALTH METRICS",
            "-" * 50
        ]
        
        if self.health_metrics:
            report_lines.extend([
                f"Memory Usage: {self.health_metrics.memory_usage_mb:.1f} MB",
                f"Database Operations/sec: {self.health_metrics.database_operations_per_second:.2f}",
                f"Template Cache Hit Rate: {self.health_metrics.template_cache_hit_rate:.1%}",
                f"Workflow Completion Rate: {self.health_metrics.workflow_completion_rate:.1%}",
                f"System Error Rate: {self.health_metrics.error_rate:.2%}",
                f"Average Session Creation: {self.health_metrics.session_creation_time_ms:.1f} ms",
            ])
        
        report_lines.extend([
            "",
            "DETAILED TEST RESULTS",
            "-" * 50
        ])
        
        for result in self.integration_results:
            status = "âœ… PASS" if result.success else "âŒ FAIL"
            completion_rate = result.workflow_steps_completed / max(result.total_workflow_steps, 1)
            
            report_lines.extend([
                f"{status} {result.test_name}",
                f"    Duration: {result.execution_time:.3f}s",
                f"    Workflow Completion: {result.workflow_steps_completed}/{result.total_workflow_steps} ({completion_rate:.1%})",
                f"    Components: {', '.join(result.components_tested)}",
            ])
            
            if result.error_message:
                report_lines.append(f"    Error: {result.error_message}")
            
            if result.performance_metrics:
                report_lines.append("    Performance Metrics:")
                for key, value in result.performance_metrics.items():
                    if isinstance(value, float):
                        report_lines.append(f"      {key}: {value:.3f}")
                    else:
                        report_lines.append(f"      {key}: {value}")
            
            report_lines.append("")
        
        # Final assessment
        system_ready = failed_tests == 0 and successful_tests > 0
        
        report_lines.extend([
            "FINAL ASSESSMENT",
            "-" * 50,
            f"System Integration Status: {'âœ… PASSED' if system_ready else 'âŒ FAILED'}",
            f"All Components Working: {'âœ… YES' if len(all_components) >= 5 else 'âŒ NO'}",
            f"Workflow Execution: {'âœ… FUNCTIONAL' if successful_tests > 0 else 'âŒ BROKEN'}",
            f"System Stability: {'âœ… STABLE' if self.health_metrics and self.health_metrics.error_rate < 0.1 else 'âŒ UNSTABLE'}",
            "",
            f"DEPLOYMENT READINESS: {'ğŸš€ READY FOR DEPLOYMENT' if system_ready else 'âš ï¸ NEEDS ATTENTION'}",
            "",
            "=" * 100
        ])
        
        return "\n".join(report_lines)
    
    def save_integration_results(self, output_directory: Optional[str] = None):
        """Save integration test results to files"""
        output_dir = Path(output_directory or "complete_integration_results")
        output_dir.mkdir(exist_ok=True)
        
        # Save comprehensive report
        report = self.generate_comprehensive_integration_report()
        with open(output_dir / "complete_integration_report.txt", 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Save raw results as JSON
        results_data = {
            'test_results': [asdict(result) for result in self.integration_results],
            'health_metrics': asdict(self.health_metrics) if self.health_metrics else None,
            'test_environment': str(self.temp_dir),
            'execution_summary': {
                'total_tests': len(self.integration_results),
                'successful_tests': sum(1 for r in self.integration_results if r.success),
                'failed_tests': sum(1 for r in self.integration_results if not r.success),
                'total_execution_time': sum(r.execution_time for r in self.integration_results)
            }
        }
        
        with open(output_dir / "integration_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Integration test results saved to: {output_dir}")
        
        return output_dir
    
    def cleanup(self):
        """Cleanup test environment"""
        try:
            # Shutdown components
            if self.database:
                self.database.shutdown()
            if self.template_manager:
                self.template_manager.shutdown()
            
            # Clean up temporary files
            import shutil
            if Path(self.temp_dir).exists():
                shutil.rmtree(self.temp_dir)
            
            self.logger.info("Complete system integration test environment cleaned up")
            
        except Exception as e:
            self.logger.error(f"Error during cleanup: {e}")


def main():
    """Main entry point for complete system integration testing"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Deep Thinking Engine Complete System Integration Test')
    
    parser.add_argument('--output-dir', help='Output directory for results')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO')
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 100)
    print("DEEP THINKING ENGINE - COMPLETE SYSTEM INTEGRATION TEST")
    print("=" * 100)
    
    tester = None
    
    try:
        # Initialize tester
        tester = CompleteSystemIntegrationTester()
        print(f"âœ… Complete system initialized: {tester.temp_dir}")
        
        # Run basic integration tests
        print("\nğŸ§ª Running complete system integration tests...")
        results = tester.run_complete_system_integration_test()
        
        # Generate and display report
        report = tester.generate_comprehensive_integration_report()
        print("\n" + report)
        
        # Save results
        output_dir = tester.save_integration_results(args.output_dir)
        print(f"\nğŸ“„ Detailed results saved to: {output_dir}")
        
        # Determine success
        successful_tests = sum(1 for r in results if r.success)
        total_tests = len(results)
        
        if successful_tests == total_tests:
            print("\nğŸ‰ ALL COMPLETE SYSTEM INTEGRATION TESTS PASSED!")
            print("âœ… Deep Thinking Engine is fully integrated and ready for deployment!")
            return True
        else:
            failed_tests = total_tests - successful_tests
            print(f"\nâš ï¸  {failed_tests} integration test(s) FAILED out of {total_tests}")
            print("âŒ Complete system integration needs attention before deployment")
            return False
        
    except Exception as e:
        print(f"âŒ Complete system integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        if tester:
            tester.cleanup()


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)